{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_met_phi</th>\n",
       "      <th>PRI_met_sumet</th>\n",
       "      <th>PRI_jet_num</th>\n",
       "      <th>PRI_jet_leading_pt</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>s</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.910</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.240</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>b</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>b</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>b</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>b</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100005</td>\n",
       "      <td>b</td>\n",
       "      <td>89.744</td>\n",
       "      <td>13.550</td>\n",
       "      <td>59.149</td>\n",
       "      <td>116.344</td>\n",
       "      <td>2.636</td>\n",
       "      <td>284.584</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.362</td>\n",
       "      <td>...</td>\n",
       "      <td>2.237</td>\n",
       "      <td>282.849</td>\n",
       "      <td>3</td>\n",
       "      <td>90.547</td>\n",
       "      <td>-2.412</td>\n",
       "      <td>-0.653</td>\n",
       "      <td>56.165</td>\n",
       "      <td>0.224</td>\n",
       "      <td>3.106</td>\n",
       "      <td>193.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100006</td>\n",
       "      <td>s</td>\n",
       "      <td>148.754</td>\n",
       "      <td>28.862</td>\n",
       "      <td>107.782</td>\n",
       "      <td>106.130</td>\n",
       "      <td>0.733</td>\n",
       "      <td>158.359</td>\n",
       "      <td>0.113</td>\n",
       "      <td>2.941</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.443</td>\n",
       "      <td>294.074</td>\n",
       "      <td>2</td>\n",
       "      <td>123.010</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1.450</td>\n",
       "      <td>56.867</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>179.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100007</td>\n",
       "      <td>s</td>\n",
       "      <td>154.916</td>\n",
       "      <td>10.418</td>\n",
       "      <td>94.714</td>\n",
       "      <td>29.169</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.897</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.761</td>\n",
       "      <td>187.299</td>\n",
       "      <td>1</td>\n",
       "      <td>30.638</td>\n",
       "      <td>-0.715</td>\n",
       "      <td>-1.724</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>30.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100008</td>\n",
       "      <td>b</td>\n",
       "      <td>105.594</td>\n",
       "      <td>50.559</td>\n",
       "      <td>100.989</td>\n",
       "      <td>4.288</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024</td>\n",
       "      <td>129.804</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100009</td>\n",
       "      <td>s</td>\n",
       "      <td>128.053</td>\n",
       "      <td>88.941</td>\n",
       "      <td>69.272</td>\n",
       "      <td>193.392</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.845</td>\n",
       "      <td>294.741</td>\n",
       "      <td>1</td>\n",
       "      <td>167.735</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>-2.514</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>167.735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id Prediction  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  \\\n",
       "0  100000          s       138.470                       51.655        97.827   \n",
       "1  100001          b       160.937                       68.768       103.235   \n",
       "2  100002          b      -999.000                      162.172       125.953   \n",
       "3  100003          b       143.905                       81.417        80.943   \n",
       "4  100004          b       175.864                       16.915       134.805   \n",
       "5  100005          b        89.744                       13.550        59.149   \n",
       "6  100006          s       148.754                       28.862       107.782   \n",
       "7  100007          s       154.916                       10.418        94.714   \n",
       "8  100008          b       105.594                       50.559       100.989   \n",
       "9  100009          s       128.053                       88.941        69.272   \n",
       "\n",
       "   DER_pt_h  DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0    27.980                 0.910           124.711                2.666   \n",
       "1    48.146              -999.000          -999.000             -999.000   \n",
       "2    35.635              -999.000          -999.000             -999.000   \n",
       "3     0.414              -999.000          -999.000             -999.000   \n",
       "4    16.405              -999.000          -999.000             -999.000   \n",
       "5   116.344                 2.636           284.584               -0.540   \n",
       "6   106.130                 0.733           158.359                0.113   \n",
       "7    29.169              -999.000          -999.000             -999.000   \n",
       "8     4.288              -999.000          -999.000             -999.000   \n",
       "9   193.392              -999.000          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep       ...        PRI_met_phi  PRI_met_sumet  \\\n",
       "0               3.064       ...             -0.277        258.733   \n",
       "1               3.473       ...             -1.916        164.546   \n",
       "2               3.148       ...             -2.186        260.414   \n",
       "3               3.310       ...              0.060         86.062   \n",
       "4               3.891       ...             -0.871         53.131   \n",
       "5               1.362       ...              2.237        282.849   \n",
       "6               2.941       ...             -1.443        294.074   \n",
       "7               2.897       ...             -1.761        187.299   \n",
       "8               2.904       ...              0.024        129.804   \n",
       "9               1.609       ...              0.845        294.741   \n",
       "\n",
       "   PRI_jet_num  PRI_jet_leading_pt  PRI_jet_leading_eta  PRI_jet_leading_phi  \\\n",
       "0            2              67.435                2.150                0.444   \n",
       "1            1              46.226                0.725                1.158   \n",
       "2            1              44.251                2.053               -2.028   \n",
       "3            0            -999.000             -999.000             -999.000   \n",
       "4            0            -999.000             -999.000             -999.000   \n",
       "5            3              90.547               -2.412               -0.653   \n",
       "6            2             123.010                0.864                1.450   \n",
       "7            1              30.638               -0.715               -1.724   \n",
       "8            0            -999.000             -999.000             -999.000   \n",
       "9            1             167.735               -2.767               -2.514   \n",
       "\n",
       "   PRI_jet_subleading_pt  PRI_jet_subleading_eta  PRI_jet_subleading_phi  \\\n",
       "0                 46.062                   1.240                  -2.475   \n",
       "1               -999.000                -999.000                -999.000   \n",
       "2               -999.000                -999.000                -999.000   \n",
       "3               -999.000                -999.000                -999.000   \n",
       "4               -999.000                -999.000                -999.000   \n",
       "5                 56.165                   0.224                   3.106   \n",
       "6                 56.867                   0.131                  -2.767   \n",
       "7               -999.000                -999.000                -999.000   \n",
       "8               -999.000                -999.000                -999.000   \n",
       "9               -999.000                -999.000                -999.000   \n",
       "\n",
       "   PRI_jet_all_pt  \n",
       "0         113.497  \n",
       "1          46.226  \n",
       "2          44.251  \n",
       "3           0.000  \n",
       "4           0.000  \n",
       "5         193.660  \n",
       "6         179.877  \n",
       "7          30.638  \n",
       "8           0.000  \n",
       "9         167.735  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # Import not allowed for the final project\n",
    "data = pd.read_csv(DATA_FOLDER + 'train.csv')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Below is the implementation of the required standard algorithms. All the methods were previously implemented in the labs,\n",
    "besides the logistic regression and the regularized logistic regression\n",
    "\n",
    "'''\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "\n",
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute loss, gradient\n",
    "        grad, err = compute_gradient(y, tx, w)\n",
    "        loss = calculate_mse(err)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            grad, _ = compute_gradient(y_batch, tx_batch, w)\n",
    "            w = w - gamma * grad\n",
    "            loss = compute_loss(y, tx, w)\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\"SGD({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)\n",
    "\n",
    "#Logistic Regression\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# Try it without regularization first\n",
    "def cost_log(tx,y,w):\n",
    "    res = -y*np.log(sigmoid(tx.dot(w)))-(1-y)*np.log(1-sigmoid(tx.dot(w)))\n",
    "    return res[res != np.inf].mean()\n",
    "\n",
    "def gradient_log(tx,y,w,gamma):\n",
    "    return w - gamma * (tx.T.dot(sigmoid(tx.dot(w))-y))/y.shape[0]\n",
    "\n",
    "\n",
    "def logistic_reg(y,tx,initial_w,max_iters,gamma):\n",
    "    if(initial_w is None):\n",
    "        initial_w = np.zeros(tx.shape[1])\n",
    "        \n",
    "    w = initial_w\n",
    "    \n",
    "    thresh = 1e-9\n",
    "    loss = []\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        w = gradient_log(tx,y,w,gamma)\n",
    "        l = cost_log(tx,y,w)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={lo}, w = {weights}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, lo=l, weights = w))\n",
    "        \n",
    "        loss.append(l)\n",
    "        if len(loss) > 1 and abs(loss[-1]-loss[-2]) < thresh:\n",
    "            break\n",
    "    \n",
    "    return w,loss\n",
    "\n",
    "def cost_log_reg(tx,y,w,lambda_):\n",
    "    return (-y*np.log(sigmoid(tx.dot(w)))-(1-y)*np.log(1-sigmoid(tx.dot(w)))).mean() +(lambda_)**2 * (w.T.dot(w))\n",
    "\n",
    "def gradient_log_reg(tx,y,w,gamma,lambda_):\n",
    "    return w - gamma * (tx.T.dot(sigmoid(tx.dot(w))-y))/y.shape[0] + 2 * lambda_ * w\n",
    "\n",
    "def logistic_reg_regularized(y,tx,lambda_,initial_w,max_iters,gamma):\n",
    "    \n",
    "    if(initial_w is None):\n",
    "        initial_w = np.zeros(tx.shape[1])\n",
    "    \n",
    "    thresh = 1e-9\n",
    "    loss = []\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        w = gradient_log_reg(tx,y,w,gamma)\n",
    "        l = cost_log_reg(tx,y,w)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w = {weights}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, weights = w))\n",
    "        loss.append(l)\n",
    "        if len(loss > 1) and abs(loss[-1]-loss[-2]) < thresh:\n",
    "            break\n",
    "    \n",
    "    return w,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_met_phi</th>\n",
       "      <th>PRI_met_sumet</th>\n",
       "      <th>PRI_jet_num</th>\n",
       "      <th>PRI_jet_leading_pt</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>s</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.910</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.240</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>b</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>b</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>b</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>b</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100005</td>\n",
       "      <td>b</td>\n",
       "      <td>89.744</td>\n",
       "      <td>13.550</td>\n",
       "      <td>59.149</td>\n",
       "      <td>116.344</td>\n",
       "      <td>2.636</td>\n",
       "      <td>284.584</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.362</td>\n",
       "      <td>...</td>\n",
       "      <td>2.237</td>\n",
       "      <td>282.849</td>\n",
       "      <td>3</td>\n",
       "      <td>90.547</td>\n",
       "      <td>-2.412</td>\n",
       "      <td>-0.653</td>\n",
       "      <td>56.165</td>\n",
       "      <td>0.224</td>\n",
       "      <td>3.106</td>\n",
       "      <td>193.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100006</td>\n",
       "      <td>s</td>\n",
       "      <td>148.754</td>\n",
       "      <td>28.862</td>\n",
       "      <td>107.782</td>\n",
       "      <td>106.130</td>\n",
       "      <td>0.733</td>\n",
       "      <td>158.359</td>\n",
       "      <td>0.113</td>\n",
       "      <td>2.941</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.443</td>\n",
       "      <td>294.074</td>\n",
       "      <td>2</td>\n",
       "      <td>123.010</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1.450</td>\n",
       "      <td>56.867</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>179.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100007</td>\n",
       "      <td>s</td>\n",
       "      <td>154.916</td>\n",
       "      <td>10.418</td>\n",
       "      <td>94.714</td>\n",
       "      <td>29.169</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.897</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.761</td>\n",
       "      <td>187.299</td>\n",
       "      <td>1</td>\n",
       "      <td>30.638</td>\n",
       "      <td>-0.715</td>\n",
       "      <td>-1.724</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>30.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100008</td>\n",
       "      <td>b</td>\n",
       "      <td>105.594</td>\n",
       "      <td>50.559</td>\n",
       "      <td>100.989</td>\n",
       "      <td>4.288</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024</td>\n",
       "      <td>129.804</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100009</td>\n",
       "      <td>s</td>\n",
       "      <td>128.053</td>\n",
       "      <td>88.941</td>\n",
       "      <td>69.272</td>\n",
       "      <td>193.392</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.845</td>\n",
       "      <td>294.741</td>\n",
       "      <td>1</td>\n",
       "      <td>167.735</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>-2.514</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>167.735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id Prediction  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  \\\n",
       "0  100000          s       138.470                       51.655        97.827   \n",
       "1  100001          b       160.937                       68.768       103.235   \n",
       "2  100002          b      -999.000                      162.172       125.953   \n",
       "3  100003          b       143.905                       81.417        80.943   \n",
       "4  100004          b       175.864                       16.915       134.805   \n",
       "5  100005          b        89.744                       13.550        59.149   \n",
       "6  100006          s       148.754                       28.862       107.782   \n",
       "7  100007          s       154.916                       10.418        94.714   \n",
       "8  100008          b       105.594                       50.559       100.989   \n",
       "9  100009          s       128.053                       88.941        69.272   \n",
       "\n",
       "   DER_pt_h  DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0    27.980                 0.910           124.711                2.666   \n",
       "1    48.146              -999.000          -999.000             -999.000   \n",
       "2    35.635              -999.000          -999.000             -999.000   \n",
       "3     0.414              -999.000          -999.000             -999.000   \n",
       "4    16.405              -999.000          -999.000             -999.000   \n",
       "5   116.344                 2.636           284.584               -0.540   \n",
       "6   106.130                 0.733           158.359                0.113   \n",
       "7    29.169              -999.000          -999.000             -999.000   \n",
       "8     4.288              -999.000          -999.000             -999.000   \n",
       "9   193.392              -999.000          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep       ...        PRI_met_phi  PRI_met_sumet  \\\n",
       "0               3.064       ...             -0.277        258.733   \n",
       "1               3.473       ...             -1.916        164.546   \n",
       "2               3.148       ...             -2.186        260.414   \n",
       "3               3.310       ...              0.060         86.062   \n",
       "4               3.891       ...             -0.871         53.131   \n",
       "5               1.362       ...              2.237        282.849   \n",
       "6               2.941       ...             -1.443        294.074   \n",
       "7               2.897       ...             -1.761        187.299   \n",
       "8               2.904       ...              0.024        129.804   \n",
       "9               1.609       ...              0.845        294.741   \n",
       "\n",
       "   PRI_jet_num  PRI_jet_leading_pt  PRI_jet_leading_eta  PRI_jet_leading_phi  \\\n",
       "0            2              67.435                2.150                0.444   \n",
       "1            1              46.226                0.725                1.158   \n",
       "2            1              44.251                2.053               -2.028   \n",
       "3            0            -999.000             -999.000             -999.000   \n",
       "4            0            -999.000             -999.000             -999.000   \n",
       "5            3              90.547               -2.412               -0.653   \n",
       "6            2             123.010                0.864                1.450   \n",
       "7            1              30.638               -0.715               -1.724   \n",
       "8            0            -999.000             -999.000             -999.000   \n",
       "9            1             167.735               -2.767               -2.514   \n",
       "\n",
       "   PRI_jet_subleading_pt  PRI_jet_subleading_eta  PRI_jet_subleading_phi  \\\n",
       "0                 46.062                   1.240                  -2.475   \n",
       "1               -999.000                -999.000                -999.000   \n",
       "2               -999.000                -999.000                -999.000   \n",
       "3               -999.000                -999.000                -999.000   \n",
       "4               -999.000                -999.000                -999.000   \n",
       "5                 56.165                   0.224                   3.106   \n",
       "6                 56.867                   0.131                  -2.767   \n",
       "7               -999.000                -999.000                -999.000   \n",
       "8               -999.000                -999.000                -999.000   \n",
       "9               -999.000                -999.000                -999.000   \n",
       "\n",
       "   PRI_jet_all_pt  \n",
       "0         113.497  \n",
       "1          46.226  \n",
       "2          44.251  \n",
       "3           0.000  \n",
       "4           0.000  \n",
       "5         193.660  \n",
       "6         179.877  \n",
       "7          30.638  \n",
       "8           0.000  \n",
       "9         167.735  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Below is the implementation of the Higgs Bosson ML Challenge. Before applying logistic regression for the classification,\n",
    "we first do some feature engineering in order to clean the data, and remove some unnecessary features.\n",
    "\n",
    "'''\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "X = data.iloc[:,2:]\n",
    "y = data.loc[:,'Prediction']\n",
    "y = (y=='s')*1\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "f, axarr = plt.subplots(5,6)\n",
    "f.set_figheight(20)\n",
    "f.set_figwidth(20)\n",
    "for i in range(5):\n",
    "    for j in range(6):\n",
    "        axarr[i,j].get_xaxis().set_visible(False)\n",
    "        axarr[i,j].get_yaxis().set_visible(False)\n",
    "        axarr[i,j].hist(data.iloc[:,(i+1)*(j+1)].values,bins = 10,histtype = 'step')\n",
    "'''\n",
    "\n",
    "for index in range(2,len(data.columns)):\n",
    "    col = data.iloc[:,index]\n",
    "    myplt = col[col!=-999].hist(figsize = (15,15),bins=10,grid = False,edgecolor = 'black')\n",
    "    myplt.set_xlabel(data.columns[index])\n",
    "    plt.savefig('fig'+str(index)+'.png')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_features_index = [0,17,20,22,27,30]\n",
    "\n",
    "'''\n",
    "As one can see from the above distribution graphs, the phi variables have a very different distribution from the other variables.\n",
    "Hence, we conclude that they provide little information about our dataset, so we can remove these columns from our table\n",
    "\n",
    "'''\n",
    "data.drop(columns = data.columns[phi_features_index],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_lep_pt</th>\n",
       "      <th>PRI_lep_eta</th>\n",
       "      <th>PRI_met</th>\n",
       "      <th>PRI_met_sumet</th>\n",
       "      <th>PRI_jet_num</th>\n",
       "      <th>PRI_jet_leading_pt</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.910</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>51.626</td>\n",
       "      <td>2.273</td>\n",
       "      <td>16.824</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.240</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>36.918</td>\n",
       "      <td>0.501</td>\n",
       "      <td>44.704</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>121.409</td>\n",
       "      <td>-0.953</td>\n",
       "      <td>54.283</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>53.321</td>\n",
       "      <td>-0.522</td>\n",
       "      <td>31.082</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>29.774</td>\n",
       "      <td>0.798</td>\n",
       "      <td>2.723</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b</td>\n",
       "      <td>89.744</td>\n",
       "      <td>13.550</td>\n",
       "      <td>59.149</td>\n",
       "      <td>116.344</td>\n",
       "      <td>2.636</td>\n",
       "      <td>284.584</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>1.362</td>\n",
       "      <td>61.619</td>\n",
       "      <td>...</td>\n",
       "      <td>31.565</td>\n",
       "      <td>-0.884</td>\n",
       "      <td>40.735</td>\n",
       "      <td>282.849</td>\n",
       "      <td>3</td>\n",
       "      <td>90.547</td>\n",
       "      <td>-2.412</td>\n",
       "      <td>56.165</td>\n",
       "      <td>0.224</td>\n",
       "      <td>193.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>s</td>\n",
       "      <td>148.754</td>\n",
       "      <td>28.862</td>\n",
       "      <td>107.782</td>\n",
       "      <td>106.130</td>\n",
       "      <td>0.733</td>\n",
       "      <td>158.359</td>\n",
       "      <td>0.113</td>\n",
       "      <td>2.941</td>\n",
       "      <td>2.545</td>\n",
       "      <td>...</td>\n",
       "      <td>97.240</td>\n",
       "      <td>0.675</td>\n",
       "      <td>38.421</td>\n",
       "      <td>294.074</td>\n",
       "      <td>2</td>\n",
       "      <td>123.010</td>\n",
       "      <td>0.864</td>\n",
       "      <td>56.867</td>\n",
       "      <td>0.131</td>\n",
       "      <td>179.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>s</td>\n",
       "      <td>154.916</td>\n",
       "      <td>10.418</td>\n",
       "      <td>94.714</td>\n",
       "      <td>29.169</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.897</td>\n",
       "      <td>1.526</td>\n",
       "      <td>...</td>\n",
       "      <td>28.740</td>\n",
       "      <td>0.506</td>\n",
       "      <td>22.275</td>\n",
       "      <td>187.299</td>\n",
       "      <td>1</td>\n",
       "      <td>30.638</td>\n",
       "      <td>-0.715</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>30.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b</td>\n",
       "      <td>105.594</td>\n",
       "      <td>50.559</td>\n",
       "      <td>100.989</td>\n",
       "      <td>4.288</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.904</td>\n",
       "      <td>4.288</td>\n",
       "      <td>...</td>\n",
       "      <td>26.325</td>\n",
       "      <td>0.210</td>\n",
       "      <td>37.791</td>\n",
       "      <td>129.804</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>s</td>\n",
       "      <td>128.053</td>\n",
       "      <td>88.941</td>\n",
       "      <td>69.272</td>\n",
       "      <td>193.392</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>28.859</td>\n",
       "      <td>...</td>\n",
       "      <td>32.742</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>132.678</td>\n",
       "      <td>294.741</td>\n",
       "      <td>1</td>\n",
       "      <td>167.735</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>167.735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Prediction  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  \\\n",
       "0          s       138.470                       51.655        97.827   \n",
       "1          b       160.937                       68.768       103.235   \n",
       "2          b      -999.000                      162.172       125.953   \n",
       "3          b       143.905                       81.417        80.943   \n",
       "4          b       175.864                       16.915       134.805   \n",
       "5          b        89.744                       13.550        59.149   \n",
       "6          s       148.754                       28.862       107.782   \n",
       "7          s       154.916                       10.418        94.714   \n",
       "8          b       105.594                       50.559       100.989   \n",
       "9          s       128.053                       88.941        69.272   \n",
       "\n",
       "   DER_pt_h  DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0    27.980                 0.910           124.711                2.666   \n",
       "1    48.146              -999.000          -999.000             -999.000   \n",
       "2    35.635              -999.000          -999.000             -999.000   \n",
       "3     0.414              -999.000          -999.000             -999.000   \n",
       "4    16.405              -999.000          -999.000             -999.000   \n",
       "5   116.344                 2.636           284.584               -0.540   \n",
       "6   106.130                 0.733           158.359                0.113   \n",
       "7    29.169              -999.000          -999.000             -999.000   \n",
       "8     4.288              -999.000          -999.000             -999.000   \n",
       "9   193.392              -999.000          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot       ...        PRI_lep_pt  PRI_lep_eta  \\\n",
       "0               3.064      41.928       ...            51.626        2.273   \n",
       "1               3.473       2.078       ...            36.918        0.501   \n",
       "2               3.148       9.336       ...           121.409       -0.953   \n",
       "3               3.310       0.414       ...            53.321       -0.522   \n",
       "4               3.891      16.405       ...            29.774        0.798   \n",
       "5               1.362      61.619       ...            31.565       -0.884   \n",
       "6               2.941       2.545       ...            97.240        0.675   \n",
       "7               2.897       1.526       ...            28.740        0.506   \n",
       "8               2.904       4.288       ...            26.325        0.210   \n",
       "9               1.609      28.859       ...            32.742       -0.317   \n",
       "\n",
       "   PRI_met  PRI_met_sumet  PRI_jet_num  PRI_jet_leading_pt  \\\n",
       "0   16.824        258.733            2              67.435   \n",
       "1   44.704        164.546            1              46.226   \n",
       "2   54.283        260.414            1              44.251   \n",
       "3   31.082         86.062            0            -999.000   \n",
       "4    2.723         53.131            0            -999.000   \n",
       "5   40.735        282.849            3              90.547   \n",
       "6   38.421        294.074            2             123.010   \n",
       "7   22.275        187.299            1              30.638   \n",
       "8   37.791        129.804            0            -999.000   \n",
       "9  132.678        294.741            1             167.735   \n",
       "\n",
       "   PRI_jet_leading_eta  PRI_jet_subleading_pt  PRI_jet_subleading_eta  \\\n",
       "0                2.150                 46.062                   1.240   \n",
       "1                0.725               -999.000                -999.000   \n",
       "2                2.053               -999.000                -999.000   \n",
       "3             -999.000               -999.000                -999.000   \n",
       "4             -999.000               -999.000                -999.000   \n",
       "5               -2.412                 56.165                   0.224   \n",
       "6                0.864                 56.867                   0.131   \n",
       "7               -0.715               -999.000                -999.000   \n",
       "8             -999.000               -999.000                -999.000   \n",
       "9               -2.767               -999.000                -999.000   \n",
       "\n",
       "   PRI_jet_all_pt  \n",
       "0         113.497  \n",
       "1          46.226  \n",
       "2          44.251  \n",
       "3           0.000  \n",
       "4           0.000  \n",
       "5         193.660  \n",
       "6         179.877  \n",
       "7          30.638  \n",
       "8           0.000  \n",
       "9         167.735  \n",
       "\n",
       "[10 rows x 26 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We've dropped the unnecessary columns\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 0 missing\n",
      "DER_mass_MMC 38114 missing\n",
      "DER_mass_transverse_met_lep 0 missing\n",
      "DER_mass_vis 0 missing\n",
      "DER_pt_h 0 missing\n",
      "DER_deltaeta_jet_jet 177457 missing\n",
      "DER_mass_jet_jet 177457 missing\n",
      "DER_prodeta_jet_jet 177457 missing\n",
      "DER_deltar_tau_lep 0 missing\n",
      "DER_pt_tot 0 missing\n",
      "DER_sum_pt 0 missing\n",
      "DER_pt_ratio_lep_tau 0 missing\n",
      "DER_met_phi_centrality 0 missing\n",
      "DER_lep_eta_centrality 177457 missing\n",
      "PRI_tau_pt 0 missing\n",
      "PRI_tau_eta 0 missing\n",
      "PRI_lep_pt 0 missing\n",
      "PRI_lep_eta 0 missing\n",
      "PRI_met 0 missing\n",
      "PRI_met_sumet 0 missing\n",
      "PRI_jet_num 0 missing\n",
      "PRI_jet_leading_pt 99913 missing\n",
      "PRI_jet_leading_eta 99913 missing\n",
      "PRI_jet_subleading_pt 177457 missing\n",
      "PRI_jet_subleading_eta 177457 missing\n",
      "PRI_jet_all_pt 0 missing\n"
     ]
    }
   ],
   "source": [
    "for col in data:\n",
    "    data_missing= (data[col] == -999.000)\n",
    "    print(col,data_missing.sum(), 'missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b    74421\n",
      "s    25492\n",
      "Name: Prediction, dtype: int64\n",
      "b    49834\n",
      "s    27710\n",
      "Name: Prediction, dtype: int64\n",
      "s    25734\n",
      "b    24645\n",
      "Name: Prediction, dtype: int64\n",
      "b    15433\n",
      "s     6731\n",
      "Name: Prediction, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the classification based on the values of the jet particle\n",
    "\n",
    "for jet_cat in range(0,4):\n",
    "    jet_cat_num = (data['PRI_jet_num'] == jet_cat)\n",
    "    \n",
    "    print(data[jet_cat_num]['Prediction'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_div = []\n",
    "for i in range(4):\n",
    "    jet_div.append(data[data['PRI_jet_num']==i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(jet_div[1]['PRI_jet_subleading_eta'] == -999).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the data by the 'PRI_jet_num' column to see if we can do remove some columns that may provide no information about the group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty(jet_frame):\n",
    "    \n",
    "    for cols in jet_frame.columns:\n",
    "        if((jet_frame[cols]==-999).all()):\n",
    "            jet_frame = jet_frame.drop(cols,axis = 1)\n",
    "        \n",
    "    return jet_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function removes all the empty columns from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above true conditional statements, the columns 'PRI_jet_num' and 'PRI_jet_all_pt' are always 0 for the first group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009999999999976694"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allEqual = True\n",
    "eps=1e-2\n",
    "\n",
    "res = jet_div[0]['DER_pt_h'] == jet_div[0]['DER_pt_tot']\n",
    "res = jet_div[0].loc[:,'DER_pt_h']  == jet_div[0].loc[:,'DER_pt_tot']\n",
    "res[res == False]\n",
    "#7609 is one of the indexes where the values in the columns 'DER_pt_h' and 'DER_pt_tot' differ\n",
    "abs(jet_div[0].loc[7609]['DER_pt_h'] - jet_div[0].loc[7609]['DER_pt_tot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above code, we can see that for the first group, the values of the columns 'DER_pt_h' and 'DER_pt_tot' are equal for most of the columns and even when they differ, the difference is less than 10^-3. Hence, we can remove one of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emiljano\\Anaconda3\\envs\\newEnv\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "jet_div[0].drop(columns = ['PRI_jet_num','PRI_jet_all_pt','DER_pt_tot'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0009999999999976694"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print((jet_div[1]['PRI_jet_num'] == 1).all())\n",
    "temp = jet_div[1]['PRI_jet_leading_pt'] == jet_div[1]['PRI_jet_all_pt']\n",
    "temp[temp == False]\n",
    "abs(jet_div[1].loc[1829]['PRI_jet_leading_pt'] - jet_div[1].loc[1829]['PRI_jet_all_pt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 2nd group, we can remove the 'PRI_jet_num' column since we know that it is 1. We also notice that the columns 'PRI_jet_leading_pt' and 'PRI_jet_all_pt' have almost always equal values, and when they differ the difference is less then 10^-3. Hence, we can remove the columns 'PRI_jet_num' and 'PRI_jet_leading_pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emiljano\\Anaconda3\\envs\\newEnv\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "jet_div[1].drop(columns  = ['PRI_jet_num','PRI_jet_leading_pt'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emiljano\\Anaconda3\\envs\\newEnv\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# We follow the same reasoning as above for the 3rd group\n",
    "jet_div[2].drop(columns  = ['PRI_jet_num','PRI_jet_leading_pt'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#print((jet_div[3]['PRI_jet_num'] == 3).all())\n",
    "eps = 1e-2\n",
    "print((abs(abs(jet_div[2]['PRI_jet_leading_eta'] - jet_div[2]['PRI_jet_subleading_eta']) - jet_div[2]['DER_deltaeta_jet_jet']) <eps ).all())\n",
    "print((abs(abs(jet_div[3]['PRI_jet_leading_eta'] - jet_div[3]['PRI_jet_subleading_eta']) - jet_div[3]['DER_deltaeta_jet_jet']) <eps ).all())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From the above cell, we can see that the values in the column 'DER_deltaeta_jet_jet' for the 3rd and 4th group are almost equal to the absolute value of the difference between the columns 'PRI_jet_leading_eta' and 'PRI_jet_subleading_eta'. In the cases when these values differ, the difference is insignifcant. Hence, we can remove the 'PRI_jet_subleading_eta' for the 2nd and 3rd groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emiljano\\Anaconda3\\envs\\newEnv\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "jet_div[2].drop(columns = ['DER_deltaeta_jet_jet'],inplace = True)\n",
    "jet_div[3].drop(columns = ['DER_deltaeta_jet_jet'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emiljano\\Anaconda3\\envs\\newEnv\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(jet_div)):\n",
    "    jet_div[i].drop(columns = ['Prediction'],inplace  = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(jet_div)):\n",
    "    jet_div[i] = remove_empty(jet_div[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We removed the Prediction columns from the dataset, because its values are already stored in the y vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def cost_log_reg(tx,y,w,lambda_):\n",
    "    return (-y*np.log(sigmoid(tx.dot(w)))-(1-y)*np.log(1-sigmoid(tx.dot(w)))).mean() +(lambda_)**2 * (w.T.dot(w))\n",
    "'''\n",
    "x = jet_div[0]\n",
    "y_train = y[x.index]\n",
    "w = np.random.rand(x.shape[1]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.apply(lambda col_x: (col_x - np.mean(col_x))/np.std(col_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.insert(0,'dummy',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserted a column of 1s in our dataset, in order to account for the bias w_0 when multiplying the weights with x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emiljano\\Anaconda3\\envs\\newEnv\\lib\\site-packages\\ipykernel_launcher.py:65: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=1.172788924520989, w = [-0.01799261  0.16130029  0.43031386  0.05051181  0.22766302  0.08770469\n",
      "  0.82328916  0.47866143  0.02103525  0.05140939  0.62991054  0.19015415\n",
      "  0.64465784  0.39554879  0.54764695]\n",
      "Gradient Descent(1/999): loss=1.1308509226769075, w = [-0.04002262  0.17181301  0.39773105  0.04020642  0.22129454  0.09457939\n",
      "  0.8122529   0.45286029  0.026537    0.05765826  0.61489133  0.16580247\n",
      "  0.62963848  0.37565519  0.53828333]\n",
      "Gradient Descent(2/999): loss=1.090579243856298, w = [-0.06181587  0.18176305  0.36599471  0.02990744  0.21486825  0.10112923\n",
      "  0.80125365  0.42771597  0.03171047  0.0635632   0.59993033  0.14186967\n",
      "  0.61467313  0.35631901  0.52880517]\n",
      "Gradient Descent(3/999): loss=1.0519650162772, w = [-0.08336427  0.19115842  0.33511805  0.01963216  0.20839412  0.10736565\n",
      "  0.79030958  0.40324412  0.03656068  0.0691302   0.58504527  0.11837883\n",
      "  0.59978003  0.33754601  0.51922893]\n",
      "Gradient Descent(4/999): loss=1.0150078584203956, w = [-0.10465953  0.20000839  0.30511304  0.00939821  0.20188235  0.11330068\n",
      "  0.77943937  0.37945934  0.04109349  0.07436623  0.57025399  0.09535289\n",
      "  0.58497755  0.3193412   0.50957176]\n",
      "Gradient Descent(5/999): loss=0.9797038695475241, w = [-0.12569322  0.20832348  0.27599033 -0.00077658  0.19534344  0.11894695\n",
      "  0.76866218  0.35637507  0.04531565  0.07927921  0.55557436  0.07281452\n",
      "  0.57028404  0.30170876  0.49985142]\n",
      "Gradient Descent(6/999): loss=0.946045363324495, w = [-0.14645687  0.21611542  0.24775909 -0.01087427  0.18878805  0.1243175\n",
      "  0.75799753  0.33400341  0.04923475  0.08387803  0.5410241   0.05078593\n",
      "  0.5557177   0.28465197  0.49008619]\n",
      "Gradient Descent(7/999): loss=0.9140206133130405, w = [-0.16694208  0.2233972   0.2204269  -0.02087702  0.18222699  0.12942577\n",
      "  0.74746515  0.31235497  0.05285925  0.08817255  0.52662072  0.02928875\n",
      "  0.54129648  0.26817316  0.48029482]\n",
      "Gradient Descent(8/999): loss=0.8836136201127729, w = [-0.1871406   0.23018299  0.19399959 -0.0307672   0.17567116  0.13428547\n",
      "  0.7370849   0.29143865  0.05619839  0.09217354  0.5123813   0.00834378\n",
      "  0.52703793  0.25227361  0.47049636]\n",
      "Gradient Descent(9/999): loss=0.8548039130769494, w = [-0.20704449  0.2364881   0.1684811  -0.04052761  0.16913149  0.13891044\n",
      "  0.72687658  0.27126155  0.05926219  0.09589267  0.4983224  -0.01202917\n",
      "  0.51295904  0.23695355  0.46071008]\n",
      "Gradient Descent(10/999): loss=0.8275664025596697, w = [-0.2266462   0.24232897  0.14387341 -0.05014164  0.16261886  0.14331457\n",
      "  0.71685978  0.25182875  0.06206139  0.09934247  0.48445991 -0.03181154\n",
      "  0.49907614  0.22221201  0.45095534]\n",
      "Gradient Descent(11/999): loss=0.801871300506691, w = [-0.24593873  0.24772302  0.12017634 -0.05959346  0.15614402  0.14751171\n",
      "  0.70705369  0.23314314  0.0646074   0.10253625  0.47080892 -0.05098621\n",
      "  0.48540474  0.20804682  0.44125143]\n",
      "Gradient Descent(12/999): loss=0.7776841263724915, w = [-0.26491575  0.25268861  0.09738751 -0.06886817  0.14971751  0.15151552\n",
      "  0.69747694  0.21520536  0.06691218  0.10548798  0.45738355 -0.06953773\n",
      "  0.47195942  0.19445455  0.4316174 ]\n",
      "Gradient Descent(13/999): loss=0.7549658108768664, w = [-0.28357166  0.25724494  0.07550222 -0.07795197  0.14334957  0.15533942\n",
      "  0.68814735  0.19801363  0.06898819  0.10821229  0.44419689 -0.08745252\n",
      "  0.45875371  0.18143044  0.42207192]\n",
      "Gradient Descent(14/999): loss=0.7336729018067849, w = [-0.30190176  0.26141188  0.05451345 -0.08683228  0.13705003  0.15899649\n",
      "  0.67908184  0.18156371  0.07084831  0.11072425  0.43126087 -0.10471904\n",
      "  0.44579999  0.16896845  0.4126331 ]\n",
      "Gradient Descent(15/999): loss=0.7137578649732339, w = [-0.3199023   0.26520982  0.03441184 -0.09549789  0.13082823  0.16249938\n",
      "  0.67029615  0.16584886  0.07250569  0.11303933  0.41858615 -0.12132798\n",
      "  0.43310939  0.15706119  0.4033183 ]\n",
      "Gradient Descent(16/999): loss=0.6951694619707317, w = [-0.3375705   0.26865954  0.0151857  -0.10393899  0.12469292  0.16586026\n",
      "  0.66180476  0.15085985  0.07397369  0.11517321  0.40618209 -0.13727235\n",
      "  0.42069169  0.14570003  0.394144  ]\n",
      "Gradient Descent(17/999): loss=0.67785317799318, w = [-0.35490465  0.27178201 -0.00317889 -0.11214729  0.1186522   0.16909076\n",
      "  0.65362065  0.13658499  0.07526573  0.11714168  0.39405669 -0.15254764\n",
      "  0.40855532  0.13487511  0.38512559]\n",
      "Gradient Descent(18/999): loss=0.6617450482766554, w = [-0.37190411  0.27459823 -0.02069811 -0.1201161   0.11271342  0.17220192\n",
      "  0.64575517  0.12301011  0.07639522  0.11896045  0.38221656 -0.16715193\n",
      "  0.39670724  0.12457533  0.37627727]\n",
      "Gradient Descent(19/999): loss=0.6467987550140012, w = [-0.3885693   0.27712906 -0.03739034 -0.12784031  0.10688316  0.17520414\n",
      "  0.63821785  0.11011867  0.07737539  0.12064503  0.37066692 -0.18108605\n",
      "  0.385153    0.11478842  0.36761188]\n",
      "Gradient Descent(20/999): loss=0.6333003075136279, w = [-0.4049017   0.27939512 -0.05327615 -0.13531648  0.10116719  0.17810718\n",
      "  0.63101617  0.09789173  0.07821927  0.12221057  0.35941164 -0.1943537\n",
      "  0.37389674  0.10550089  0.3591408 ]\n",
      "Gradient Descent(21/999): loss=0.6204669855715695, w = [-0.42090384  0.28141663 -0.06837841 -0.14254288  0.09557039  0.18092015\n",
      "  0.62415541  0.08630804  0.0789395   0.12367174  0.34845325 -0.20696167\n",
      "  0.36294121  0.09669792  0.35087385]\n",
      "Gradient Descent(22/999): loss=0.6086002148819887, w = [-0.43657922  0.28321331 -0.08272215 -0.14951941  0.09009679  0.18365153\n",
      "  0.61763849  0.07534419  0.07954829  0.12504252  0.337793   -0.21891986\n",
      "  0.35228785  0.08836341  0.34281922]\n",
      "Gradient Descent(23/999): loss=0.5976355683777144, w = [-0.45193229  0.28480428 -0.09633457 -0.1562476   0.08474952  0.18630916\n",
      "  0.61146586  0.06497476  0.08005731  0.12633614  0.32743093 -0.23024134\n",
      "  0.34193687  0.08047996  0.3349834 ]\n",
      "Gradient Descent(24/999): loss=0.5875103302708413, w = [-0.46696831  0.28620795 -0.10924494 -0.16273054  0.07953083  0.18890026\n",
      "  0.60563545  0.05517262  0.08047758  0.12756492  0.31736596 -0.24094232\n",
      "  0.33188726  0.0730288   0.3273712 ]\n",
      "Gradient Descent(25/999): loss=0.5781617011633846, w = [-0.48169329  0.28744195 -0.12148447 -0.16897279  0.07444212  0.19143146\n",
      "  0.60014261  0.04590918  0.08081948  0.12874017  0.30759596 -0.25104214\n",
      "  0.32213688  0.06598993  0.31998573]\n",
      "Gradient Descent(26/999): loss=0.5695288485204442, w = [-0.49611381  0.28852296 -0.13308595 -0.17498024  0.06948404  0.19390878\n",
      "  0.59498029  0.0371549   0.08109259  0.12987214  0.29811789 -0.26056287\n",
      "  0.31268257  0.05934223  0.31282853]\n",
      "Gradient Descent(27/999): loss=0.5615532379687671, w = [-0.51023695  0.2894667  -0.14408319 -0.18075995  0.06465649  0.19633766\n",
      "  0.5901393   0.02887983  0.08130582  0.13096992  0.28892787 -0.26952883\n",
      "  0.30352017  0.05306397  0.30589954]\n",
      "Gradient Descent(28/999): loss=0.5541793256720172, w = [-0.52407011  0.29028778 -0.15451053 -0.18631991  0.05995873  0.19872293\n",
      "  0.58560863  0.02105398  0.08146727  0.13204146  0.28002121 -0.27796614\n",
      "  0.2946447   0.04713305  0.29919728]\n",
      "Gradient Descent(29/999): loss=0.5473548718979788, w = [-0.53762091  0.29099973 -0.16440243 -0.19166877  0.05538938  0.20106893\n",
      "  0.58137579  0.01364765  0.08158438  0.13309366  0.27139252 -0.28590217\n",
      "  0.28605043  0.04152735  0.29271895]\n",
      "Gradient Descent(30/999): loss=0.5410314066297972, w = [-0.55089706  0.29161491 -0.17379277 -0.1968156   0.05094659  0.20337948\n",
      "  0.57742729  0.00663196  0.08166385  0.13413238  0.26303581 -0.29336481\n",
      "  0.27773104  0.03622516  0.28646074]\n",
      "Gradient Descent(31/999): loss=0.5351645129666974, w = [-5.63906242e-01  2.92144518e-01 -1.82714151e-01 -2.01769609e-01\n",
      "  4.66281260e-02  2.05657947e-01  5.73749303e-01 -2.06043261e-05\n",
      "  8.17117294e-02  1.35162539e-01  2.54944648e-01 -3.00381563e-01\n",
      "  2.69679793e-01  3.12056651e-02  2.80417992e-01]\n",
      "Gradient Descent(32/999): loss=0.5297138496324024, w = [-0.57665605  0.2925986  -0.19119738 -0.20654002  0.04243149  0.20790721\n",
      "  0.57032807 -0.00633579  0.08173343  0.13618825  0.24711231 -0.30697893\n",
      "  0.26188967  0.02644931  0.2745855 ]\n",
      "Gradient Descent(33/999): loss=0.5246429393788721, w = [-0.58915389  0.29298609 -0.19927125 -0.21113591  0.03835401  0.21012973\n",
      "  0.56715016 -0.01233742  0.08173377  0.13721287  0.23953185 -0.3131821\n",
      "  0.25435347  0.02193793  0.26895764]\n",
      "Gradient Descent(34/999): loss=0.5199188636954472, w = [-0.60140699  0.29331493 -0.20696258 -0.21556618  0.03439294  0.21232762\n",
      "  0.56420261 -0.01804745  0.08171702  0.13823909  0.23219622 -0.31901485\n",
      "  0.24706391  0.01765481  0.26352849]\n",
      "Gradient Descent(35/999): loss=0.5155119672995366, w = [-0.61342234  0.29359214 -0.21429624 -0.2198395   0.03054575  0.21450266\n",
      "  0.5614729  -0.02348607  0.08168696  0.13926901  0.22509828 -0.32449957\n",
      "  0.24001365  0.01358475  0.25829194]\n",
      "Gradient Descent(36/999): loss=0.51139567043897, w = [-0.62520671  0.29382396 -0.22129525 -0.22396428  0.02681083  0.21665639\n",
      "  0.55894902 -0.02867184  0.0816469   0.14030425  0.2182309  -0.32965736\n",
      "  0.23319541  0.00971462  0.25324174]\n",
      "Gradient Descent(37/999): loss=0.507546498928887, w = [-0.63676662  0.29401589 -0.22798077 -0.22794865  0.02318961  0.21879013\n",
      "  0.55661942 -0.03362187  0.08159974  0.14134595  0.21158699 -0.33450807\n",
      "  0.22660193  0.00603487  0.24837156]\n",
      "Gradient Descent(38/999): loss=0.5039441941277284, w = [-0.64810829  0.2941729  -0.23437198 -0.23180048  0.0196905   0.22090505\n",
      "  0.55447303 -0.03835193  0.08154797  0.14239492  0.20515954 -0.33907044\n",
      "  0.22022605  0.00254273  0.24367498]\n",
      "Gradient Descent(39/999): loss=0.500570746007093, w = [-0.65923768  0.29429945 -0.24048621 -0.23552732  0.01632889  0.22300228\n",
      "  0.55249919 -0.04287663  0.08149365  0.14345159  0.19894166 -0.34336212\n",
      "  0.21406076 -0.00075795  0.23914551]\n",
      "Gradient Descent(40/999): loss=0.49740791633423626, w = [-0.67016054  0.29439962 -0.24634038 -0.23913637  0.01310961  0.22508291\n",
      "  0.55068776 -0.04720945  0.0814385   0.14451616  0.19292652 -0.34739971\n",
      "  0.20809913 -0.00387115  0.23477657]\n",
      "Gradient Descent(41/999): loss=0.4944371048353128, w = [-0.68088253  0.29447704 -0.25195177 -0.24263443  0.01001601  0.22714796\n",
      "  0.54902911 -0.05136269  0.08138397  0.14558857  0.1871074  -0.35119868\n",
      "  0.20233433 -0.00681767  0.23056177]\n",
      "Gradient Descent(42/999): loss=0.4916415512417384, w = [-0.6914091   0.29453483 -0.25733661 -0.24602794  0.00702813  0.22919821\n",
      "  0.54751417 -0.05534768  0.0813314   0.14666862  0.18147772 -0.35477349\n",
      "  0.19675968 -0.00962029  0.22649501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(43/999): loss=0.48900688626912986, w = [-0.70174552  0.29457565 -0.26250939 -0.24932304  0.00413351  0.23123426\n",
      "  0.54613436 -0.05917484  0.08128198  0.14775596  0.17603106 -0.35813765\n",
      "  0.19136871 -0.01229502  0.22257046]\n",
      "Gradient Descent(44/999): loss=0.48652057004148935, w = [-0.71189683  0.29460185 -0.26748312 -0.25252558  0.00132523  0.23325661\n",
      "  0.54488156 -0.06285381  0.08123673  0.14885015  0.1707612  -0.3613038\n",
      "  0.18615512 -0.0148528   0.21878255]\n",
      "Gradient Descent(45/999): loss=0.4841714134439981, w = [-0.72186788  0.29461545 -0.27226971 -0.25564105 -0.00140109  0.23526565\n",
      "  0.54374812 -0.06639349  0.08119657  0.1499507   0.16566209 -0.36428378\n",
      "  0.18111279 -0.01730192  0.21512591]\n",
      "Gradient Descent(46/999): loss=0.48194930900039035, w = [-0.73166334  0.29461827 -0.27688014 -0.25867466 -0.00404875  0.23726175\n",
      "  0.5427268  -0.06980216  0.08116223  0.15105702  0.16072788 -0.36708866\n",
      "  0.17623581 -0.01964939  0.21159537]\n",
      "Gradient Descent(47/999): loss=0.47984507149309935, w = [-0.74128769  0.29461191 -0.28132459 -0.26163132 -0.00662059  0.23924523\n",
      "  0.54181083 -0.07308745  0.08113435  0.1521685   0.15595287 -0.36972879\n",
      "  0.17151843 -0.02190139  0.20818596]\n",
      "Gradient Descent(48/999): loss=0.47785032729192173, w = [-0.75074525  0.29459778 -0.2856125  -0.26451561 -0.00911923  0.24121638\n",
      "  0.54099378 -0.07625645  0.08111344  0.15328451  0.15133157 -0.37221383\n",
      "  0.16695511 -0.02406359  0.2048929 ]\n",
      "Gradient Descent(49/999): loss=0.4759574271208126, w = [-0.7600402   0.29457715 -0.28975266 -0.26733189 -0.01154713  0.24317548\n",
      "  0.54026965 -0.07931574  0.08109991  0.1544044   0.14685864 -0.37455283\n",
      "  0.16254046 -0.02614117  0.20171157]\n",
      "Gradient Descent(50/999): loss=0.47415937237860467, w = [-0.76917656  0.29455113 -0.29375324 -0.27008422 -0.01390668  0.24512277\n",
      "  0.53963277 -0.08227145  0.08109409  0.1555275   0.14252895 -0.37675426\n",
      "  0.1582693  -0.02813895  0.19863758]\n",
      "Gradient Descent(51/999): loss=0.47244975089139385, w = [-0.7781582   0.29452071 -0.29762185 -0.27277644 -0.01620017  0.24705849\n",
      "  0.53907782 -0.08512925  0.08109621  0.15665317  0.13833751 -0.37882601\n",
      "  0.15413662 -0.03006142  0.1956667 ]\n",
      "Gradient Descent(52/999): loss=0.47082268009866546, w = [-0.78698886  0.29448677 -0.30136558 -0.27541212 -0.01842981  0.24898286\n",
      "  0.5385998  -0.08789444  0.08110645  0.15778076  0.13427953 -0.3807755\n",
      "  0.15013758 -0.03191273  0.19279485]\n",
      "Gradient Descent(53/999): loss=0.4692727564929595, w = [-0.79567215  0.29445009 -0.30499104 -0.27799466 -0.02059773  0.25089609\n",
      "  0.53819401 -0.09057194  0.08112493  0.15890961  0.13035037 -0.38260966\n",
      "  0.1462675  -0.03369675  0.19001816]\n",
      "Gradient Descent(54/999): loss=0.46779501047718414, w = [-0.80421156  0.29441136 -0.30850442 -0.28052722 -0.022706    0.25279838\n",
      "  0.53785601 -0.09316634  0.0811517   0.16003911  0.12654556 -0.38433497\n",
      "  0.1425219  -0.03541711  0.18733289]\n",
      "Gradient Descent(55/999): loss=0.46638486597914597, w = [-0.81261045  0.2943712  -0.31191146 -0.28301277 -0.02475659  0.25468992\n",
      "  0.53758167 -0.09568194  0.08118677  0.16116866  0.12286079 -0.38595751\n",
      "  0.13889643 -0.03707718  0.18473548]\n",
      "Gradient Descent(56/999): loss=0.46503810426753434, w = [-0.8208721   0.29433014 -0.31521755 -0.28545412 -0.02675142  0.25657089\n",
      "  0.53736706 -0.09812273  0.08123013  0.16229766  0.11929189 -0.38748299\n",
      "  0.1353869  -0.0386801   0.18222251]\n",
      "Gradient Descent(57/999): loss=0.4637508314894237, w = [-0.82899964  0.29428866 -0.31842775 -0.28785389 -0.02869236  0.25844147\n",
      "  0.53720852 -0.10049245  0.08128169  0.16342556  0.11583487 -0.38891674\n",
      "  0.1319893  -0.04022882  0.17979071]\n",
      "Gradient Descent(58/999): loss=0.4625194495104423, w = [-0.83699613  0.29424719 -0.32154675 -0.29021454 -0.03058117  0.26030184\n",
      "  0.53710259 -0.10279461  0.08134136  0.16455181  0.11248586 -0.39026379\n",
      "  0.12869973 -0.0417261   0.17743696]\n",
      "Gradient Descent(59/999): loss=0.4613406296904711, w = [-0.84486451  0.29420609 -0.32457897 -0.29253838 -0.03241958  0.26215214\n",
      "  0.53704602 -0.10503247  0.08140903  0.16567589  0.10924115 -0.39152884\n",
      "  0.12551446 -0.0431745   0.17515824]\n",
      "Gradient Descent(60/999): loss=0.4602112892709898, w = [-0.85260766  0.2941657  -0.32752856 -0.2948276  -0.03420925  0.26399255\n",
      "  0.53703577 -0.10720911  0.08148454  0.16679731  0.10609714 -0.39271631\n",
      "  0.12242988 -0.04457646  0.1729517 ]\n",
      "Gradient Descent(61/999): loss=0.45912857008944036, w = [-0.86022833  0.29412629 -0.3303994  -0.29708423 -0.03595178  0.26582322\n",
      "  0.53706895 -0.10932741  0.08156773  0.16791561  0.10305038 -0.39383035\n",
      "  0.11944253 -0.04593422  0.17081459]\n",
      "Gradient Descent(62/999): loss=0.4580898193692105, w = [-0.86772922  0.29408811 -0.33319512 -0.29931018 -0.0376487   0.2676443\n",
      "  0.53714287 -0.11139007  0.08165842  0.16903034  0.10009755 -0.39487485\n",
      "  0.11654906 -0.04724992  0.16874429]\n",
      "Gradient Descent(63/999): loss=0.4570925723629697, w = [-0.87511293  0.29405139 -0.33591914 -0.30150726 -0.03930149  0.26945594\n",
      "  0.537255   -0.11339963  0.0817564   0.17014108  0.09723545 -0.39585348\n",
      "  0.11374626 -0.04852553  0.16673828]\n",
      "Gradient Descent(64/999): loss=0.45613453665317866, w = [-0.88238197  0.29401631 -0.33857468 -0.30367715 -0.04091159  0.27125829\n",
      "  0.53740293 -0.11535847  0.08186148  0.17124744  0.09446099 -0.3967697\n",
      "  0.11103103 -0.04976294  0.16479415]\n",
      "Gradient Descent(65/999): loss=0.4552135779356903, w = [-0.88953882  0.29398303 -0.34116476 -0.30582145 -0.04248037  0.27305147\n",
      "  0.53758443 -0.11726884  0.08197343  0.17234906  0.09177119 -0.39762674\n",
      "  0.10840038 -0.0509639   0.16290961]\n",
      "Gradient Descent(66/999): loss=0.4543277071323014, w = [-0.89658583  0.29395168 -0.34369223 -0.30794163 -0.04400916  0.27483564\n",
      "  0.53779739 -0.11913285  0.08209203  0.17344559  0.0891632  -0.39842765\n",
      "  0.10585145 -0.05213006  0.16108245]\n",
      "Gradient Descent(67/999): loss=0.45347506869537746, w = [-0.90352532  0.29392239 -0.34615974 -0.3100391  -0.04549923  0.27661094\n",
      "  0.53803981 -0.12095248  0.08221705  0.1745367   0.08663426 -0.39917531\n",
      "  0.10338146 -0.05326297  0.15931057]\n",
      "Gradient Descent(68/999): loss=0.45265392998304144, w = [-0.91035955  0.29389524 -0.34856984 -0.31211518 -0.04695183  0.27837748\n",
      "  0.53830984 -0.12272963  0.08234825  0.1756221   0.08418171 -0.39987243\n",
      "  0.10098774 -0.05436411  0.15759194]\n",
      "Gradient Descent(69/999): loss=0.4518626715965711, w = [-0.91709068  0.29387032 -0.35092489 -0.31417109 -0.04836812  0.2801354\n",
      "  0.5386057  -0.12446605  0.08248539  0.17670151  0.081803   -0.40052155\n",
      "  0.09866774 -0.05543484  0.15592463]\n",
      "Gradient Descent(70/999): loss=0.4510997785837546, w = [-0.92372083  0.2938477  -0.35322713 -0.31620799 -0.04974926  0.28188484\n",
      "  0.53892576 -0.12616343  0.08262823  0.17777468  0.07949565 -0.40112507\n",
      "  0.09641898 -0.05647648  0.15430679]\n",
      "Gradient Descent(71/999): loss=0.4503638324223642, w = [-0.93025208  0.29382742 -0.35547868 -0.31822698 -0.05109635  0.28362591\n",
      "  0.53926844 -0.12782334  0.08277654  0.17884136  0.0772573  -0.40168527\n",
      "  0.09423907 -0.05749023  0.15273665]\n",
      "Gradient Descent(72/999): loss=0.4496535037069071, w = [-0.93668641  0.29380952 -0.35768154 -0.32022907 -0.05241044  0.28535873\n",
      "  0.5396323  -0.12944727  0.08293007  0.17990133  0.07508566 -0.40220427\n",
      "  0.09212572 -0.05847727  0.1512125 ]\n",
      "Gradient Descent(73/999): loss=0.44896754547035617, w = [-0.94302579  0.29379403 -0.35983759 -0.32221522 -0.05369256  0.28708344\n",
      "  0.54001595 -0.13103664  0.08308857  0.18095441  0.07297852 -0.4026841\n",
      "  0.09007673 -0.05943867  0.14973273]\n",
      "Gradient Descent(74/999): loss=0.44830478707951366, w = [-0.94927211  0.29378096 -0.36194861 -0.32418634 -0.0549437   0.28880015\n",
      "  0.54041809 -0.13259279  0.08325182  0.18200041  0.07093376 -0.40312665\n",
      "  0.08808996 -0.06037547  0.14829578]\n",
      "Gradient Descent(75/999): loss=0.44766412864916166, w = [-0.95542721  0.29377033 -0.3640163  -0.32614327 -0.0561648   0.29050896\n",
      "  0.54083752 -0.13411699  0.08341957  0.18303916  0.06894933 -0.40353373\n",
      "  0.08616335 -0.06128866  0.14690015]\n",
      "Gradient Descent(76/999): loss=0.4470445359260037, w = [-0.96149289  0.29376213 -0.36604225 -0.3280868  -0.05735679  0.29221001\n",
      "  0.54127308 -0.13561044  0.0835916   0.18407052  0.06702326 -0.40390702\n",
      "  0.08429493 -0.06217913  0.1455444 ]\n",
      "Gradient Descent(77/999): loss=0.4464450355981739, w = [-0.96747091  0.29375636 -0.36802797 -0.33001769 -0.05852053  0.2939034\n",
      "  0.54172369 -0.13707428  0.08376766  0.18509434  0.06515365 -0.40424814\n",
      "  0.08248279 -0.06304778  0.14422718]\n",
      "Gradient Descent(78/999): loss=0.4458647109909218, w = [-0.97336296  0.293753   -0.3699749  -0.33193663 -0.05965688  0.29558924\n",
      "  0.54218833 -0.13850959  0.08394754  0.18611052  0.06333866 -0.4045586\n",
      "  0.0807251  -0.06389541  0.14294715]\n",
      "Gradient Descent(79/999): loss=0.44530269811280543, w = [-0.97917072  0.29375204 -0.3718844  -0.33384428 -0.06076667  0.29726763\n",
      "  0.54266606 -0.13991741  0.084131    0.18711895  0.06157652 -0.40483982\n",
      "  0.07902007 -0.06472282  0.14170306]\n",
      "Gradient Descent(80/999): loss=0.4447581820205434, w = [-0.9848958   0.29375346 -0.37375775 -0.33574127 -0.06185069  0.29893869\n",
      "  0.54315598 -0.14129871  0.08431784  0.18811952  0.05986554 -0.40509318\n",
      "  0.07736601 -0.06553075  0.14049369]\n",
      "Gradient Descent(81/999): loss=0.4442303934736526, w = [-0.99053977  0.29375722 -0.37559618 -0.33762817 -0.06290969  0.30060252\n",
      "  0.54365723 -0.14265442  0.08450784  0.18911217  0.05820406 -0.40531994\n",
      "  0.07576126 -0.06631989  0.13931789]\n",
      "Gradient Descent(82/999): loss=0.44371860585302064, w = [-0.99610418  0.29376329 -0.37740086 -0.33950553 -0.06394441  0.30225922\n",
      "  0.54416902 -0.14398543  0.08470078  0.19009682  0.0565905  -0.40552133\n",
      "  0.07420424 -0.06709091  0.13817453]\n",
      "Gradient Descent(83/999): loss=0.4432221323199932, w = [-1.00159053  0.29377164 -0.37917288 -0.34137387 -0.06495557  0.3039089\n",
      "  0.5446906  -0.14529258  0.08489647  0.19107341  0.05502333 -0.40569849\n",
      "  0.07269341 -0.06784445  0.13706255]\n",
      "Gradient Descent(84/999): loss=0.4427403231948598, w = [-1.00700028  0.29378224 -0.3809133  -0.34323366 -0.06594384  0.30555164\n",
      "  0.54522127 -0.14657667  0.08509469  0.19204189  0.05350107 -0.40585251\n",
      "  0.07122728 -0.0685811   0.13598092]\n",
      "Gradient Descent(85/999): loss=0.4422725635357005, w = [-1.01233486  0.29379503 -0.38262312 -0.34508536 -0.0669099   0.30718755\n",
      "  0.54576035 -0.14783847  0.08529527  0.19300223  0.0520223  -0.40598442\n",
      "  0.06980445 -0.06930144  0.13492865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(86/999): loss=0.4418182709003109, w = [-1.01759565  0.29380999 -0.38430329 -0.34692939 -0.06785437  0.30881672\n",
      "  0.54630724 -0.14907872  0.085498    0.1939544   0.05058566 -0.4060952\n",
      "  0.06842352 -0.070006    0.13390479]\n",
      "Gradient Descent(87/999): loss=0.44137689327561963, w = [-1.02278403  0.29382705 -0.38595472 -0.34876616 -0.06877788  0.31043925\n",
      "  0.54686133 -0.1502981   0.08570271  0.19489837  0.0491898  -0.40618578\n",
      "  0.06708318 -0.0706953   0.13290842]\n",
      "Gradient Descent(88/999): loss=0.44094790716042354, w = [-1.0279013   0.29384619 -0.38757828 -0.35059604 -0.06968101  0.31205522\n",
      "  0.54742208 -0.15149729  0.08590921  0.19583412  0.04783345 -0.40625704\n",
      "  0.06578213 -0.07136983  0.13193868]\n",
      "Gradient Descent(89/999): loss=0.440530815788647, w = [-1.03294878  0.29386735 -0.38917477 -0.35241938 -0.07056435  0.31366473\n",
      "  0.54798896 -0.15267692  0.08611733  0.19676165  0.04651538 -0.4063098\n",
      "  0.06451914 -0.07203006  0.13099472]\n",
      "Gradient Descent(90/999): loss=0.44012514748146103, w = [-1.03792772  0.29389047 -0.390745   -0.35423651 -0.07142844  0.31526787\n",
      "  0.54856148 -0.15383759  0.0863269   0.19768097  0.04523439 -0.40634486\n",
      "  0.06329302 -0.07267643  0.13007572]\n",
      "Gradient Descent(91/999): loss=0.43973045411769157, w = [-1.04283936  0.29391552 -0.39228971 -0.35604775 -0.07227382  0.31686472\n",
      "  0.54913919 -0.1549799   0.08653776  0.19859207  0.04398933 -0.40636298\n",
      "  0.06210261 -0.07330936  0.12918091]\n",
      "Gradient Descent(92/999): loss=0.4393463097129192, w = [-1.0476849   0.29394245 -0.39380962 -0.35785339 -0.07310102  0.31845536\n",
      "  0.54972164 -0.1561044   0.08674974  0.19949497  0.04277908 -0.40636487\n",
      "  0.0609468  -0.07392926  0.12830955]\n",
      "Gradient Descent(93/999): loss=0.4389723090984907, w = [-1.05246553  0.29397119 -0.39530541 -0.3596537  -0.07391052  0.32003988\n",
      "  0.55030844 -0.15721162  0.0869627   0.20038968  0.04160259 -0.4063512\n",
      "  0.05982451 -0.07453652  0.12746089]\n",
      "Gradient Descent(94/999): loss=0.43860806669249247, w = [-1.05718238  0.29400171 -0.39677775 -0.36144894 -0.07470282  0.32161836\n",
      "  0.55089919 -0.15830207  0.08717649  0.20127623  0.04045879 -0.40632261\n",
      "  0.05873471 -0.07513149  0.12663427]\n",
      "Gradient Descent(95/999): loss=0.43825321535544853, w = [-1.06183659  0.29403395 -0.39822724 -0.36323935 -0.07547837  0.32319088\n",
      "  0.55149353 -0.15937624  0.08739096  0.20215465  0.03934671 -0.40627974\n",
      "  0.05767638 -0.07571454  0.12582899]\n",
      "Gradient Descent(96/999): loss=0.4379074053240475, w = [-1.06642925  0.29406785 -0.39965451 -0.36502515 -0.07623764  0.32475753\n",
      "  0.55209112 -0.1604346   0.08760597  0.20302497  0.03826535 -0.40622314\n",
      "  0.05664857 -0.07628599  0.12504442]\n",
      "Gradient Descent(97/999): loss=0.4375703032169186, w = [-1.07096144  0.29410338 -0.40106011 -0.36680657 -0.07698105  0.32631837\n",
      "  0.55269165 -0.16147761  0.0878214   0.20388722  0.0372138  -0.40615339\n",
      "  0.05565033 -0.07684617  0.12427994]\n",
      "Gradient Descent(98/999): loss=0.4372415911068961, w = [-1.0754342   0.29414047 -0.40244461 -0.36858381 -0.07770903  0.32787348\n",
      "  0.55329481 -0.16250569  0.0880371   0.20474144  0.03619114 -0.406071\n",
      "  0.05468075 -0.07739538  0.12353496]\n",
      "Gradient Descent(99/999): loss=0.436920965654704, w = [-1.07984857  0.29417907 -0.40380852 -0.37035704 -0.07842198  0.32942295\n",
      "  0.55390032 -0.16351926  0.08825297  0.20558767  0.03519651 -0.40597648\n",
      "  0.05373897 -0.07793392  0.12280888]\n",
      "Gradient Descent(100/999): loss=0.43660813729945885, w = [-1.08420554  0.29421914 -0.40515236 -0.37212645 -0.07912031  0.33096683\n",
      "  0.55450791 -0.16451872  0.08846887  0.20642597  0.03422905 -0.40587031\n",
      "  0.05282413 -0.07846206  0.12210117]\n",
      "Gradient Descent(101/999): loss=0.4363028295017385, w = [-1.0885061   0.29426063 -0.40647661 -0.3738922  -0.07980438  0.33250521\n",
      "  0.55511734 -0.16550445  0.08868469  0.20725638  0.03328794 -0.40575295\n",
      "  0.05193542 -0.07898008  0.12141128]\n",
      "Gradient Descent(102/999): loss=0.4360047780353212, w = [-1.0927512   0.29430348 -0.40778174 -0.37565446 -0.08047458  0.33403815\n",
      "  0.55572836 -0.16647683  0.08890033  0.20807895  0.03237241 -0.40562482\n",
      "  0.05107205 -0.07948823  0.1207387 ]\n",
      "Gradient Descent(103/999): loss=0.43571373032404176, w = [-1.09694179  0.29434764 -0.40906819 -0.37741335 -0.08113125  0.33556572\n",
      "  0.55634075 -0.1674362   0.08911567  0.20889374  0.03148168 -0.40548634\n",
      "  0.05023325 -0.07998675  0.12008292]\n",
      "Gradient Descent(104/999): loss=0.43542944482047663, w = [-1.10107878  0.29439307 -0.41033641 -0.37916903 -0.08177475  0.33708799\n",
      "  0.55695432 -0.16838291  0.08933062  0.2097008   0.03061502 -0.40533791\n",
      "  0.04941829 -0.08047589  0.11944348]\n",
      "Gradient Descent(105/999): loss=0.43515169042347496, w = [-1.10516306  0.29443972 -0.41158679 -0.38092161 -0.0824054   0.33860503\n",
      "  0.55756885 -0.16931728  0.08954507  0.21050019  0.02977171 -0.4051799\n",
      "  0.04862645 -0.08095587  0.11881989]\n",
      "Gradient Descent(106/999): loss=0.43488024593170904, w = [-1.10919553  0.29448754 -0.41281974 -0.38267122 -0.08302354  0.3401169\n",
      "  0.55818418 -0.17023964  0.08975893  0.21129198  0.02895105 -0.40501267\n",
      "  0.04785703 -0.0814269   0.11821173]\n",
      "Gradient Descent(107/999): loss=0.4346148995307845, w = [-1.11317704  0.29453649 -0.41403565 -0.38441797 -0.08362947  0.34162367\n",
      "  0.55880012 -0.17115028  0.08997211  0.21207621  0.0281524  -0.40483656\n",
      "  0.04710937 -0.08188921  0.11761855]\n",
      "Gradient Descent(108/999): loss=0.4343554483114779, w = [-1.11710843  0.29458651 -0.41523488 -0.38616197 -0.0842235   0.34312539\n",
      "  0.55941652 -0.17204951  0.09018453  0.21285297  0.02737508 -0.4046519\n",
      "  0.04638283 -0.08234298  0.11703993]\n",
      "Gradient Descent(109/999): loss=0.4341016978169899, w = [-1.12099053  0.29463756 -0.4164178  -0.38790332 -0.08480592  0.34462214\n",
      "  0.56003322 -0.17293761  0.0903961   0.2136223   0.02661849 -0.40445901\n",
      "  0.04567677 -0.08278841  0.11647548]\n",
      "Gradient Descent(110/999): loss=0.43385346161720406, w = [-1.12482413  0.2946896  -0.41758473 -0.3896421  -0.08537702  0.34611397\n",
      "  0.56065008 -0.17381485  0.09060674  0.21438428  0.025882   -0.40425818\n",
      "  0.04499059 -0.08322569  0.1159248 ]\n",
      "Gradient Descent(111/999): loss=0.43361056090812516, w = [-1.12861004  0.29474259 -0.41873603 -0.39137841 -0.08593707  0.34760095\n",
      "  0.56126697 -0.17468149  0.09081638  0.21513897  0.02516505 -0.4040497\n",
      "  0.0443237  -0.08365499  0.11538752]\n",
      "Gradient Descent(112/999): loss=0.4333728241347571, w = [-1.13234903  0.29479647 -0.419872   -0.39311232 -0.08648634  0.34908312\n",
      "  0.56188375 -0.1755378   0.09102495  0.21588645  0.02446705 -0.40383384\n",
      "  0.04367554 -0.08407649  0.11486328]\n",
      "Gradient Descent(113/999): loss=0.43314008663589576, w = [-1.13604184  0.29485121 -0.42099295 -0.39484391 -0.08702509  0.35056055\n",
      "  0.56250033 -0.17638401  0.09123237  0.21662677  0.02378746 -0.40361086\n",
      "  0.04304555 -0.08449036  0.11435171]\n",
      "Gradient Descent(114/999): loss=0.4329121903093559, w = [-1.13968924  0.29490676 -0.42209919 -0.39657325 -0.08755358  0.3520333\n",
      "  0.56311658 -0.17722037  0.09143858  0.21736001  0.02312574 -0.40338101\n",
      "  0.0424332  -0.08489674  0.11385248]\n",
      "Gradient Descent(115/999): loss=0.4326889832962955, w = [-1.14329193  0.29496309 -0.42319101 -0.39830042 -0.08807204  0.35350142\n",
      "  0.5637324  -0.17804711  0.09164352  0.21808624  0.02248139 -0.40314453\n",
      "  0.04183799 -0.0852958   0.11336526]\n",
      "Gradient Descent(116/999): loss=0.43247031968336963, w = [-1.14685064  0.29502015 -0.42426867 -0.40002546 -0.0885807   0.35496496\n",
      "  0.5643477  -0.17886444  0.09184713  0.21880552  0.0218539  -0.40290165\n",
      "  0.0412594  -0.08568769  0.11288974]\n",
      "Gradient Descent(117/999): loss=0.43225605922161825, w = [-1.15036606  0.29507791 -0.42533245 -0.40174844 -0.08907981  0.35642398\n",
      "  0.56496238 -0.17967259  0.09204935  0.21951794  0.02124279 -0.40265259\n",
      "  0.04069696 -0.08607254  0.1124256 ]\n",
      "Gradient Descent(118/999): loss=0.4320460670609252, w = [-1.15383887  0.29513632 -0.42638262 -0.40346941 -0.08956958  0.35787853\n",
      "  0.56557636 -0.18047177  0.09225012  0.22022355  0.02064759 -0.40239756\n",
      "  0.04015019 -0.0864505   0.11197254]\n",
      "Gradient Descent(119/999): loss=0.4318402134991399, w = [-1.15726974  0.29519535 -0.42741942 -0.40518842 -0.09005023  0.35932867\n",
      "  0.56618957 -0.18126217  0.09244941  0.22092243  0.02006785 -0.40213675\n",
      "  0.03961865 -0.0868217   0.11153029]\n",
      "Gradient Descent(120/999): loss=0.43163837374489755, w = [-1.16065934  0.29525496 -0.4284431  -0.40690553 -0.09052196  0.36077444\n",
      "  0.56680192 -0.18204399  0.09264715  0.22161465  0.01950312 -0.40187036\n",
      "  0.03910189 -0.08718627  0.11109855]\n",
      "Gradient Descent(121/999): loss=0.43144042769327134, w = [-1.16400829  0.29531512 -0.4294539  -0.40862076 -0.09098498  0.36221589\n",
      "  0.56741336 -0.18281743  0.09284331  0.22230028  0.01895299 -0.40159858\n",
      "  0.0385995  -0.08754433  0.11067705]\n",
      "Gradient Descent(122/999): loss=0.4312462597134907, w = [-1.16731724  0.29537578 -0.43045205 -0.41033417 -0.09143948  0.36365308\n",
      "  0.56802381 -0.18358266  0.09303783  0.2229794   0.01841703 -0.40132158\n",
      "  0.03811105 -0.08789601  0.11026554]\n",
      "Gradient Descent(123/999): loss=0.43105575844795396, w = [-1.17058681  0.29543691 -0.43143776 -0.41204579 -0.09188566  0.36508605\n",
      "  0.56863322 -0.18433987  0.09323069  0.22365206  0.01789486 -0.40103954\n",
      "  0.03763615 -0.08824142  0.10986376]\n",
      "Gradient Descent(124/999): loss=0.4308688166218694, w = [-1.17381758  0.29549849 -0.43241127 -0.41375565 -0.0923237   0.36651484\n",
      "  0.56924153 -0.18508922  0.09342184  0.22431835  0.01738608 -0.40075261\n",
      "  0.03717441 -0.08858067  0.10947146]\n",
      "Gradient Descent(125/999): loss=0.43068533086285915, w = [-1.17701017  0.29556047 -0.43337277 -0.41546379 -0.09275378  0.36793952\n",
      "  0.56984869 -0.18583088  0.09361124  0.22497833  0.01689031 -0.40046095\n",
      "  0.03672546 -0.08891387  0.1090884 ]\n",
      "Gradient Descent(126/999): loss=0.43050520152994215, w = [-1.18016516  0.29562283 -0.43432248 -0.41717024 -0.09317608  0.36936011\n",
      "  0.57045465 -0.18656502  0.09379886  0.22563208  0.0164072  -0.40016471\n",
      "  0.03628893 -0.08924114  0.10871436]\n",
      "Gradient Descent(127/999): loss=0.4303283325513362, w = [-1.18328311  0.29568553 -0.43526058 -0.41887503 -0.09359076  0.37077667\n",
      "  0.57105937 -0.1872918   0.09398467  0.22627966  0.01593638 -0.39986404\n",
      "  0.03586447 -0.08956257  0.10834911]\n",
      "Gradient Descent(128/999): loss=0.43015463127054904, w = [-1.18636458  0.29574854 -0.43618728 -0.42057817 -0.093998    0.37218924\n",
      "  0.57166281 -0.18801136  0.09416863  0.22692114  0.01547753 -0.39955908\n",
      "  0.03545174 -0.08987826  0.10799242]\n",
      "Gradient Descent(129/999): loss=0.42998400830026556, w = [-1.18941013  0.29581184 -0.43710277 -0.42227971 -0.09439794  0.37359786\n",
      "  0.57226493 -0.18872386  0.09435073  0.22755659  0.01503029 -0.39924996\n",
      "  0.0350504  -0.09018831  0.1076441 ]\n",
      "Gradient Descent(130/999): loss=0.4298163773836174, w = [-1.19242029  0.29587538 -0.43800721 -0.42397965 -0.09479075  0.37500258\n",
      "  0.57286569 -0.18942945  0.09453093  0.22818609  0.01459436 -0.3989368\n",
      "  0.03466014 -0.09049281  0.10730392]\n",
      "Gradient Descent(131/999): loss=0.42965165526234333, w = [-1.19539559  0.29593915 -0.4389008  -0.42567802 -0.09517657  0.37640344\n",
      "  0.57346506 -0.19012825  0.09470922  0.22880969  0.01416942 -0.39861974\n",
      "  0.03428064 -0.09079186  0.10697171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(132/999): loss=0.429489761551516, w = [-1.19833656  0.29600311 -0.4397837  -0.42737484 -0.09555556  0.37780048\n",
      "  0.57406302 -0.19082041  0.09488556  0.22942747  0.01375517 -0.39829889\n",
      "  0.0339116  -0.09108554  0.10664725]\n",
      "Gradient Descent(133/999): loss=0.4293306186203876, w = [-1.2012437   0.29606724 -0.44065608 -0.42907013 -0.09592786  0.37919374\n",
      "  0.57465953 -0.19150607  0.09505994  0.23003949  0.01335132 -0.39797435\n",
      "  0.03355273 -0.09137394  0.10633037]\n",
      "Gradient Descent(134/999): loss=0.4291741514790907, w = [-1.2041175   0.2961315  -0.44151812 -0.43076389 -0.0962936   0.38058326\n",
      "  0.57525457 -0.19218534  0.09523233  0.23064583  0.01295757 -0.39764626\n",
      "  0.03320375 -0.09165714  0.10602088]\n",
      "Gradient Descent(135/999): loss=0.42902028767078754, w = [-1.20695847  0.29619589 -0.44236995 -0.43245615 -0.09665292  0.38196908\n",
      "  0.57584812 -0.19285836  0.09540273  0.23124654  0.01257366 -0.3973147\n",
      "  0.03286437 -0.09193522  0.1057186 ]\n",
      "Gradient Descent(136/999): loss=0.4288689571690157, w = [-1.20976707  0.29626036 -0.44321175 -0.43414692 -0.09700596  0.38335124\n",
      "  0.57644015 -0.19352525  0.09557111  0.23184169  0.01219931 -0.39697979\n",
      "  0.03253433 -0.09220828  0.10542337]\n",
      "Gradient Descent(137/999): loss=0.42872009227992364, w = [-1.21254379  0.2963249  -0.44404367 -0.43583621 -0.09735283  0.38472978\n",
      "  0.57703066 -0.19418613  0.09573746  0.23243135  0.01183426 -0.39664162\n",
      "  0.03221337 -0.09247637  0.10513502]\n",
      "Gradient Descent(138/999): loss=0.42857362754914596, w = [-1.21528909  0.29638947 -0.44486584 -0.43752403 -0.09769366  0.38610473\n",
      "  0.57761961 -0.19484111  0.09590177  0.23301559  0.01147826 -0.39630028\n",
      "  0.03190124 -0.09273958  0.10485337]\n",
      "Gradient Descent(139/999): loss=0.42842949967301824, w = [-1.21800342  0.29645406 -0.44567843 -0.43921039 -0.09802858  0.38747614\n",
      "  0.578207   -0.19549031  0.09606402  0.23359446  0.01113108 -0.39595587\n",
      "  0.03159769 -0.09299797  0.10457829]\n",
      "Gradient Descent(140/999): loss=0.4282876474139791, w = [-1.22068723  0.29651865 -0.44648156 -0.4408953  -0.0983577   0.38884403\n",
      "  0.57879281 -0.19613384  0.09622421  0.23416803  0.01079246 -0.39560848\n",
      "  0.03130249 -0.09325163  0.10430961]\n",
      "Gradient Descent(141/999): loss=0.42814801151983195, w = [-1.22334095  0.2965832  -0.44727538 -0.44257877 -0.09868114  0.39020844\n",
      "  0.57937703 -0.1967718   0.09638233  0.23473637  0.01046217 -0.39525818\n",
      "  0.0310154  -0.09350062  0.10404719]\n",
      "Gradient Descent(142/999): loss=0.4280105346467726, w = [-1.22596503  0.29664771 -0.44806001 -0.4442608  -0.098999    0.39156941\n",
      "  0.57995965 -0.1974043   0.09653836  0.23529953  0.01014    -0.39490507\n",
      "  0.03073621 -0.09374501  0.10379087]\n",
      "Gradient Descent(143/999): loss=0.42787516128587083, w = [-1.22855987  0.29671214 -0.44883561 -0.44594141 -0.0993114   0.39292698\n",
      "  0.58054066 -0.19803144  0.0966923   0.23585757  0.00982573 -0.39454922\n",
      "  0.03046469 -0.09398486  0.10354052]\n",
      "Gradient Descent(144/999): loss=0.4277418376929322, w = [-1.2311259   0.29677647 -0.44960228 -0.44762059 -0.09961844  0.39428117\n",
      "  0.58112005 -0.19865332  0.09684415  0.23641057  0.00951914 -0.39419071\n",
      "  0.03020064 -0.09422024  0.10329601]\n",
      "Gradient Descent(145/999): loss=0.4276105118214683, w = [-1.23366352  0.2968407  -0.45036016 -0.44929836 -0.09992022  0.39563202\n",
      "  0.58169781 -0.19927003  0.0969939   0.23695857  0.00922003 -0.39382962\n",
      "  0.02994384 -0.09445122  0.1030572 ]\n",
      "Gradient Descent(146/999): loss=0.427481133258692, w = [-1.23617314  0.29690479 -0.45110937 -0.45097471 -0.10021685  0.39697957\n",
      "  0.58227395 -0.19988168  0.09714154  0.23750163  0.00892819 -0.39346601\n",
      "  0.02969411 -0.09467784  0.10282395]\n",
      "Gradient Descent(147/999): loss=0.4273536531643285, w = [-1.23865515  0.29696873 -0.45185003 -0.45264965 -0.10050842  0.39832384\n",
      "  0.58284844 -0.20048835  0.09728708  0.23803983  0.00864345 -0.39309995\n",
      "  0.02945125 -0.09490019  0.10259615]\n",
      "Gradient Descent(148/999): loss=0.42722802421213024, w = [-1.24110994  0.2970325  -0.45258226 -0.45432319 -0.10079502  0.39966487\n",
      "  0.5834213  -0.20109014  0.09743051  0.2385732   0.0083656  -0.39273151\n",
      "  0.02921506 -0.0951183   0.10237366]\n",
      "Gradient Descent(149/999): loss=0.4271042005339357, w = [-1.24353787  0.29709608 -0.45330617 -0.45599532 -0.10107675  0.40100269\n",
      "  0.58399251 -0.20168712  0.09757184  0.23910182  0.00809446 -0.39236076\n",
      "  0.02898538 -0.09533225  0.10215638]\n",
      "Gradient Descent(150/999): loss=0.426982137666176, w = [-1.24593934  0.29715946 -0.45402188 -0.45766605 -0.1013537   0.40233733\n",
      "  0.58456208 -0.20227939  0.09771106  0.23962573  0.00782986 -0.39198775\n",
      "  0.02876201 -0.09554208  0.10194418]\n",
      "Gradient Descent(151/999): loss=0.4268617924986579, w = [-1.2483147   0.29722261 -0.45472949 -0.45933538 -0.10162596  0.40366882\n",
      "  0.58513    -0.20286704  0.09784817  0.240145    0.00757163 -0.39161255\n",
      "  0.0285448  -0.09574785  0.10173696]\n",
      "Gradient Descent(152/999): loss=0.42674312322556857, w = [-1.25066432  0.29728552 -0.45542912 -0.4610033  -0.10189361  0.40499719\n",
      "  0.58569628 -0.20345013  0.09798317  0.24065968  0.0073196  -0.39123522\n",
      "  0.02833358 -0.09594963  0.10153459]\n",
      "Gradient Descent(153/999): loss=0.4266260892985384, w = [-1.25298855  0.29734817 -0.45612087 -0.46266984 -0.10215673  0.40632247\n",
      "  0.58626091 -0.20402876  0.09811607  0.24116982  0.00707359 -0.39085582\n",
      "  0.02812817 -0.09614744  0.10133697]\n",
      "Gradient Descent(154/999): loss=0.42651065138170585, w = [-1.25528773  0.29741056 -0.45680484 -0.46433497 -0.10241542  0.40764469\n",
      "  0.58682389 -0.204603    0.09824688  0.24167548  0.00683347 -0.39047439\n",
      "  0.02792843 -0.09634137  0.10114401]\n",
      "Gradient Descent(155/999): loss=0.4263967713086438, w = [-1.25756222  0.29747265 -0.45748114 -0.4659987  -0.10266973  0.40896388\n",
      "  0.58738523 -0.20517293  0.09837558  0.24217672  0.00659907 -0.390091\n",
      "  0.0277342  -0.09653144  0.10095559]\n",
      "Gradient Descent(156/999): loss=0.42628441204111384, w = [-1.25981234  0.29753445 -0.45814987 -0.46766104 -0.10291976  0.41028006\n",
      "  0.58794492 -0.20573861  0.0985022   0.24267357  0.00637024 -0.38970569\n",
      "  0.02754533 -0.09671771  0.10077161]\n",
      "Gradient Descent(157/999): loss=0.42617353762948895, w = [-1.26203843  0.29759592 -0.45881112 -0.46932198 -0.10316558  0.41159327\n",
      "  0.58850298 -0.20630014  0.09862673  0.24316611  0.00614685 -0.38931851\n",
      "  0.02736169 -0.09690023  0.10059199]\n",
      "Gradient Descent(158/999): loss=0.42606411317482573, w = [-1.26424081  0.29765707 -0.459465   -0.47098152 -0.10340726  0.41290352\n",
      "  0.5890594  -0.20685756  0.09874918  0.24365437  0.00592873 -0.38892951\n",
      "  0.02718312 -0.09707905  0.10041662]\n",
      "Gradient Descent(159/999): loss=0.42595610479249096, w = [-1.2664198   0.29771787 -0.46011159 -0.47263966 -0.10364488  0.41421086\n",
      "  0.58961419 -0.20741097  0.09886956  0.24413841  0.00571577 -0.38853875\n",
      "  0.02700949 -0.09725421  0.10024541]\n",
      "Gradient Descent(160/999): loss=0.4258494795772396, w = [-1.26857572  0.29777832 -0.46075098 -0.4742964  -0.1038785   0.4155153\n",
      "  0.59016735 -0.20796041  0.09898787  0.24461828  0.00550783 -0.38814626\n",
      "  0.02684068 -0.09742576  0.10007827]\n",
      "Gradient Descent(161/999): loss=0.42574420556972775, w = [-1.27070887  0.2978384  -0.46138327 -0.47595174 -0.10410819  0.41681687\n",
      "  0.59071889 -0.20850597  0.09910413  0.24509403  0.00530478 -0.38775209\n",
      "  0.02667654 -0.09759375  0.09991512]\n",
      "Gradient Descent(162/999): loss=0.42564025172434805, w = [-1.27281956  0.29789809 -0.46200856 -0.47760567 -0.10433402  0.41811559\n",
      "  0.59126881 -0.20904771  0.09921833  0.24556571  0.00510648 -0.38735628\n",
      "  0.02651696 -0.09775821  0.09975587]\n",
      "Gradient Descent(163/999): loss=0.4255375878783514, w = [-1.27490809  0.29795739 -0.46262691 -0.4792582  -0.10455605  0.4194115\n",
      "  0.59181712 -0.20958568  0.09933049  0.24603336  0.00491283 -0.38695888\n",
      "  0.02636182 -0.0979192   0.09960043]\n",
      "Gradient Descent(164/999): loss=0.4254361847221919, w = [-1.27697475  0.29801628 -0.46323843 -0.48090932 -0.10477436  0.42070462\n",
      "  0.59236381 -0.21011995  0.09944062  0.24649704  0.0047237  -0.38655992\n",
      "  0.02621099 -0.09807675  0.09944873]\n",
      "Gradient Descent(165/999): loss=0.4253360137710398, w = [-1.27901982  0.29807475 -0.46384319 -0.48255903 -0.10498899  0.42199496\n",
      "  0.59290891 -0.21065059  0.09954872  0.24695679  0.00453897 -0.38615945\n",
      "  0.02606436 -0.0982309   0.09930069]\n",
      "Gradient Descent(166/999): loss=0.42523704733738565, w = [-1.2810436   0.2981328  -0.46444128 -0.48420733 -0.10520001  0.42328257\n",
      "  0.59345241 -0.21117765  0.09965481  0.24741265  0.00435853 -0.3857575\n",
      "  0.02592182 -0.09838171  0.09915622]\n",
      "Gradient Descent(167/999): loss=0.42513925850474765, w = [-1.28304637  0.2981904  -0.46503278 -0.48585421 -0.10540749  0.42456745\n",
      "  0.59399432 -0.2117012   0.0997589   0.24786468  0.00418228 -0.38535411\n",
      "  0.02578326 -0.0985292   0.09901525]\n",
      "Gradient Descent(168/999): loss=0.42504262110234947, w = [-1.28502839  0.29824756 -0.46561777 -0.48749967 -0.10561147  0.42584963\n",
      "  0.59453465 -0.21222128  0.09986099  0.24831291  0.0040101  -0.38494931\n",
      "  0.02564858 -0.09867342  0.09887771]\n",
      "Gradient Descent(169/999): loss=0.42494710968080157, w = [-1.28698994  0.29830425 -0.46619633 -0.48914372 -0.10581201  0.42712915\n",
      "  0.5950734  -0.21273796  0.0999611   0.24875739  0.00384189 -0.38454315\n",
      "  0.02551767 -0.0988144   0.09874353]\n",
      "Gradient Descent(170/999): loss=0.42485269948869386, w = [-1.28893129  0.29836048 -0.46676853 -0.49078635 -0.10600918  0.42840601\n",
      "  0.59561058 -0.2132513   0.10005924  0.24919818  0.00367756 -0.38413566\n",
      "  0.02539043 -0.09895219  0.09861263]\n",
      "Gradient Descent(171/999): loss=0.4247593664500683, w = [-1.2908527   0.29841622 -0.46733446 -0.49242755 -0.10620302  0.42968024\n",
      "  0.59614621 -0.21376133  0.10015542  0.2496353   0.003517   -0.38372686\n",
      "  0.02526676 -0.09908682  0.09848495]\n",
      "Gradient Descent(172/999): loss=0.4246670871427632, w = [-1.29275442  0.29847148 -0.46789418 -0.49406732 -0.10639358  0.43095187\n",
      "  0.59668027 -0.21426813  0.10024965  0.2500688   0.00336012 -0.3833168\n",
      "  0.02514658 -0.09921833  0.09836042]\n",
      "Gradient Descent(173/999): loss=0.42457583877754324, w = [-1.29463671  0.29852624 -0.46844778 -0.49570567 -0.10658092  0.43222091\n",
      "  0.5972128  -0.21477173  0.10034195  0.25049873  0.00320682 -0.3829055\n",
      "  0.02502978 -0.09934676  0.09823897]\n",
      "Gradient Descent(174/999): loss=0.4244855991780201, w = [-1.29649981  0.29858049 -0.46899531 -0.49734258 -0.10676509  0.43348739\n",
      "  0.59774378 -0.2152722   0.10043232  0.25092512  0.00305702 -0.38249299\n",
      "  0.02491628 -0.09947214  0.09812054]\n",
      "Gradient Descent(175/999): loss=0.424396346761305, w = [-1.29834399  0.29863423 -0.46953687 -0.49897806 -0.10694614  0.43475133\n",
      "  0.59827324 -0.21576958  0.10052078  0.25134802  0.00291063 -0.38207931\n",
      "  0.024806   -0.0995945   0.09800507]\n",
      "Gradient Descent(176/999): loss=0.42430806051938363, w = [-1.30016946  0.29868744 -0.4700725  -0.50061209 -0.10712412  0.43601275\n",
      "  0.59880117 -0.21626392  0.10060735  0.25176747  0.00276755 -0.38166448\n",
      "  0.02469884 -0.09971388  0.0978925 ]\n",
      "Gradient Descent(177/999): loss=0.4242207200011713, w = [-1.30197648  0.29874013 -0.47060229 -0.50224469 -0.10729906  0.43727167\n",
      "  0.59932759 -0.21675527  0.10069203  0.25218351  0.00262772 -0.38124854\n",
      "  0.02459472 -0.09983032  0.09778276]\n",
      "Gradient Descent(178/999): loss=0.4241343052952004, w = [-1.30376528  0.29879228 -0.47112631 -0.50387584 -0.10747103  0.43852811\n",
      "  0.59985251 -0.21724367  0.10077484  0.25259617  0.00249105 -0.3808315\n",
      "  0.02449357 -0.09994385  0.0976758 ]\n",
      "Gradient Descent(179/999): loss=0.4240487970129684, w = [-1.30553609  0.29884388 -0.47164461 -0.50550555 -0.10764006  0.43978209\n",
      "  0.60037593 -0.21772917  0.10085579  0.2530055   0.00235745 -0.3804134\n",
      "  0.02439531 -0.10005451  0.09757156]\n",
      "Gradient Descent(180/999): loss=0.42396417627284677, w = [-1.30728913  0.29889493 -0.47215726 -0.50713381 -0.1078062   0.44103364\n",
      "  0.60089786 -0.21821182  0.1009349   0.25341153  0.00222686 -0.37999426\n",
      "  0.02429986 -0.10016231  0.09746998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(181/999): loss=0.4238804246845858, w = [-1.30902463  0.29894542 -0.47266434 -0.50876061 -0.10796949  0.44228276\n",
      "  0.60141831 -0.21869166  0.10101217  0.25381431  0.0020992  -0.37957411\n",
      "  0.02420715 -0.10026731  0.09737102]\n",
      "Gradient Descent(182/999): loss=0.42379752433437284, w = [-1.31074281  0.29899535 -0.4731659  -0.51038596 -0.10812997  0.44352948\n",
      "  0.60193729 -0.21916874  0.10108763  0.25421386  0.00197439 -0.37915298\n",
      "  0.0241171  -0.10036952  0.09727461]\n",
      "Gradient Descent(183/999): loss=0.4237154577704049, w = [-1.31244389  0.2990447  -0.473662   -0.51200985 -0.10828769  0.44477383\n",
      "  0.6024548  -0.21964309  0.10116129  0.25461023  0.00185236 -0.37873088\n",
      "  0.02402964 -0.10046898  0.09718071]\n",
      "Gradient Descent(184/999): loss=0.4236342079889821, w = [-1.31412808  0.29909347 -0.47415271 -0.51363227 -0.10844268  0.44601581\n",
      "  0.60297086 -0.22011477  0.10123316  0.25500345  0.00173306 -0.37830785\n",
      "  0.02394472 -0.10056573  0.09708926]\n",
      "Gradient Descent(185/999): loss=0.42355375842105825, w = [-1.31579558  0.29914166 -0.47463809 -0.51525324 -0.10859499  0.44725545\n",
      "  0.60348548 -0.2205838   0.10130325  0.25539356  0.0016164  -0.3778839\n",
      "  0.02386225 -0.10065978  0.09700022]\n",
      "Gradient Descent(186/999): loss=0.42347409291931026, w = [-1.31744661  0.29918927 -0.47511821 -0.51687273 -0.10874465  0.44849276\n",
      "  0.60399866 -0.22105024  0.10137159  0.25578059  0.00150233 -0.37745906\n",
      "  0.02378218 -0.10075118  0.09691353]\n",
      "Gradient Descent(187/999): loss=0.423395195745595, w = [-1.31908138  0.29923627 -0.4755931  -0.51849076 -0.10889171  0.44972777\n",
      "  0.60451041 -0.22151411  0.10143818  0.25616458  0.00139077 -0.37703335\n",
      "  0.02370445 -0.10083995  0.09682915]\n",
      "Gradient Descent(188/999): loss=0.4233170515588901, w = [-1.32070007e+00  2.99282675e-01 -4.76062851e-01 -5.20107316e-01\n",
      " -1.09036195e-01  4.50960483e-01  6.05020744e-01 -2.21975472e-01\n",
      "  1.01503040e-01  2.56545567e-01  1.28167797e-03 -3.76606787e-01\n",
      "  2.36289852e-02 -1.00926113e-01  9.67470322e-02]\n",
      "Gradient Descent(189/999): loss=0.4232396454035986, w = [-1.32230289e+00  2.99328473e-01 -4.76527502e-01 -5.21722394e-01\n",
      " -1.09178147e-01  4.52190927e-01  6.05529666e-01 -2.22434348e-01\n",
      "  1.01566184e-01  2.56923576e-01  1.17498217e-03 -3.76179401e-01\n",
      "  2.35557372e-02 -1.01009709e-01  9.66671271e-02]\n",
      "Gradient Descent(190/999): loss=0.42316296269829334, w = [-1.32389003e+00  2.99373660e-01 -4.76987114e-01 -5.23335994e-01\n",
      " -1.09317600e-01  4.53419116e-01  6.06037187e-01 -2.22890780e-01\n",
      "  1.01627627e-01  2.57298645e-01  1.07062642e-03 -3.75751209e-01\n",
      "  2.34846451e-02 -1.01090760e-01  9.65893925e-02]\n",
      "Gradient Descent(191/999): loss=0.4230869892248202, w = [-1.32546169e+00  2.99418232e-01 -4.77441744e-01 -5.24948114e-01\n",
      " -1.09454589e-01  4.54645068e-01  6.06543316e-01 -2.23344805e-01\n",
      "  1.01687383e-01  2.57670807e-01  9.68553679e-04 -3.75322230e-01\n",
      "  2.34156525e-02 -1.01169297e-01  9.65137843e-02]\n",
      "Gradient Descent(192/999): loss=0.4230117111177696, w = [-1.32701805e+00  2.99462183e-01 -4.77891446e-01 -5.26558750e-01\n",
      " -1.09589150e-01  4.55868799e-01  6.07048060e-01 -2.23796459e-01\n",
      "  1.01745469e-01  2.58040094e-01  8.68708378e-04 -3.74892486e-01\n",
      "  2.33487047e-02 -1.01245345e-01  9.64402599e-02]\n",
      "Gradient Descent(193/999): loss=0.4229371148542936, w = [-1.32855930e+00  2.99505510e-01 -4.78336274e-01 -5.28167899e-01\n",
      " -1.09721316e-01  4.57090326e-01  6.07551430e-01 -2.24245779e-01\n",
      "  1.01801897e-01  2.58406537e-01  7.71036417e-04 -3.74461994e-01\n",
      "  2.32837482e-02 -1.01318933e-01  9.63687773e-02]\n",
      "Gradient Descent(194/999): loss=0.4228631872442935, w = [-1.33008562e+00  2.99548207e-01 -4.78776283e-01 -5.29775560e-01\n",
      " -1.09851119e-01  4.58309665e-01  6.08053433e-01 -2.24692798e-01\n",
      "  1.01856684e-01  2.58770170e-01  6.75485100e-04 -3.74030774e-01\n",
      "  2.32207311e-02 -1.01390087e-01  9.62992955e-02]\n",
      "Gradient Descent(195/999): loss=0.42278991542088507, w = [-1.33159720e+00  2.99590272e-01 -4.79211526e-01 -5.31381730e-01\n",
      " -1.09978593e-01  4.59526833e-01  6.08554078e-01 -2.25137552e-01\n",
      "  1.01909844e-01  2.59131022e-01  5.82003106e-04 -3.73598844e-01\n",
      "  2.31596029e-02 -1.01458834e-01  9.62317741e-02]\n",
      "Gradient Descent(196/999): loss=0.4227172868312186, w = [-1.33309421e+00  2.99631700e-01 -4.79642053e-01 -5.32986406e-01\n",
      " -1.10103768e-01  4.60741845e-01  6.09053375e-01 -2.25580074e-01\n",
      "  1.01961393e-01  2.59489125e-01  4.90540444e-04 -3.73166222e-01\n",
      "  2.31003144e-02 -1.01525199e-01  9.61661740e-02]\n",
      "Gradient Descent(197/999): loss=0.42264528922758404, w = [-1.33457683e+00  2.99672487e-01 -4.80067918e-01 -5.34589585e-01\n",
      " -1.10226676e-01  4.61954717e-01  6.09551331e-01 -2.26020398e-01\n",
      "  1.02011344e-01  2.59844509e-01  4.01048420e-04 -3.72732926e-01\n",
      "  2.30428175e-02 -1.01589209e-01  9.61024566e-02]\n",
      "Gradient Descent(198/999): loss=0.4225739106587984, w = [-1.33604522e+00  2.99712629e-01 -4.80489170e-01 -5.36191266e-01\n",
      " -1.10347347e-01  4.63165465e-01  6.10047956e-01 -2.26458555e-01\n",
      "  1.02059714e-01  2.60197203e-01  3.13479604e-04 -3.72298973e-01\n",
      "  2.29870655e-02 -1.01650889e-01  9.60405841e-02]\n",
      "Gradient Descent(199/999): loss=0.4225031394618854, w = [-1.33749957e+00  2.99752125e-01 -4.80905859e-01 -5.37791446e-01\n",
      " -1.10465812e-01  4.64374104e-01  6.10543258e-01 -2.26894580e-01\n",
      "  1.02106517e-01  2.60547238e-01  2.27787789e-04 -3.71864380e-01\n",
      "  2.29330131e-02 -1.01710264e-01  9.59805197e-02]\n",
      "Gradient Descent(200/999): loss=0.4224329642540164, w = [-1.33894003e+00  2.99790969e-01 -4.81318036e-01 -5.39390123e-01\n",
      " -1.10582099e-01  4.65580649e-01  6.11037246e-01 -2.27328502e-01\n",
      "  1.02151768e-01  2.60894643e-01  1.43927961e-04 -3.71429164e-01\n",
      "  2.28806160e-02 -1.01767359e-01  9.59222271e-02]\n",
      "Gradient Descent(201/999): loss=0.4223633739247277, w = [-1.34036678e+00  2.99829158e-01 -4.81725748e-01 -5.40987294e-01\n",
      " -1.10696237e-01  4.66785116e-01  6.11529928e-01 -2.27760352e-01\n",
      "  1.02195482e-01  2.61239445e-01  6.18562668e-05 -3.70993340e-01\n",
      "  2.28298310e-02 -1.01822198e-01  9.58656709e-02]\n",
      "Gradient Descent(202/999): loss=0.4222943576283549, w = [-1.34177998e+00  2.99866691e-01 -4.82129044e-01 -5.42582958e-01\n",
      " -1.10808255e-01  4.67987518e-01  6.12021312e-01 -2.28190163e-01\n",
      "  1.02237675e-01  2.61581674e-01 -1.84700196e-05 -3.70556924e-01\n",
      "  2.27806161e-02 -1.01874806e-01  9.58108163e-02]\n",
      "Gradient Descent(203/999): loss=0.4222259047767303, w = [-1.34317978e+00  2.99903564e-01 -4.82527971e-01 -5.44177111e-01\n",
      " -1.10918181e-01  4.69187870e-01  6.12511408e-01 -2.28617962e-01\n",
      "  1.02278360e-01  2.61921358e-01 -9.70925271e-05 -3.70119932e-01\n",
      "  2.27329305e-02 -1.01925207e-01  9.57576294e-02]\n",
      "Gradient Descent(204/999): loss=0.42215800503211354, w = [-1.34456635e+00  2.99939773e-01 -4.82922576e-01 -5.45769753e-01\n",
      " -1.11026041e-01  4.70386188e-01  6.13000224e-01 -2.29043781e-01\n",
      "  1.02317553e-01  2.62258523e-01 -1.74051817e-04 -3.69682379e-01\n",
      "  2.26867342e-02 -1.01973424e-01  9.57060769e-02]\n",
      "Gradient Descent(205/999): loss=0.42209064830033943, w = [-1.34593984e+00  2.99975318e-01 -4.83312906e-01 -5.47360880e-01\n",
      " -1.11131862e-01  4.71582484e-01  6.13487767e-01 -2.29467647e-01\n",
      "  1.02355269e-01  2.62593198e-01 -2.49387413e-04 -3.69244280e-01\n",
      "  2.26419886e-02 -1.02019481e-01  9.56561260e-02]\n",
      "Gradient Descent(206/999): loss=0.4220238247241639, w = [-1.34730041e+00  3.00010194e-01 -4.83699004e-01 -5.48950491e-01\n",
      " -1.11235670e-01  4.72776774e-01  6.13974047e-01 -2.29889590e-01\n",
      "  1.02391522e-01  2.62925408e-01 -3.23137828e-04 -3.68805649e-01\n",
      "  2.25986559e-02 -1.02063400e-01  9.56077449e-02]\n",
      "Gradient Descent(207/999): loss=0.42195752467686465, w = [-1.34864820e+00  3.00044400e-01 -4.84080918e-01 -5.50538585e-01\n",
      " -1.11337492e-01  4.73969070e-01  6.14459072e-01 -2.30309636e-01\n",
      "  1.02426328e-01  2.63255181e-01 -3.95340593e-04 -3.68366501e-01\n",
      "  2.25566991e-02 -1.02105205e-01  9.55609021e-02]\n",
      "Gradient Descent(208/999): loss=0.4218917387559755, w = [-1.34998337e+00  3.00077934e-01 -4.84458691e-01 -5.52125158e-01\n",
      " -1.11437352e-01  4.75159387e-01  6.14942850e-01 -2.30727815e-01\n",
      "  1.02459701e-01  2.63582543e-01 -4.66032282e-04 -3.67926849e-01\n",
      "  2.25160825e-02 -1.02144918e-01  9.55155670e-02]\n",
      "Gradient Descent(209/999): loss=0.4218264577772833, w = [-1.35130607e+00  3.00110793e-01 -4.84832366e-01 -5.53710210e-01\n",
      " -1.11535275e-01  4.76347737e-01  6.15425389e-01 -2.31144153e-01\n",
      "  1.02491656e-01  2.63907518e-01 -5.35248540e-04 -3.67486708e-01\n",
      "  2.24767712e-02 -1.02182561e-01  9.54717094e-02]\n",
      "Gradient Descent(210/999): loss=0.4217616727689436, w = [-1.35261644e+00  3.00142975e-01 -4.85201988e-01 -5.55293738e-01\n",
      " -1.11631285e-01  4.77534135e-01  6.15906697e-01 -2.31558677e-01\n",
      "  1.02522207e-01  2.64230133e-01 -6.03024107e-04 -3.67046089e-01\n",
      "  2.24387311e-02 -1.02218156e-01  9.54292999e-02]\n",
      "Gradient Descent(211/999): loss=0.4216973749658397, w = [-1.35391462e+00  3.00174478e-01 -4.85567599e-01 -5.56875741e-01\n",
      " -1.11725407e-01  4.78718593e-01  6.16386783e-01 -2.31971413e-01\n",
      "  1.02551369e-01  2.64550413e-01 -6.69392840e-04 -3.66605007e-01\n",
      "  2.24019291e-02 -1.02251725e-01  9.53883094e-02]\n",
      "Gradient Descent(212/999): loss=0.42163355580405987, w = [-1.35520075e+00  3.00205302e-01 -4.85929241e-01 -5.58456217e-01\n",
      " -1.11817664e-01  4.79901124e-01  6.16865654e-01 -2.32382386e-01\n",
      "  1.02579157e-01  2.64868383e-01 -7.34387742e-04 -3.66163475e-01\n",
      "  2.23663328e-02 -1.02283290e-01  9.53487097e-02]\n",
      "Gradient Descent(213/999): loss=0.4215702069155862, w = [-1.35647498e+00  3.00235443e-01 -4.86286955e-01 -5.60035165e-01\n",
      " -1.11908079e-01  4.81081742e-01  6.17343319e-01 -2.32791623e-01\n",
      "  1.02605585e-01  2.65184066e-01 -7.98040979e-04 -3.65721504e-01\n",
      "  2.23319109e-02 -1.02312871e-01  9.53104729e-02]\n",
      "Gradient Descent(214/999): loss=0.42150732012309605, w = [-1.35773744e+00  3.00264901e-01 -4.86640784e-01 -5.61612582e-01\n",
      " -1.11996674e-01  4.82260458e-01  6.17819785e-01 -2.33199148e-01\n",
      "  1.02630668e-01  2.65497487e-01 -8.60383908e-04 -3.65279108e-01\n",
      "  2.22986327e-02 -1.02340491e-01  9.52735718e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(215/999): loss=0.42144488743498365, w = [-1.35898827e+00  3.00293673e-01 -4.86990767e-01 -5.63188468e-01\n",
      " -1.12083473e-01  4.83437285e-01  6.18295060e-01 -2.33604986e-01\n",
      "  1.02654419e-01  2.65808670e-01 -9.21447093e-04 -3.64836298e-01\n",
      "  2.22664683e-02 -1.02366168e-01  9.52379797e-02]\n",
      "Gradient Descent(216/999): loss=0.4213829010404713, w = [-1.36022760e+00  3.00321759e-01 -4.87336945e-01 -5.64762820e-01\n",
      " -1.12168496e-01  4.84612236e-01  6.18769153e-01 -2.34009161e-01\n",
      "  1.02676854e-01  2.66117638e-01 -9.81260331e-04 -3.64393086e-01\n",
      "  2.22353886e-02 -1.02389925e-01  9.52036703e-02]\n",
      "Gradient Descent(217/999): loss=0.4213213533048987, w = [-1.36145556e+00  3.00349157e-01 -4.87679357e-01 -5.66335639e-01\n",
      " -1.12251766e-01  4.85785323e-01  6.19242070e-01 -2.34411696e-01\n",
      "  1.02697986e-01  2.66424414e-01 -1.03985267e-03 -3.63949484e-01\n",
      "  2.22053654e-02 -1.02411780e-01  9.51706180e-02]\n",
      "Gradient Descent(218/999): loss=0.421260236765134, w = [-1.36267229e+00  3.00375866e-01 -4.88018043e-01 -5.67906921e-01\n",
      " -1.12333304e-01  4.86956557e-01  6.19713820e-01 -2.34812616e-01\n",
      "  1.02717830e-01  2.66729021e-01 -1.09725243e-03 -3.63505503e-01\n",
      "  2.21763708e-02 -1.02431756e-01  9.51387977e-02]\n",
      "Gradient Descent(219/999): loss=0.4211995441251566, w = [-1.36387791e+00  3.00401885e-01 -4.88353040e-01 -5.69476666e-01\n",
      " -1.12413129e-01  4.88125951e-01  6.20184409e-01 -2.35211943e-01\n",
      "  1.02736399e-01  2.67031481e-01 -1.15348721e-03 -3.63061155e-01\n",
      "  2.21483782e-02 -1.02449870e-01  9.51081844e-02]\n",
      "Gradient Descent(220/999): loss=0.4211392682517114, w = [-1.36507255e+00  3.00427213e-01 -4.88684388e-01 -5.71044872e-01\n",
      " -1.12491264e-01  4.89293516e-01  6.20653847e-01 -2.35609701e-01\n",
      "  1.02753708e-01  2.67331817e-01 -1.20858395e-03 -3.62616450e-01\n",
      "  2.21213611e-02 -1.02466143e-01  9.50787541e-02]\n",
      "Gradient Descent(221/999): loss=0.4210794021701614, w = [-1.36625632e+00  3.00451849e-01 -4.89012124e-01 -5.72611539e-01\n",
      " -1.12567728e-01  4.90459264e-01  6.21122140e-01 -2.36005911e-01\n",
      "  1.02769770e-01  2.67630051e-01 -1.26256888e-03 -3.62171398e-01\n",
      "  2.20952941e-02 -1.02480595e-01  9.50504830e-02]\n",
      "Gradient Descent(222/999): loss=0.421019939060385, w = [-1.36742937e+00  3.00475792e-01 -4.89336285e-01 -5.74176665e-01\n",
      " -1.12642540e-01  4.91623206e-01  6.21589295e-01 -2.36400595e-01\n",
      "  1.02784600e-01  2.67926204e-01 -1.31546760e-03 -3.61726012e-01\n",
      "  2.20701523e-02 -1.02493244e-01  9.50233478e-02]\n",
      "Gradient Descent(223/999): loss=0.4209608722528602, w = [-1.36859180e+00  3.00499042e-01 -4.89656908e-01 -5.75740249e-01\n",
      " -1.12715720e-01  4.92785354e-01  6.22055321e-01 -2.36793775e-01\n",
      "  1.02798210e-01  2.68220298e-01 -1.36730506e-03 -3.61280300e-01\n",
      "  2.20459114e-02 -1.02504109e-01  9.49973255e-02]\n",
      "Gradient Descent(224/999): loss=0.4209021952248271, w = [-1.36974374  0.3005216  -0.48997403 -0.57730229 -0.11278729  0.49394572\n",
      "  0.62252022 -0.23718547  0.10281062  0.26851235 -0.00141811 -0.36083427\n",
      "  0.02202255 -0.10251321  0.09497239]\n",
      "Gradient Descent(225/999): loss=0.4208439015965601, w = [-1.37088531  0.30054346 -0.49028768 -0.57886279 -0.11285726  0.49510431\n",
      "  0.62298401 -0.23757571  0.10282183  0.26880239 -0.00146789 -0.36038794\n",
      "  0.02200004 -0.10252056  0.09494853]\n",
      "Gradient Descent(226/999): loss=0.42078598512777193, w = [-1.37201662  0.30056462 -0.49059791 -0.58042174 -0.11292566  0.49626114\n",
      "  0.62344669 -0.2379645   0.10283186  0.26909043 -0.00151669 -0.35994131\n",
      "  0.02197836 -0.10252619  0.09492571]\n",
      "Gradient Descent(227/999): loss=0.4207284397140871, w = [-1.37313779  0.30058509 -0.49090474 -0.58197915 -0.1129925   0.49741622\n",
      "  0.62390827 -0.23835188  0.10284073  0.2693765  -0.00156452 -0.3594944\n",
      "  0.02195749 -0.10253011  0.09490392]\n",
      "Gradient Descent(228/999): loss=0.42067125938363825, w = [-1.37424892  0.30060486 -0.49120821 -0.58353501 -0.1130578   0.49856956\n",
      "  0.62436876 -0.23873785  0.10284845  0.2696606  -0.0016114  -0.35904721\n",
      "  0.02193741 -0.10253233  0.09488314]\n",
      "Gradient Descent(229/999): loss=0.42061443829376177, w = [-1.37535015  0.30062394 -0.49150835 -0.58508932 -0.11312158  0.49972117\n",
      "  0.62482815 -0.23912245  0.10285503  0.26994277 -0.00165736 -0.35859976\n",
      "  0.0219181  -0.10253289  0.09486334]\n",
      "Gradient Descent(230/999): loss=0.4205579707277762, w = [-1.37644156  0.30064232 -0.4918052  -0.58664208 -0.11318386  0.50087107\n",
      "  0.62528647 -0.23950568  0.10286049  0.27022303 -0.00170242 -0.35815204\n",
      "  0.02189954 -0.10253178  0.0948445 ]\n",
      "Gradient Descent(231/999): loss=0.4205018510918457, w = [-1.37752328  0.30066    -0.4920988  -0.5881933  -0.11324465  0.50201925\n",
      "  0.62574372 -0.23988757  0.10286483  0.27050138 -0.0017466  -0.35770408\n",
      "  0.0218817  -0.10252904  0.09482661]\n",
      "Gradient Descent(232/999): loss=0.42044607391195044, w = [-1.37859541  0.30067699 -0.49238916 -0.58974296 -0.11330397  0.50316573\n",
      "  0.6261999  -0.24026814  0.10286807  0.27077786 -0.00178991 -0.35725587\n",
      "  0.02186457 -0.10252467  0.09480964]\n",
      "Gradient Descent(233/999): loss=0.42039063383092984, w = [-1.37965805  0.30069327 -0.49267634 -0.59129108 -0.11336184  0.50431053\n",
      "  0.62665502 -0.2406474   0.10287023  0.27105247 -0.00183238 -0.35680744\n",
      "  0.02184813 -0.10251871  0.09479358]\n",
      "Gradient Descent(234/999): loss=0.42033552560561116, w = [-1.38071132  0.30070887 -0.49296035 -0.59283764 -0.11341826  0.50545364\n",
      "  0.62710909 -0.24102537  0.10287131  0.27132524 -0.00187402 -0.35635878\n",
      "  0.02183237 -0.10251115  0.09477841]\n",
      "Gradient Descent(235/999): loss=0.42028074410400956, w = [-1.38175531  0.30072376 -0.49324123 -0.59438265 -0.11347327  0.50659509\n",
      "  0.62756211 -0.24140208  0.10287133  0.27159619 -0.00191486 -0.3559099\n",
      "  0.02181725 -0.10250202  0.09476411]\n",
      "Gradient Descent(236/999): loss=0.42022628430261866, w = [-1.38279013  0.30073796 -0.49351902 -0.59592612 -0.11352687  0.50773488\n",
      "  0.62801409 -0.24177754  0.1028703   0.27186533 -0.00195491 -0.35546082\n",
      "  0.02180277 -0.10249134  0.09475066]\n",
      "Gradient Descent(237/999): loss=0.4201721412837813, w = [-1.38381588  0.30075146 -0.49379374 -0.59746802 -0.11357908  0.50887301\n",
      "  0.62846504 -0.24215176  0.10286824  0.27213269 -0.00199419 -0.35501154\n",
      "  0.02178892 -0.10247912  0.09473805]\n",
      "Gradient Descent(238/999): loss=0.4201183102330967, w = [-1.38483266  0.30076426 -0.49406542 -0.59900838 -0.11362992  0.51000951\n",
      "  0.62891497 -0.24252477  0.10286515  0.27239827 -0.00203272 -0.35456207\n",
      "  0.02177566 -0.10246538  0.09472625]\n",
      "Gradient Descent(239/999): loss=0.4200647864369459, w = [-1.38584057  0.30077637 -0.4943341  -0.60054718 -0.1136794   0.51114438\n",
      "  0.62936387 -0.24289658  0.10286105  0.2726621  -0.00207051 -0.35411241\n",
      "  0.02176299 -0.10245014  0.09471525]\n",
      "Gradient Descent(240/999): loss=0.42001156528005373, w = [-1.38683969  0.30078778 -0.4945998  -0.60208443 -0.11372753  0.51227762\n",
      "  0.62981177 -0.2432672   0.10285595  0.27292419 -0.00210758 -0.35366258\n",
      "  0.02175089 -0.1024334   0.09470504]\n",
      "Gradient Descent(241/999): loss=0.41995864224311663, w = [-1.38783014  0.3007985  -0.49486256 -0.60362013 -0.11377433  0.51340925\n",
      "  0.63025865 -0.24363666  0.10284986  0.27318457 -0.00214395 -0.35321258\n",
      "  0.02173935 -0.10241519  0.09469559]\n",
      "Gradient Descent(242/999): loss=0.4199060129005194, w = [-1.388812    0.30080852 -0.4951224  -0.60515427 -0.11381982  0.51453928\n",
      "  0.63070454 -0.24400497  0.1028428   0.27344324 -0.00217963 -0.35276242\n",
      "  0.02172835 -0.10239552  0.0946869 ]\n",
      "Gradient Descent(243/999): loss=0.41985367291808234, w = [-1.38978537  0.30081785 -0.49537936 -0.60668686 -0.113864    0.51566771\n",
      "  0.63114943 -0.24437215  0.10283477  0.27370023 -0.00221463 -0.3523121\n",
      "  0.02171787 -0.10237441  0.09467894]\n",
      "Gradient Descent(244/999): loss=0.4198016180509003, w = [-1.39075034  0.30082648 -0.49563346 -0.6082179  -0.1139069   0.51679455\n",
      "  0.63159333 -0.24473821  0.10282579  0.27395554 -0.00224898 -0.35186164\n",
      "  0.02170791 -0.10235187  0.0946717 ]\n",
      "Gradient Descent(245/999): loss=0.41974984414120975, w = [-1.39170699  0.30083442 -0.49588473 -0.60974738 -0.11394853  0.51791982\n",
      "  0.63203625 -0.24510316  0.10281587  0.27420921 -0.00228268 -0.35141104\n",
      "  0.02169844 -0.10232792  0.09466517]\n",
      "Gradient Descent(246/999): loss=0.41969834711632564, w = [-1.39265543  0.30084167 -0.49613319 -0.6112753  -0.11398889  0.51904352\n",
      "  0.63247819 -0.24546703  0.10280502  0.27446123 -0.00231575 -0.3509603\n",
      "  0.02168946 -0.10230257  0.09465933]\n",
      "Gradient Descent(247/999): loss=0.41964712298665513, w = [-1.39359574  0.30084823 -0.49637889 -0.61280168 -0.11402801  0.52016565\n",
      "  0.63291917 -0.24582983  0.10279326  0.27471163 -0.00234821 -0.35050943\n",
      "  0.02168095 -0.10227584  0.09465416]\n",
      "Gradient Descent(248/999): loss=0.41959616784371234, w = [-1.394528    0.30085409 -0.49662184 -0.61432649 -0.1140659   0.52128624\n",
      "  0.63335918 -0.24619157  0.10278058  0.27496043 -0.00238006 -0.35005845\n",
      "  0.0216729  -0.10224774  0.09464965]\n",
      "Gradient Descent(249/999): loss=0.41954547785823915, w = [-1.39545231  0.30085927 -0.49686207 -0.61584976 -0.11410256  0.52240528\n",
      "  0.63379823 -0.24655227  0.10276701  0.27520763 -0.00241133 -0.34960735\n",
      "  0.0216653  -0.10221828  0.09464579]\n",
      "Gradient Descent(250/999): loss=0.41949504927833897, w = [-1.39636874  0.30086375 -0.49709961 -0.61737147 -0.11413802  0.52352278\n",
      "  0.63423633 -0.24691194  0.10275256  0.27545326 -0.00244202 -0.34915614\n",
      "  0.02165812 -0.10218749  0.09464256]\n",
      "Gradient Descent(251/999): loss=0.4194448784276803, w = [-1.39727739  0.30086755 -0.49733448 -0.61889162 -0.11417229  0.52463876\n",
      "  0.63467348 -0.2472706   0.10273723  0.27569733 -0.00247214 -0.34870483\n",
      "  0.02165138 -0.10215537  0.09463996]\n",
      "Gradient Descent(252/999): loss=0.4193949617037441, w = [-1.39817833  0.30087066 -0.49756671 -0.62041023 -0.11420537  0.52575322\n",
      "  0.63510969 -0.24762826  0.10272104  0.27593984 -0.00250171 -0.34825343\n",
      "  0.02164504 -0.10212194  0.09463796]\n",
      "Gradient Descent(253/999): loss=0.4193452955760992, w = [-1.39907165  0.30087308 -0.49779633 -0.62192727 -0.11423728  0.52686616\n",
      "  0.63554497 -0.24798493  0.102704    0.27618083 -0.00253074 -0.34780194\n",
      "  0.0216391  -0.10208722  0.09463655]\n",
      "Gradient Descent(254/999): loss=0.4192958765847396, w = [-1.39995743  0.30087482 -0.49802337 -0.62344277 -0.11426804  0.52797761\n",
      "  0.63597932 -0.24834063  0.10268612  0.2764203  -0.00255925 -0.34735036\n",
      "  0.02163355 -0.10205121  0.09463572]\n",
      "Gradient Descent(255/999): loss=0.41924670133847913, w = [-1.40083575  0.30087587 -0.49824784 -0.62495671 -0.11429765  0.52908755\n",
      "  0.63641274 -0.24869536  0.10266741  0.27665826 -0.00258723 -0.34689871\n",
      "  0.02162837 -0.10201393  0.09463547]\n",
      "Gradient Descent(256/999): loss=0.41919776651333457, w = [-1.40170669  0.30087624 -0.49846978 -0.6264691  -0.11432612  0.53019601\n",
      "  0.63684525 -0.24904916  0.10264788  0.27689474 -0.00261471 -0.34644698\n",
      "  0.02162357 -0.1019754   0.09463576]\n",
      "Gradient Descent(257/999): loss=0.41914906885102315, w = [-1.40257033  0.30087592 -0.4986892  -0.62797994 -0.11435348  0.53130298\n",
      "  0.63727684 -0.24940202  0.10262754  0.27712974 -0.00264169 -0.34599519\n",
      "  0.02161912 -0.10193562  0.0946366 ]\n",
      "Gradient Descent(258/999): loss=0.4191006051574329, w = [-1.40342674  0.30087493 -0.49890614 -0.62948922 -0.11437972  0.53240848\n",
      "  0.63770752 -0.24975396  0.10260639  0.27736328 -0.00266819 -0.34554333\n",
      "  0.02161502 -0.10189461  0.09463797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(259/999): loss=0.41905237230118186, w = [-1.404276    0.30087325 -0.49912061 -0.63099696 -0.11440487  0.53351252\n",
      "  0.63813731 -0.250105    0.10258446  0.27759536 -0.00269422 -0.34509142\n",
      "  0.02161125 -0.10185239  0.09463986]\n",
      "Gradient Descent(260/999): loss=0.4190043672121676, w = [-1.40511819  0.30087089 -0.49933265 -0.63250314 -0.11442893  0.53461509\n",
      "  0.63856619 -0.25045514  0.10256176  0.27782602 -0.00271978 -0.34463946\n",
      "  0.02160781 -0.10180896  0.09464226]\n",
      "Gradient Descent(261/999): loss=0.4189565868802014, w = [-1.40595338  0.30086786 -0.49954227 -0.63400777 -0.11445191  0.53571622\n",
      "  0.63899419 -0.25080439  0.10253828  0.27805524 -0.00274488 -0.34418746\n",
      "  0.0216047  -0.10176435  0.09464516]\n",
      "Gradient Descent(262/999): loss=0.418909028353643, w = [-1.40678164  0.30086415 -0.4997495  -0.63551085 -0.11447383  0.53681589\n",
      "  0.6394213  -0.25115278  0.10251404  0.27828307 -0.00276954 -0.34373541\n",
      "  0.02160189 -0.10171856  0.09464854]\n",
      "Gradient Descent(263/999): loss=0.41886168873807655, w = [-1.40760305  0.30085977 -0.49995436 -0.63701239 -0.11449469  0.53791414\n",
      "  0.63984752 -0.25150031  0.10248906  0.27850949 -0.00279377 -0.34328333\n",
      "  0.02159938 -0.1016716   0.09465239]\n",
      "Gradient Descent(264/999): loss=0.4188145651950374, w = [-1.40841768  0.30085471 -0.50015688 -0.63851237 -0.1145145   0.53901095\n",
      "  0.64027287 -0.25184699  0.10246333  0.27873453 -0.00281757 -0.34283122\n",
      "  0.02159716 -0.10162349  0.09465671]\n",
      "Gradient Descent(265/999): loss=0.41876765494074164, w = [-1.4092256   0.30084897 -0.50035707 -0.6400108  -0.11453329  0.54010633\n",
      "  0.64069735 -0.25219284  0.10243687  0.2789582  -0.00284095 -0.34237909\n",
      "  0.02159523 -0.10157424  0.09466148]\n",
      "Gradient Descent(266/999): loss=0.4187209552448794, w = [-1.41002688  0.30084257 -0.50055497 -0.64150769 -0.11455105  0.5412003\n",
      "  0.64112097 -0.25253787  0.1024097   0.27918051 -0.00286392 -0.34192693\n",
      "  0.02159357 -0.10152387  0.09466669]\n",
      "Gradient Descent(267/999): loss=0.41867446342940406, w = [-1.41082158  0.3008355  -0.50075058 -0.64300303 -0.11456779  0.54229286\n",
      "  0.64154372 -0.25288208  0.10238181  0.27940147 -0.00288649 -0.34147476\n",
      "  0.02159218 -0.10147237  0.09467234]\n",
      "Gradient Descent(268/999): loss=0.4186281768673874, w = [-1.41160979  0.30082775 -0.50094395 -0.64449682 -0.11458354  0.54338402\n",
      "  0.64196562 -0.2532255   0.10235322  0.2796211  -0.00290867 -0.34102258\n",
      "  0.02159106 -0.10141978  0.09467841]\n",
      "Gradient Descent(269/999): loss=0.4185820929818809, w = [-1.41239156  0.30081934 -0.50113508 -0.64598907 -0.11459829  0.54447378\n",
      "  0.64238667 -0.25356812  0.10232394  0.2798394  -0.00293046 -0.34057039\n",
      "  0.02159018 -0.1013661   0.09468489]\n",
      "Gradient Descent(270/999): loss=0.418536209244792, w = [-1.41316696  0.30081027 -0.501324   -0.64747977 -0.11461206  0.54556215\n",
      "  0.64280687 -0.25390996  0.10229397  0.2800564  -0.00295188 -0.34011819\n",
      "  0.02158955 -0.10131133  0.09469177]\n",
      "Gradient Descent(271/999): loss=0.41849052317583824, w = [-1.41393606  0.30080053 -0.50151072 -0.64896893 -0.11462485  0.54664913\n",
      "  0.64322623 -0.25425104  0.10226333  0.28027209 -0.00297293 -0.33966601\n",
      "  0.02158916 -0.1012555   0.09469905]\n",
      "Gradient Descent(272/999): loss=0.4184450323414623, w = [-1.41469893  0.30079013 -0.50169528 -0.65045655 -0.11463668  0.54773474\n",
      "  0.64364475 -0.25459136  0.10223203  0.2804865  -0.00299362 -0.33921382\n",
      "  0.02158899 -0.10119861  0.09470671]\n",
      "Gradient Descent(273/999): loss=0.41839973435383726, w = [-1.41545562  0.30077906 -0.5018777  -0.65194262 -0.11464756  0.54881898\n",
      "  0.64406244 -0.25493093  0.10220006  0.28069963 -0.00301396 -0.33876165\n",
      "  0.02158906 -0.10114068  0.09471474]\n",
      "Gradient Descent(274/999): loss=0.418354626869826, w = [-1.4162062   0.30076734 -0.50205798 -0.65342715 -0.11465749  0.54990185\n",
      "  0.64447931 -0.25526976  0.10216745  0.28091149 -0.00303395 -0.33830949\n",
      "  0.02158934 -0.10108172  0.09472315]\n",
      "Gradient Descent(275/999): loss=0.418309707590053, w = [-1.41695074  0.30075496 -0.50223616 -0.65491014 -0.11466649  0.55098336\n",
      "  0.64489535 -0.25560787  0.1021342   0.28112211 -0.0030536  -0.33785736\n",
      "  0.02158983 -0.10102173  0.0947319 ]\n",
      "Gradient Descent(276/999): loss=0.41826497425790354, w = [-1.41768929  0.30074192 -0.50241225 -0.65639159 -0.11467456  0.55206352\n",
      "  0.64531057 -0.25594526  0.10210032  0.28133147 -0.00307292 -0.33740524\n",
      "  0.02159053 -0.10096074  0.09474101]\n",
      "Gradient Descent(277/999): loss=0.41822042465862636, w = [-1.41842192  0.30072823 -0.50258627 -0.6578715  -0.11468171  0.55314233\n",
      "  0.64572498 -0.25628194  0.10206582  0.28153961 -0.00309191 -0.33695315\n",
      "  0.02159142 -0.10089874  0.09475046]\n",
      "Gradient Descent(278/999): loss=0.41817605661841667, w = [-1.41914869  0.30071388 -0.50275825 -0.65934987 -0.11468796  0.55421981\n",
      "  0.64613859 -0.25661793  0.1020307   0.28174653 -0.00311058 -0.3365011\n",
      "  0.02159251 -0.10083576  0.09476024]\n",
      "Gradient Descent(279/999): loss=0.41813186800352886, w = [-1.41986966  0.30069888 -0.5029282  -0.66082671 -0.1146933   0.55529594\n",
      "  0.64655138 -0.25695323  0.10199497  0.28195223 -0.00312895 -0.33604908\n",
      "  0.02159378 -0.1007718   0.09477034]\n",
      "Gradient Descent(280/999): loss=0.41808785671941057, w = [-1.42058489  0.30068323 -0.50309614 -0.66230201 -0.11469776  0.55637075\n",
      "  0.64696338 -0.25728785  0.10195865  0.28215673 -0.003147   -0.33559709\n",
      "  0.02159524 -0.10070687  0.09478075]\n",
      "Gradient Descent(281/999): loss=0.41804402070986735, w = [-1.42129443  0.30066694 -0.5032621  -0.66377577 -0.11470133  0.55744424\n",
      "  0.64737458 -0.2576218   0.10192174  0.28236005 -0.00316476 -0.33514515\n",
      "  0.02159687 -0.10064099  0.09479148]\n",
      "Gradient Descent(282/999): loss=0.41800035795622514, w = [-1.42199835  0.30064999 -0.50342609 -0.665248   -0.11470403  0.55851641\n",
      "  0.647785   -0.25795509  0.10188425  0.28256218 -0.00318222 -0.33469326\n",
      "  0.02159867 -0.10057416  0.0948025 ]\n",
      "Gradient Descent(283/999): loss=0.4179568664765433, w = [-1.4226967   0.30063241 -0.50358812 -0.6667187  -0.11470586  0.55958726\n",
      "  0.64819462 -0.25828773  0.10184618  0.28276314 -0.00319939 -0.33424141\n",
      "  0.02160064 -0.1005064   0.09481381]\n",
      "Gradient Descent(284/999): loss=0.4179135443248216, w = [-1.42338954  0.30061417 -0.50374822 -0.66818786 -0.11470684  0.56065681\n",
      "  0.64860346 -0.25861973  0.10180754  0.28296295 -0.00321628 -0.33378962\n",
      "  0.02160276 -0.10043771  0.09482541]\n",
      "Gradient Descent(285/999): loss=0.41787038959023165, w = [-1.42407692  0.3005953  -0.50390642 -0.66965549 -0.11470697  0.56172506\n",
      "  0.64901153 -0.2589511   0.10176835  0.2831616  -0.00323289 -0.33333788\n",
      "  0.02160505 -0.10036811  0.09483728]\n",
      "Gradient Descent(286/999): loss=0.417827400396368, w = [-1.4247589   0.30057579 -0.50406271 -0.6711216  -0.11470625  0.56279202\n",
      "  0.64941882 -0.25928184  0.10172861  0.28335912 -0.00324923 -0.33288621\n",
      "  0.02160748 -0.10029761  0.09484943]\n",
      "Gradient Descent(287/999): loss=0.4177845749005395, w = [-1.42543554  0.30055564 -0.50421713 -0.67258617 -0.11470471  0.56385768\n",
      "  0.64982534 -0.25961197  0.10168833  0.2835555  -0.00326531 -0.33243459\n",
      "  0.02161005 -0.10022621  0.09486183]\n",
      "Gradient Descent(288/999): loss=0.41774191129302524, w = [-1.42610689  0.30053485 -0.50436969 -0.67404922 -0.11470234  0.56492207\n",
      "  0.65023109 -0.25994148  0.1016475   0.28375076 -0.00328112 -0.33198305\n",
      "  0.02161277 -0.10015393  0.09487449]\n",
      "Gradient Descent(289/999): loss=0.4176994077963909, w = [-1.42677299  0.30051343 -0.50452041 -0.67551074 -0.11469915  0.56598517\n",
      "  0.65063609 -0.2602704   0.10160616  0.28394491 -0.00329668 -0.33153157\n",
      "  0.02161562 -0.10008077  0.0948874 ]\n",
      "Gradient Descent(290/999): loss=0.41765706266482217, w = [-1.42743392  0.30049137 -0.5046693  -0.67697074 -0.11469516  0.567047\n",
      "  0.65104032 -0.26059873  0.10156429  0.28413796 -0.00331198 -0.33108016\n",
      "  0.0216186  -0.10000676  0.09490055]\n",
      "Gradient Descent(291/999): loss=0.4176148741834383, w = [-1.4280897   0.30046869 -0.50481639 -0.67842921 -0.11469036  0.56810756\n",
      "  0.65144381 -0.26092648  0.1015219   0.28432992 -0.00332704 -0.33062883\n",
      "  0.02162171 -0.09993188  0.09491394]\n",
      "Gradient Descent(292/999): loss=0.4175728406676666, w = [-1.42874041  0.30044538 -0.50496169 -0.67988616 -0.11468477  0.56916686\n",
      "  0.65184654 -0.26125365  0.10147901  0.28452079 -0.00334186 -0.33017758\n",
      "  0.02162494 -0.09985617  0.09492755]\n",
      "Gradient Descent(293/999): loss=0.4175309604625809, w = [-1.42938609  0.30042144 -0.50510522 -0.68134159 -0.11467839  0.57022489\n",
      "  0.65224853 -0.26158026  0.10143562  0.28471059 -0.00335645 -0.32972642\n",
      "  0.02162829 -0.09977962  0.09494138]\n",
      "Gradient Descent(294/999): loss=0.41748923194231596, w = [-1.43002679  0.30039687 -0.50524699 -0.6827955  -0.11467124  0.57128168\n",
      "  0.65264977 -0.2619063   0.10139174  0.28489932 -0.00337079 -0.32927533\n",
      "  0.02163176 -0.09970224  0.09495543]\n",
      "Gradient Descent(295/999): loss=0.4174476535094378, w = [-1.43066255  0.30037168 -0.50538703 -0.68424789 -0.11466331  0.57233722\n",
      "  0.65305028 -0.2622318   0.10134738  0.28508699 -0.00338492 -0.32882434\n",
      "  0.02163534 -0.09962405  0.09496969]\n",
      "Gradient Descent(296/999): loss=0.4174062235943622, w = [-1.43129344  0.30034587 -0.50552534 -0.68569877 -0.11465462  0.57339151\n",
      "  0.65345006 -0.26255675  0.10130253  0.28527362 -0.00339881 -0.32837343\n",
      "  0.02163902 -0.09954505  0.09498416]\n",
      "Gradient Descent(297/999): loss=0.4173649406547733, w = [-1.43191949  0.30031944 -0.50566194 -0.68714812 -0.11464518  0.57444457\n",
      "  0.6538491  -0.26288117  0.10125722  0.28545921 -0.00341249 -0.32792262\n",
      "  0.02164281 -0.09946526  0.09499881]\n",
      "Gradient Descent(298/999): loss=0.4173238031750772, w = [-1.43254076  0.30029239 -0.50579686 -0.68859597 -0.11463498  0.5754964\n",
      "  0.65424742 -0.26320505  0.10121143  0.28564376 -0.00342596 -0.32747191\n",
      "  0.02164669 -0.09938468  0.09501366]\n",
      "Gradient Descent(299/999): loss=0.4172828096658291, w = [-1.43315729  0.30026473 -0.5059301  -0.6900423  -0.11462404  0.57654699\n",
      "  0.65464501 -0.26352842  0.10116519  0.28582729 -0.00343921 -0.32702129\n",
      "  0.02165068 -0.09930331  0.0950287 ]\n",
      "Gradient Descent(300/999): loss=0.4172419586632024, w = [-1.43376914  0.30023645 -0.50606168 -0.69148711 -0.11461236  0.57759636\n",
      "  0.65504189 -0.26385127  0.1011185   0.28600981 -0.00345225 -0.32657078\n",
      "  0.02165475 -0.09922118  0.09504391]\n",
      "Gradient Descent(301/999): loss=0.41720124872847236, w = [-1.43437634  0.30020756 -0.50619162 -0.69293042 -0.11459996  0.57864451\n",
      "  0.65543805 -0.26417361  0.10107136  0.28619132 -0.0034651  -0.32612037\n",
      "  0.02165892 -0.09913829  0.0950593 ]\n",
      "Gradient Descent(302/999): loss=0.41716067844749166, w = [-1.43497895  0.30017806 -0.50631994 -0.69437222 -0.11458683  0.57969145\n",
      "  0.6558335  -0.26449546  0.10102378  0.28637183 -0.00347774 -0.32567006\n",
      "  0.02166317 -0.09905465  0.09507486]\n",
      "Gradient Descent(303/999): loss=0.4171202464301846, w = [-1.435577    0.30014795 -0.50644664 -0.69581251 -0.11457299  0.58073718\n",
      "  0.65622825 -0.26481681  0.10097577  0.28655135 -0.00349019 -0.32521987\n",
      "  0.0216675  -0.09897026  0.09509058]\n",
      "Gradient Descent(304/999): loss=0.41707995131008324, w = [-1.43617055  0.30011723 -0.50657175 -0.69725129 -0.11455844  0.5817817\n",
      "  0.65662229 -0.26513767  0.10092734  0.28672989 -0.00350244 -0.32476979\n",
      "  0.02167191 -0.09888514  0.09510646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(305/999): loss=0.4170397917438047, w = [-1.43675964  0.30008592 -0.50669528 -0.69868857 -0.11454318  0.58282502\n",
      "  0.65701562 -0.26545805  0.10087848  0.28690746 -0.00351451 -0.32431982\n",
      "  0.0216764  -0.09879929  0.09512249]\n",
      "Gradient Descent(306/999): loss=0.41699976641062286, w = [-1.43734431  0.30005399 -0.50681723 -0.70012434 -0.11452723  0.58386715\n",
      "  0.65740827 -0.26577796  0.10082921  0.28708406 -0.00352639 -0.32386997\n",
      "  0.02168097 -0.09871272  0.09513867]\n",
      "Gradient Descent(307/999): loss=0.41695987401198137, w = [-1.43792461  0.30002147 -0.50693764 -0.70155862 -0.11451059  0.58490808\n",
      "  0.65780021 -0.26609741  0.10077953  0.2872597  -0.00353808 -0.32342024\n",
      "  0.0216856  -0.09862545  0.09515499]\n",
      "Gradient Descent(308/999): loss=0.41692011327106776, w = [-1.43850057  0.29998835 -0.50705651 -0.70299139 -0.11449327  0.58594783\n",
      "  0.65819147 -0.26641639  0.10072945  0.28743439 -0.0035496  -0.32297063\n",
      "  0.02169031 -0.09853747  0.09517145]\n",
      "Gradient Descent(309/999): loss=0.4168804829323542, w = [-1.43907225  0.29995464 -0.50717386 -0.70442267 -0.11447527  0.58698639\n",
      "  0.65858204 -0.26673492  0.10067897  0.28760813 -0.00356095 -0.32252114\n",
      "  0.02169508 -0.09844881  0.09518804]\n",
      "Gradient Descent(310/999): loss=0.4168409817611876, w = [-1.43963968  0.29992033 -0.5072897  -0.70585244 -0.1144566   0.58802378\n",
      "  0.65897193 -0.267053    0.1006281   0.28778094 -0.00357212 -0.32207178\n",
      "  0.02169991 -0.09835946  0.09520476]\n",
      "Gradient Descent(311/999): loss=0.41680160854335774, w = [-1.44020291  0.29988543 -0.50740405 -0.70728072 -0.11443727  0.58905999\n",
      "  0.65936114 -0.26737064  0.10057684  0.28795282 -0.00358312 -0.32162254\n",
      "  0.0217048  -0.09826943  0.0952216 ]\n",
      "Gradient Descent(312/999): loss=0.4167623620847002, w = [-1.44076198  0.29984993 -0.50751692 -0.70870751 -0.11441728  0.59009503\n",
      "  0.65974967 -0.26768785  0.10052521  0.28812377 -0.00359396 -0.32117344\n",
      "  0.02170975 -0.09817873  0.09523856]\n",
      "Gradient Descent(313/999): loss=0.4167232412106808, w = [-1.44131692  0.29981385 -0.50762832 -0.71013281 -0.11439664  0.5911289\n",
      "  0.66013753 -0.26800462  0.1004732   0.28829382 -0.00360463 -0.32072447\n",
      "  0.02171475 -0.09808738  0.09525564]\n",
      "Gradient Descent(314/999): loss=0.4166842447660225, w = [-1.44186778  0.29977718 -0.50773826 -0.71155661 -0.11437535  0.59216162\n",
      "  0.66052472 -0.26832098  0.10042083  0.28846295 -0.00361515 -0.32027563\n",
      "  0.02171981 -0.09799537  0.09527282]\n",
      "Gradient Descent(315/999): loss=0.4166453716143076, w = [-1.4424146   0.29973993 -0.50784677 -0.71297893 -0.11435342  0.59319317\n",
      "  0.66091124 -0.26863691  0.10036809  0.28863119 -0.0036255  -0.31982693\n",
      "  0.02172492 -0.09790272  0.09529011]\n",
      "Gradient Descent(316/999): loss=0.41660662063760795, w = [-1.44295742  0.2997021  -0.50795386 -0.71439975 -0.11433085  0.59422357\n",
      "  0.6612971  -0.26895243  0.100315    0.28879853 -0.00363571 -0.31937837\n",
      "  0.02173007 -0.09780943  0.0953075 ]\n",
      "Gradient Descent(317/999): loss=0.4165679907361157, w = [-1.44349627  0.29966368 -0.50805953 -0.71581909 -0.11430766  0.59525282\n",
      "  0.66168229 -0.26926755  0.10026155  0.28896498 -0.00364576 -0.31892995\n",
      "  0.02173527 -0.09771552  0.09532499]\n",
      "Gradient Descent(318/999): loss=0.4165294808278101, w = [-1.4440312   0.29962469 -0.5081638  -0.71723695 -0.11428384  0.59628092\n",
      "  0.66206684 -0.26958227  0.10020776  0.28913055 -0.00365566 -0.31848167\n",
      "  0.02174052 -0.09762098  0.09534257]\n",
      "Gradient Descent(319/999): loss=0.4164910898480571, w = [-1.44456224  0.29958512 -0.50826669 -0.71865332 -0.11425941  0.59730789\n",
      "  0.66245072 -0.26989659  0.10015362  0.28929526 -0.00366541 -0.31803353\n",
      "  0.0217458  -0.09752583  0.09536024]\n",
      "Gradient Descent(320/999): loss=0.4164528167493205, w = [-1.44508943  0.29954498 -0.50836821 -0.72006821 -0.11423437  0.59833371\n",
      "  0.66283396 -0.27021052  0.10009915  0.28945909 -0.00367502 -0.31758555\n",
      "  0.02175113 -0.09743008  0.095378  ]\n",
      "Gradient Descent(321/999): loss=0.41641466050079073, w = [-1.44561281  0.29950427 -0.50846836 -0.72148163 -0.11420872  0.5993584\n",
      "  0.66321655 -0.27052407  0.10004435  0.28962207 -0.00368448 -0.31713771\n",
      "  0.0217565  -0.09733372  0.09539583]\n",
      "Gradient Descent(322/999): loss=0.41637662008807047, w = [-1.44613242  0.29946298 -0.50856717 -0.72289356 -0.11418247  0.60038196\n",
      "  0.6635985  -0.27083724  0.09998922  0.28978419 -0.00369381 -0.31669001\n",
      "  0.0217619  -0.09723678  0.09541375]\n",
      "Gradient Descent(323/999): loss=0.4163386945128567, w = [-1.44664829  0.29942113 -0.50866464 -0.72430402 -0.11415562  0.60140439\n",
      "  0.6639798  -0.27115004  0.09993377  0.28994546 -0.003703   -0.31624247\n",
      "  0.02176733 -0.09713925  0.09543173]\n",
      "Gradient Descent(324/999): loss=0.4163008827926198, w = [-1.44716046  0.29937871 -0.50876079 -0.725713   -0.11412818  0.60242571\n",
      "  0.66436047 -0.27146247  0.09987801  0.2901059  -0.00371206 -0.31579509\n",
      "  0.02177279 -0.09704115  0.09544979]\n",
      "Gradient Descent(325/999): loss=0.41626318396030504, w = [-1.44766896  0.29933573 -0.50885563 -0.72712051 -0.11410016  0.6034459\n",
      "  0.6647405  -0.27177453  0.09982193  0.29026549 -0.00372098 -0.31534786\n",
      "  0.02177829 -0.09694247  0.09546791]\n",
      "Gradient Descent(326/999): loss=0.41622559706401757, w = [-1.44817382  0.29929218 -0.50894918 -0.72852655 -0.11407157  0.60446497\n",
      "  0.6651199  -0.27208624  0.09976555  0.29042427 -0.00372978 -0.31490078\n",
      "  0.02178382 -0.09684324  0.09548609]\n",
      "Gradient Descent(327/999): loss=0.41618812116675075, w = [-1.4486751   0.29924808 -0.50904144 -0.72993113 -0.11404239  0.60548294\n",
      "  0.66549867 -0.27239759  0.09970886  0.29058222 -0.00373844 -0.31445387\n",
      "  0.02178937 -0.09674345  0.09550434]\n",
      "Gradient Descent(328/999): loss=0.41615075534607504, w = [-1.44917281  0.29920341 -0.50913242 -0.73133423 -0.11401265  0.60649979\n",
      "  0.66587681 -0.2727086   0.09965188  0.29073935 -0.00374698 -0.31400711\n",
      "  0.02179494 -0.09664311  0.09552263]\n",
      "Gradient Descent(329/999): loss=0.41611349869387837, w = [-1.44966699  0.29915819 -0.50922214 -0.73273587 -0.11398235  0.60751555\n",
      "  0.66625433 -0.27301926  0.09959461  0.29089567 -0.0037554  -0.31356052\n",
      "  0.02180054 -0.09654223  0.09554099]\n",
      "Gradient Descent(330/999): loss=0.4160763503160686, w = [-1.45015769  0.29911242 -0.50931062 -0.73413604 -0.11395149  0.6085302\n",
      "  0.66663123 -0.27332958  0.09953704  0.29105119 -0.00376369 -0.31311409\n",
      "  0.02180616 -0.09644082  0.09555938]\n",
      "Gradient Descent(331/999): loss=0.4160393093323235, w = [-1.45064492  0.2990661  -0.50939785 -0.73553475 -0.11392007  0.60954375\n",
      "  0.66700751 -0.27363957  0.0994792   0.29120592 -0.00377187 -0.31266782\n",
      "  0.0218118  -0.09633888  0.09557783]\n",
      "Gradient Descent(332/999): loss=0.41600237487582437, w = [-1.45112872  0.29901923 -0.50948386 -0.736932   -0.11388811  0.61055621\n",
      "  0.66738318 -0.27394924  0.09942107  0.29135985 -0.00377992 -0.31222172\n",
      "  0.02181747 -0.09623642  0.09559632]\n",
      "Gradient Descent(333/999): loss=0.4159655460929829, w = [-1.45160914  0.29897181 -0.50956865 -0.73832779 -0.1138556   0.61156758\n",
      "  0.66775823 -0.27425857  0.09936268  0.29151299 -0.00378786 -0.31177579\n",
      "  0.02182314 -0.09613345  0.09561484]\n",
      "Gradient Descent(334/999): loss=0.41592882214320726, w = [-1.45208619  0.29892384 -0.50965224 -0.73972213 -0.11382256  0.61257786\n",
      "  0.66813268 -0.27456759  0.09930401  0.29166536 -0.00379569 -0.31133002\n",
      "  0.02182884 -0.09602997  0.09563341]\n",
      "Gradient Descent(335/999): loss=0.4158922021986412, w = [-1.45255991  0.29887533 -0.50973463 -0.74111501 -0.11378898  0.61358706\n",
      "  0.66850652 -0.27487629  0.09924507  0.29181695 -0.0038034  -0.31088443\n",
      "  0.02183455 -0.09592599  0.095652  ]\n",
      "Gradient Descent(336/999): loss=0.4158556854439311, w = [-1.45303033  0.29882628 -0.50981585 -0.74250643 -0.11375487  0.61459518\n",
      "  0.66887975 -0.27518468  0.09918588  0.29196777 -0.00381101 -0.31043901\n",
      "  0.02184027 -0.09582151  0.09567063]\n",
      "Gradient Descent(337/999): loss=0.41581927107598005, w = [-1.45349749  0.29877669 -0.50989589 -0.74389641 -0.11372024  0.61560222\n",
      "  0.66925239 -0.27549277  0.09912642  0.29211782 -0.0038185  -0.30999376\n",
      "  0.021846   -0.09571655  0.09568928]\n",
      "Gradient Descent(338/999): loss=0.41578295830373213, w = [-1.45396142  0.29872656 -0.50997477 -0.74528493 -0.11368509  0.61660818\n",
      "  0.66962442 -0.27580055  0.09906672  0.29226712 -0.00382589 -0.30954868\n",
      "  0.02185175 -0.09561111  0.09570796]\n",
      "Gradient Descent(339/999): loss=0.4157467463479225, w = [-1.45442214  0.2986759  -0.5100525  -0.74667201 -0.11364942  0.61761308\n",
      "  0.66999587 -0.27610803  0.09900676  0.29241567 -0.00383317 -0.30910379\n",
      "  0.0218575  -0.09550519  0.09572666]\n",
      "Gradient Descent(340/999): loss=0.41571063444087425, w = [-1.45487968  0.2986247  -0.5101291  -0.74805764 -0.11361325  0.61861691\n",
      "  0.67036671 -0.27641522  0.09894656  0.29256346 -0.00384035 -0.30865907\n",
      "  0.02186326 -0.0953988   0.09574538]\n",
      "Gradient Descent(341/999): loss=0.4156746218262777, w = [-1.45533409  0.29857298 -0.51020457 -0.74944183 -0.11357656  0.61961967\n",
      "  0.67073697 -0.27672212  0.09888612  0.29271052 -0.00384743 -0.30821452\n",
      "  0.02186903 -0.09529196  0.09576412]\n",
      "Gradient Descent(342/999): loss=0.4156387077589673, w = [-1.45578538  0.29852072 -0.51027891 -0.75082457 -0.11353938  0.62062138\n",
      "  0.67110664 -0.27702873  0.09882544  0.29285684 -0.00385441 -0.30777016\n",
      "  0.02187481 -0.09518465  0.09578287]\n",
      "Gradient Descent(343/999): loss=0.4156028915047212, w = [-1.45623358  0.29846794 -0.51035215 -0.75220587 -0.1135017   0.62162202\n",
      "  0.67147573 -0.27733506  0.09876453  0.29300243 -0.00386128 -0.30732598\n",
      "  0.02188059 -0.0950769   0.09580163]\n",
      "Gradient Descent(344/999): loss=0.41556717234005525, w = [-1.45667873  0.29841463 -0.5104243  -0.75358574 -0.11346353  0.62262162\n",
      "  0.67184423 -0.27764112  0.09870339  0.29314729 -0.00386806 -0.30688198\n",
      "  0.02188637 -0.0949687   0.0958204 ]\n",
      "Gradient Descent(345/999): loss=0.4155315495520263, w = [-1.45712086  0.2983608  -0.51049536 -0.75496417 -0.11342487  0.62362016\n",
      "  0.67221216 -0.2779469   0.09864202  0.29329143 -0.00387475 -0.30643816\n",
      "  0.02189216 -0.09486007  0.09583917]\n",
      "Gradient Descent(346/999): loss=0.41549602243802886, w = [-1.45755998  0.29830645 -0.51056534 -0.75634116 -0.11338573  0.62461765\n",
      "  0.6725795  -0.27825241  0.09858044  0.29343485 -0.00388134 -0.30599453\n",
      "  0.02189795 -0.094751    0.09585795]\n",
      "Gradient Descent(347/999): loss=0.4154605903056109, w = [-1.45799614  0.29825159 -0.51063425 -0.75771672 -0.11334611  0.6256141\n",
      "  0.67294628 -0.27855765  0.09851863  0.29357756 -0.00388783 -0.30555109\n",
      "  0.02190374 -0.09464151  0.09587673]\n",
      "Gradient Descent(348/999): loss=0.4154252524722832, w = [-1.45842936  0.2981962  -0.5107021  -0.75909085 -0.11330602  0.6266095\n",
      "  0.67331248 -0.27886263  0.09845661  0.29371957 -0.00389424 -0.30510783\n",
      "  0.02190952 -0.0945316   0.09589551]\n",
      "Gradient Descent(349/999): loss=0.41539000826533046, w = [-1.45885967  0.2981403  -0.51076891 -0.76046355 -0.11326545  0.62760386\n",
      "  0.67367811 -0.27916735  0.09839438  0.29386087 -0.00390055 -0.30466476\n",
      "  0.02191531 -0.09442127  0.09591429]\n",
      "Gradient Descent(350/999): loss=0.41535485702164515, w = [-1.45928708  0.29808389 -0.51083468 -0.76183482 -0.11322442  0.62859719\n",
      "  0.67404317 -0.27947182  0.09833194  0.29400148 -0.00390678 -0.30422187\n",
      "  0.0219211  -0.09431054  0.09593306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(351/999): loss=0.4153197980875348, w = [-1.45971164  0.29802697 -0.51089943 -0.76320467 -0.11318293  0.62958949\n",
      "  0.67440767 -0.27977604  0.0982693   0.2941414  -0.00391291 -0.30377918\n",
      "  0.02192688 -0.0941994   0.09595182]\n",
      "Gradient Descent(352/999): loss=0.41528483081855566, w = [-1.46013337  0.29796953 -0.51096315 -0.76457309 -0.11314098  0.63058075\n",
      "  0.6747716  -0.28008     0.09820646  0.29428063 -0.00391897 -0.30333668\n",
      "  0.02193266 -0.09408787  0.09597057]\n",
      "Gradient Descent(353/999): loss=0.4152499545793478, w = [-1.4605523   0.2979116  -0.51102587 -0.76594009 -0.11309857  0.63157099\n",
      "  0.67513498 -0.28038373  0.09814342  0.29441917 -0.00392493 -0.30289437\n",
      "  0.02193843 -0.09397595  0.09598931]\n",
      "Gradient Descent(354/999): loss=0.41521516874346304, w = [-1.46096844  0.29785316 -0.51108758 -0.76730568 -0.11305572  0.6325602\n",
      "  0.6754978  -0.28068721  0.09808018  0.29455704 -0.00393081 -0.30245225\n",
      "  0.0219442  -0.09386364  0.09600804]\n",
      "Gradient Descent(355/999): loss=0.4151804726932115, w = [-1.46138183  0.29779421 -0.51114831 -0.76866984 -0.11301242  0.63354839\n",
      "  0.67586006 -0.28099045  0.09801676  0.29469424 -0.00393661 -0.30201033\n",
      "  0.02194997 -0.09375095  0.09602675]\n",
      "Gradient Descent(356/999): loss=0.4151458658194825, w = [-1.4617925   0.29773477 -0.51120805 -0.77003259 -0.11296868  0.63453556\n",
      "  0.67622177 -0.28129347  0.09795315  0.29483077 -0.00394233 -0.3015686\n",
      "  0.02195572 -0.09363788  0.09604544]\n",
      "Gradient Descent(357/999): loss=0.4151113475216154, w = [-1.46220046  0.29767483 -0.51126682 -0.77139393 -0.1129245   0.63552171\n",
      "  0.67658294 -0.28159625  0.09788936  0.29496663 -0.00394797 -0.30112706\n",
      "  0.02196147 -0.09352445  0.09606411]\n",
      "Gradient Descent(358/999): loss=0.4150769172072332, w = [-1.46260574  0.29761439 -0.51132463 -0.77275385 -0.11287988  0.63650685\n",
      "  0.67694355 -0.2818988   0.09782539  0.29510183 -0.00395353 -0.30068573\n",
      "  0.02196721 -0.09341065  0.09608276]\n",
      "Gradient Descent(359/999): loss=0.41504257429208036, w = [-1.46300838  0.29755346 -0.51138148 -0.77411236 -0.11283484  0.63749098\n",
      "  0.67730362 -0.28220113  0.09776124  0.29523638 -0.003959   -0.30024459\n",
      "  0.02197294 -0.0932965   0.09610139]\n",
      "Gradient Descent(360/999): loss=0.415008318199908, w = [-1.46340839  0.29749203 -0.51143739 -0.77546947 -0.11278937  0.6384741\n",
      "  0.67766314 -0.28250323  0.09769692  0.29537028 -0.00396441 -0.29980365\n",
      "  0.02197866 -0.09318199  0.09611999]\n",
      "Gradient Descent(361/999): loss=0.41497414836229546, w = [-1.46380579  0.29743012 -0.51149236 -0.77682516 -0.11274348  0.63945621\n",
      "  0.67802212 -0.28280512  0.09763243  0.29550353 -0.00396973 -0.29936291\n",
      "  0.02198437 -0.09306713  0.09613857]\n",
      "Gradient Descent(362/999): loss=0.41494006421853646, w = [-1.46420062  0.29736772 -0.5115464  -0.77817946 -0.11269717  0.64043733\n",
      "  0.67838056 -0.28310679  0.09756777  0.29563614 -0.00397499 -0.29892236\n",
      "  0.02199007 -0.09295193  0.09615711]\n",
      "Gradient Descent(363/999): loss=0.41490606521548795, w = [-1.46459289  0.29730483 -0.51159952 -0.77953235 -0.11265044  0.64141744\n",
      "  0.67873847 -0.28340826  0.09750294  0.29576811 -0.00398016 -0.29848202\n",
      "  0.02199576 -0.0928364   0.09617562]\n",
      "Gradient Descent(364/999): loss=0.4148721508074326, w = [-1.46498264  0.29724146 -0.51165173 -0.78088384 -0.1126033   0.64239655\n",
      "  0.67909584 -0.28370951  0.09743796  0.29589945 -0.00398527 -0.29804188\n",
      "  0.02200143 -0.09272053  0.0961941 ]\n",
      "Gradient Descent(365/999): loss=0.4148383204559553, w = [-1.46536987  0.29717761 -0.51170303 -0.78223394 -0.11255576  0.64337467\n",
      "  0.67945268 -0.28401056  0.09737281  0.29603015 -0.0039903  -0.29760195\n",
      "  0.0220071  -0.09260433  0.09621255]\n",
      "Gradient Descent(366/999): loss=0.4148045736298164, w = [-1.46575463  0.29711327 -0.51175344 -0.78358264 -0.11250781  0.6443518\n",
      "  0.67980899 -0.2843114   0.09730751  0.29616024 -0.00399526 -0.29716221\n",
      "  0.02201275 -0.09248781  0.09623096]\n",
      "Gradient Descent(367/999): loss=0.41477090980480696, w = [-1.46613692  0.29704846 -0.51180297 -0.78492994 -0.11245946  0.64532794\n",
      "  0.68016478 -0.28461205  0.09724206  0.2962897  -0.00400015 -0.29672268\n",
      "  0.02201838 -0.09237098  0.09624933]\n",
      "Gradient Descent(368/999): loss=0.414737328463642, w = [-1.46651677  0.29698318 -0.51185162 -0.78627585 -0.11241072  0.64630309\n",
      "  0.68052003 -0.28491249  0.09717645  0.29641854 -0.00400498 -0.29628336\n",
      "  0.022024   -0.09225383  0.09626767]\n",
      "Gradient Descent(369/999): loss=0.4147038290958238, w = [-1.46689421  0.29691742 -0.51189939 -0.78762037 -0.11236158  0.64727725\n",
      "  0.68087477 -0.28521275  0.0971107   0.29654677 -0.00400973 -0.29584424\n",
      "  0.02202961 -0.09213637  0.09628596]\n",
      "Gradient Descent(370/999): loss=0.4146704111975339, w = [-1.46726926  0.29685119 -0.51194631 -0.7889635  -0.11231205  0.64825043\n",
      "  0.68122898 -0.28551281  0.09704481  0.29667439 -0.00401442 -0.29540532\n",
      "  0.0220352  -0.09201861  0.09630421]\n",
      "Gradient Descent(371/999): loss=0.4146370742715183, w = [-1.46764194  0.29678449 -0.51199236 -0.79030525 -0.11226214  0.64922264\n",
      "  0.68158267 -0.28581268  0.09697878  0.29680141 -0.00401904 -0.29496662\n",
      "  0.02204077 -0.09190056  0.09632242]\n",
      "Gradient Descent(372/999): loss=0.41460381782695405, w = [-1.46801226  0.29671732 -0.51203758 -0.79164561 -0.11221184  0.65019386\n",
      "  0.68193584 -0.28611237  0.0969126   0.29692782 -0.00402359 -0.29452812\n",
      "  0.02204633 -0.09178221  0.09634058]\n",
      "Gradient Descent(373/999): loss=0.4145706413793578, w = [-1.46838027  0.29664968 -0.51208195 -0.79298459 -0.11216116  0.65116411\n",
      "  0.6822885  -0.28641187  0.09684629  0.29705364 -0.00402808 -0.29408982\n",
      "  0.02205187 -0.09166357  0.0963587 ]\n",
      "Gradient Descent(374/999): loss=0.41453754445045726, w = [-1.46874596  0.29658158 -0.51212549 -0.79432219 -0.11211011  0.65213339\n",
      "  0.68264065 -0.2867112   0.09677985  0.29717886 -0.00403251 -0.29365174\n",
      "  0.02205739 -0.09154465  0.09637677]\n",
      "Gradient Descent(375/999): loss=0.41450452656809406, w = [-1.46910937  0.29651302 -0.51216821 -0.79565841 -0.11205869  0.6531017\n",
      "  0.68299228 -0.28701034  0.09671327  0.29730349 -0.00403687 -0.29321387\n",
      "  0.0220629  -0.09142545  0.09639479]\n",
      "Gradient Descent(376/999): loss=0.41447158726612127, w = [-1.46947052  0.296444   -0.51221012 -0.79699325 -0.11200689  0.65406904\n",
      "  0.68334341 -0.28730931  0.09664657  0.29742753 -0.00404117 -0.2927762\n",
      "  0.02206838 -0.09130597  0.09641276]\n",
      "Gradient Descent(377/999): loss=0.4144387260842834, w = [-1.46982942  0.29637453 -0.51225121 -0.79832672 -0.11195473  0.65503541\n",
      "  0.68369403 -0.28760811  0.09657974  0.297551   -0.00404541 -0.29233875\n",
      "  0.02207385 -0.09118623  0.09643068]\n",
      "Gradient Descent(378/999): loss=0.414405942568128, w = [-1.4701861   0.29630459 -0.5122915  -0.79965882 -0.11190221  0.65600082\n",
      "  0.68404415 -0.28790673  0.09651279  0.29767388 -0.00404959 -0.29190151\n",
      "  0.0220793  -0.09106621  0.09644854]\n",
      "Gradient Descent(379/999): loss=0.41437323626890105, w = [-1.47054058  0.29623421 -0.512331   -0.80098955 -0.11184933  0.65696527\n",
      "  0.68439376 -0.28820519  0.09644572  0.29779619 -0.00405371 -0.29146447\n",
      "  0.02208473 -0.09094594  0.09646635]\n",
      "Gradient Descent(380/999): loss=0.41434060674345263, w = [-1.47089288  0.29616337 -0.51236972 -0.80231891 -0.11179609  0.65792876\n",
      "  0.68474287 -0.28850348  0.09637852  0.29791792 -0.00405777 -0.29102766\n",
      "  0.02209014 -0.09082541  0.09648411]\n",
      "Gradient Descent(381/999): loss=0.41430805355412736, w = [-1.47124302  0.29609208 -0.51240765 -0.8036469  -0.1117425   0.6588913\n",
      "  0.68509148 -0.28880161  0.09631122  0.29803909 -0.00406177 -0.29059105\n",
      "  0.02209553 -0.09070463  0.09650181]\n",
      "Gradient Descent(382/999): loss=0.414275576268688, w = [-1.47159101  0.29602034 -0.51244481 -0.80497353 -0.11168856  0.65985288\n",
      "  0.68543959 -0.28909958  0.0962438   0.29815969 -0.00406571 -0.29015466\n",
      "  0.0221009  -0.0905836   0.09651945]\n",
      "Gradient Descent(383/999): loss=0.4142431744602145, w = [-1.47193688  0.29594816 -0.51248121 -0.80629879 -0.11163427  0.66081351\n",
      "  0.68578721 -0.28939738  0.09617627  0.29827973 -0.0040696  -0.28971848\n",
      "  0.02210625 -0.09046233  0.09653704]\n",
      "Gradient Descent(384/999): loss=0.41421084770701255, w = [-1.47228065  0.29587553 -0.51251685 -0.8076227  -0.11157964  0.66177319\n",
      "  0.68613434 -0.28969504  0.09610863  0.29839922 -0.00407343 -0.28928251\n",
      "  0.02211158 -0.09034082  0.09655456]\n",
      "Gradient Descent(385/999): loss=0.4141785955925191, w = [-1.47262234  0.29580246 -0.51255174 -0.80894525 -0.11152467  0.66273192\n",
      "  0.68648097 -0.28999254  0.09604088  0.29851814 -0.0040772  -0.28884676\n",
      "  0.02211688 -0.09021907  0.09657203]\n",
      "Gradient Descent(386/999): loss=0.4141464177052354, w = [-1.47296196  0.29572896 -0.51258588 -0.81026644 -0.11146936  0.66368971\n",
      "  0.68682712 -0.29028988  0.09597303  0.29863652 -0.00408092 -0.28841122\n",
      "  0.02212217 -0.0900971   0.09658943]\n",
      "Gradient Descent(387/999): loss=0.4141143136386125, w = [-1.47329954  0.29565501 -0.51261929 -0.81158628 -0.11141371  0.66464656\n",
      "  0.68717278 -0.29058708  0.09590508  0.29875435 -0.00408458 -0.2879759\n",
      "  0.02212743 -0.08997489  0.09660677]\n",
      "Gradient Descent(388/999): loss=0.4140822829909995, w = [-1.47363508  0.29558063 -0.51265197 -0.81290476 -0.11135773  0.66560246\n",
      "  0.68751795 -0.29088413  0.09583702  0.29887164 -0.00408819 -0.2875408\n",
      "  0.02213267 -0.08985246  0.09662404]\n",
      "Gradient Descent(389/999): loss=0.4140503253655241, w = [-1.47396862  0.29550581 -0.51268392 -0.8142219  -0.11130143  0.66655743\n",
      "  0.68786264 -0.29118104  0.09576888  0.29898838 -0.00409175 -0.28710591\n",
      "  0.02213789 -0.08972982  0.09664126]\n",
      "Gradient Descent(390/999): loss=0.4140184403700505, w = [-1.47430017  0.29543056 -0.51271515 -0.81553768 -0.1112448   0.66751146\n",
      "  0.68820684 -0.2914778   0.09570063  0.29910459 -0.00409525 -0.28667124\n",
      "  0.02214309 -0.08960696  0.0966584 ]\n",
      "Gradient Descent(391/999): loss=0.41398662761707483, w = [-1.47462975  0.29535488 -0.51274568 -0.81685212 -0.11118785  0.66846455\n",
      "  0.68855057 -0.29177442  0.09563229  0.29922026 -0.0040987  -0.28623679\n",
      "  0.02214826 -0.08948389  0.09667548]\n",
      "Gradient Descent(392/999): loss=0.4139548867236542, w = [-1.47495737  0.29527878 -0.5127755  -0.81816522 -0.11113057  0.66941672\n",
      "  0.68889381 -0.2920709   0.09556386  0.2993354  -0.0041021  -0.28580255\n",
      "  0.02215341 -0.08936061  0.09669249]\n",
      "Gradient Descent(393/999): loss=0.41392321731132725, w = [-1.47528306  0.29520224 -0.51280462 -0.81947697 -0.11107298  0.67036795\n",
      "  0.68923659 -0.29236724  0.09549534  0.29945001 -0.00410545 -0.28536854\n",
      "  0.02215854 -0.08923714  0.09670944]\n",
      "Gradient Descent(394/999): loss=0.4138916190060527, w = [-1.47560682  0.29512528 -0.51283305 -0.82078739 -0.11101508  0.67131826\n",
      "  0.68957888 -0.29266345  0.09542674  0.29956409 -0.00410875 -0.28493474\n",
      "  0.02216364 -0.08911346  0.09672631]\n",
      "Gradient Descent(395/999): loss=0.4138600914381176, w = [-1.47592869  0.2950479  -0.5128608  -0.82209647 -0.11095687  0.67226764\n",
      "  0.68992071 -0.29295953  0.09535805  0.29967766 -0.004112   -0.28450116\n",
      "  0.02216872 -0.08898959  0.09674312]\n",
      "Gradient Descent(396/999): loss=0.41382863424208033, w = [-1.47624867  0.2949701  -0.51288787 -0.82340421 -0.11089834  0.67321609\n",
      "  0.69026206 -0.29325547  0.09528927  0.2997907  -0.0041152  -0.2840678\n",
      "  0.02217378 -0.08886552  0.09675986]\n",
      "Gradient Descent(397/999): loss=0.41379724705669346, w = [-1.47656678  0.29489188 -0.51291426 -0.82471061 -0.11083951  0.67416362\n",
      "  0.69060294 -0.29355129  0.09522042  0.29990323 -0.00411835 -0.28363465\n",
      "  0.02217881 -0.08874127  0.09677652]\n",
      "Gradient Descent(398/999): loss=0.41376592952483426, w = [-1.47688304  0.29481324 -0.51293999 -0.82601569 -0.11078038  0.67511024\n",
      "  0.69094336 -0.29384698  0.09515148  0.30001525 -0.00412146 -0.28320173\n",
      "  0.02218382 -0.08861684  0.09679311]\n",
      "Gradient Descent(399/999): loss=0.4137346812934372, w = [-1.47719747  0.29473419 -0.51296506 -0.82731943 -0.11072095  0.67605593\n",
      "  0.6912833  -0.29414254  0.09508247  0.30012675 -0.00412451 -0.28276903\n",
      "  0.0221888  -0.08849223  0.09680963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(400/999): loss=0.41370350201342876, w = [-1.47751008  0.29465472 -0.51298948 -0.82862185 -0.11066122  0.67700071\n",
      "  0.69162279 -0.29443798  0.09501338  0.30023775 -0.00412752 -0.28233655\n",
      "  0.02219376 -0.08836744  0.09682608]\n",
      "Gradient Descent(401/999): loss=0.41367239133966327, w = [-1.47782088  0.29457485 -0.51301324 -0.82992294 -0.1106012   0.67794458\n",
      "  0.69196181 -0.2947333   0.09494422  0.30034825 -0.00413048 -0.28190429\n",
      "  0.02219869 -0.08824248  0.09684245]\n",
      "Gradient Descent(402/999): loss=0.4136413489308528, w = [-1.47812991  0.29449456 -0.51303637 -0.83122271 -0.11054088  0.67888753\n",
      "  0.69230037 -0.2950285   0.09487499  0.30045824 -0.0041334  -0.28147225\n",
      "  0.0222036  -0.08811735  0.09685875]\n",
      "Gradient Descent(403/999): loss=0.4136103744495139, w = [-1.47843716  0.29441387 -0.51305886 -0.83252116 -0.11048027  0.67982958\n",
      "  0.69263847 -0.29532358  0.09480569  0.30056774 -0.00413627 -0.28104043\n",
      "  0.02220848 -0.08799206  0.09687497]\n",
      "Gradient Descent(404/999): loss=0.41357946756189534, w = [-1.47874266  0.29433277 -0.51308071 -0.83381828 -0.11041938  0.68077071\n",
      "  0.69297612 -0.29561854  0.09473632  0.30067674 -0.00413909 -0.28060884\n",
      "  0.02221334 -0.08786661  0.09689111]\n",
      "Gradient Descent(405/999): loss=0.4135486279379301, w = [-1.47904643  0.29425126 -0.51310195 -0.83511409 -0.1103582   0.68171094\n",
      "  0.69331331 -0.29591339  0.09466688  0.30078524 -0.00414187 -0.28017746\n",
      "  0.02221817 -0.08774099  0.09690718]\n",
      "Gradient Descent(406/999): loss=0.4135178552511603, w = [-1.47934847  0.29416936 -0.51312256 -0.83640859 -0.11029674  0.68265026\n",
      "  0.69365004 -0.29620813  0.09459738  0.30089326 -0.00414461 -0.27974631\n",
      "  0.02222298 -0.08761523  0.09692318]\n",
      "Gradient Descent(407/999): loss=0.41348714917869206, w = [-1.47964881  0.29408705 -0.51314256 -0.83770177 -0.11023501  0.68358869\n",
      "  0.69398633 -0.29650275  0.09452782  0.30100079 -0.0041473  -0.27931538\n",
      "  0.02222776 -0.08748931  0.09693909]\n",
      "Gradient Descent(408/999): loss=0.413456509401132, w = [-1.47994745  0.29400435 -0.51316195 -0.83899364 -0.11017299  0.68452621\n",
      "  0.69432216 -0.29679727  0.0944582   0.30110784 -0.00414995 -0.27888468\n",
      "  0.02223252 -0.08736324  0.09695493]\n",
      "Gradient Descent(409/999): loss=0.41342593560253543, w = [-1.48024442  0.29392125 -0.51318074 -0.8402842  -0.1101107   0.68546283\n",
      "  0.69465754 -0.29709167  0.09438851  0.3012144  -0.00415256 -0.27845419\n",
      "  0.02223725 -0.08723704  0.09697068]\n",
      "Gradient Descent(410/999): loss=0.4133954274703389, w = [-1.48053973  0.29383776 -0.51319894 -0.84157345 -0.11004814  0.68639855\n",
      "  0.69499247 -0.29738597  0.09431877  0.30132049 -0.00415512 -0.27802393\n",
      "  0.02224195 -0.08711069  0.09698636]\n",
      "Gradient Descent(411/999): loss=0.4133649846953295, w = [-1.4808334   0.29375387 -0.51321654 -0.8428614  -0.10998531  0.68733339\n",
      "  0.69532696 -0.29768017  0.09424897  0.3014261  -0.00415765 -0.2775939\n",
      "  0.02224663 -0.0869842   0.09700196]\n",
      "Gradient Descent(412/999): loss=0.41333460697156815, w = [-1.48112543  0.29366959 -0.51323356 -0.84414804 -0.10992222  0.68826732\n",
      "  0.695661   -0.29797426  0.09417912  0.30153123 -0.00416013 -0.27716408\n",
      "  0.02225128 -0.08685758  0.09701747]\n",
      "Gradient Descent(413/999): loss=0.4133042939963532, w = [-1.48141584  0.29358493 -0.51325    -0.84543338 -0.10985886  0.68920037\n",
      "  0.69599461 -0.29826825  0.09410922  0.3016359  -0.00416257 -0.27673449\n",
      "  0.02225591 -0.08673083  0.09703291]\n",
      "Gradient Descent(414/999): loss=0.41327404547015334, w = [-1.48170466  0.29349987 -0.51326586 -0.84671743 -0.10979523  0.69013252\n",
      "  0.69632776 -0.29856213  0.09403926  0.3017401  -0.00416497 -0.27630513\n",
      "  0.02226051 -0.08660396  0.09704826]\n",
      "Gradient Descent(415/999): loss=0.41324386109658134, w = [-1.48199188  0.29341443 -0.51328115 -0.84800017 -0.10973135  0.69106379\n",
      "  0.69666048 -0.29885592  0.09396925  0.30184383 -0.00416732 -0.27587598\n",
      "  0.02226508 -0.08647695  0.09706354]\n",
      "Gradient Descent(416/999): loss=0.4132137405823161, w = [-1.48227753  0.29332861 -0.51329588 -0.84928162 -0.10966721  0.69199417\n",
      "  0.69699277 -0.29914962  0.0938992   0.30194709 -0.00416964 -0.27544707\n",
      "  0.02226963 -0.08634983  0.09707873]\n",
      "Gradient Descent(417/999): loss=0.4131836836370841, w = [-1.48256163  0.29324241 -0.51331005 -0.85056178 -0.10960282  0.69292367\n",
      "  0.69732461 -0.29944321  0.0938291   0.3020499  -0.00417192 -0.27501837\n",
      "  0.02227415 -0.08622259  0.09709384]\n",
      "Gradient Descent(418/999): loss=0.41315368997358526, w = [-1.48284417  0.29315582 -0.51332366 -0.85184065 -0.10953817  0.69385228\n",
      "  0.69765602 -0.29973671  0.09375895  0.30215225 -0.00417416 -0.27458991\n",
      "  0.02227864 -0.08609524  0.09710886]\n",
      "Gradient Descent(419/999): loss=0.4131237593074606, w = [-1.48312519  0.29306886 -0.51333672 -0.85311823 -0.10947328  0.69478002\n",
      "  0.697987   -0.30003012  0.09368876  0.30225414 -0.00417636 -0.27416166\n",
      "  0.02228311 -0.08596777  0.0971238 ]\n",
      "Gradient Descent(420/999): loss=0.41309389135725033, w = [-1.48340468  0.29298152 -0.51334924 -0.85439452 -0.10940813  0.69570687\n",
      "  0.69831754 -0.30032344  0.09361853  0.30235558 -0.00417852 -0.27373364\n",
      "  0.02228755 -0.0858402   0.09713866]\n",
      "Gradient Descent(421/999): loss=0.41306408584433324, w = [-1.48368267  0.29289381 -0.51336123 -0.85566952 -0.10934274  0.69663285\n",
      "  0.69864765 -0.30061666  0.09354826  0.30245657 -0.00418065 -0.27330585\n",
      "  0.02229197 -0.08571252  0.09715344]\n",
      "Gradient Descent(422/999): loss=0.41303434249289334, w = [-1.48395917  0.29280572 -0.51337267 -0.85694325 -0.10927711  0.69755795\n",
      "  0.69897734 -0.3009098   0.09347795  0.30255711 -0.00418274 -0.27287828\n",
      "  0.02229635 -0.08558474  0.09716813]\n",
      "Gradient Descent(423/999): loss=0.41300466102988564, w = [-1.48423419  0.29271726 -0.51338359 -0.85821569 -0.10921123  0.69848218\n",
      "  0.69930659 -0.30120285  0.0934076   0.30265721 -0.00418478 -0.27245094\n",
      "  0.02230071 -0.08545686  0.09718273]\n",
      "Gradient Descent(424/999): loss=0.41297504118496886, w = [-1.48450775  0.29262844 -0.51339398 -0.85948685 -0.10914512  0.69940553\n",
      "  0.69963542 -0.30149581  0.09333721  0.30275686 -0.0041868  -0.27202382\n",
      "  0.02230505 -0.08532889  0.09719725]\n",
      "Gradient Descent(425/999): loss=0.4129454826904857, w = [-1.48477985  0.29253924 -0.51340386 -0.86075674 -0.10907877  0.70032801\n",
      "  0.69996383 -0.30178868  0.09326679  0.30285607 -0.00418877 -0.27159693\n",
      "  0.02230936 -0.08520082  0.09721169]\n",
      "Gradient Descent(426/999): loss=0.41291598528141354, w = [-1.48505051  0.29244968 -0.51341322 -0.86202535 -0.10901219  0.70124963\n",
      "  0.70029181 -0.30208148  0.09319633  0.30295484 -0.00419071 -0.27117026\n",
      "  0.02231364 -0.08507266  0.09722603]\n",
      "Gradient Descent(427/999): loss=0.41288654869531705, w = [-1.48531975  0.29235976 -0.51342207 -0.86329269 -0.10894537  0.70217037\n",
      "  0.70061937 -0.30237419  0.09312584  0.30305318 -0.00419261 -0.27074382\n",
      "  0.02231789 -0.08494441  0.0972403 ]\n",
      "Gradient Descent(428/999): loss=0.4128571726723327, w = [-1.48558757  0.29226947 -0.51343041 -0.86455876 -0.10887833  0.70309026\n",
      "  0.70094651 -0.30266681  0.09305532  0.30315108 -0.00419448 -0.27031761\n",
      "  0.02232212 -0.08481608  0.09725448]\n",
      "Gradient Descent(429/999): loss=0.41282785695509605, w = [-1.48585399  0.29217883 -0.51343825 -0.86582356 -0.10881105  0.70400927\n",
      "  0.70127323 -0.30295936  0.09298477  0.30324855 -0.00419631 -0.26989162\n",
      "  0.02232632 -0.08468767  0.09726857]\n",
      "Gradient Descent(430/999): loss=0.4127986012887309, w = [-1.48611902  0.29208782 -0.5134456  -0.86708709 -0.10874355  0.70492743\n",
      "  0.70159953 -0.30325183  0.09291419  0.30334558 -0.00419811 -0.26946586\n",
      "  0.0223305  -0.08455918  0.09728257]\n",
      "Gradient Descent(431/999): loss=0.41276940542080204, w = [-1.48638268  0.29199646 -0.51345245 -0.86834936 -0.10867583  0.70584472\n",
      "  0.70192541 -0.30354422  0.09284359  0.3034422  -0.00419987 -0.26904032\n",
      "  0.02233464 -0.08443061  0.09729649]\n",
      "Gradient Descent(432/999): loss=0.41274026910127176, w = [-1.48664497  0.29190475 -0.51345882 -0.86961036 -0.10860788  0.70676115\n",
      "  0.70225088 -0.30383653  0.09277296  0.30353838 -0.0042016  -0.26861501\n",
      "  0.02233876 -0.08430197  0.09731032]\n",
      "Gradient Descent(433/999): loss=0.41271119208247947, w = [-1.4869059   0.29181268 -0.5134647  -0.87087011 -0.10853972  0.70767673\n",
      "  0.70257594 -0.30412877  0.0927023   0.30363415 -0.0042033  -0.26818993\n",
      "  0.02234286 -0.08417326  0.09732406]\n",
      "Gradient Descent(434/999): loss=0.41268217411909375, w = [-1.4871655   0.29172025 -0.51347011 -0.87212859 -0.10847133  0.70859145\n",
      "  0.70290059 -0.30442093  0.09263162  0.30372949 -0.00420495 -0.26776507\n",
      "  0.02234693 -0.08404448  0.09733772]\n",
      "Gradient Descent(435/999): loss=0.41265321496807766, w = [-1.48742376  0.29162748 -0.51347504 -0.87338582 -0.10840274  0.70950531\n",
      "  0.70322482 -0.30471302  0.09256091  0.30382441 -0.00420658 -0.26734044\n",
      "  0.02235097 -0.08391564  0.09735129]\n",
      "Gradient Descent(436/999): loss=0.4126243143886632, w = [-1.48768071  0.29153436 -0.51347951 -0.8746418  -0.10833392  0.71041832\n",
      "  0.70354865 -0.30500504  0.09249019  0.30391892 -0.00420817 -0.26691603\n",
      "  0.02235498 -0.08378673  0.09736477]\n",
      "Gradient Descent(437/999): loss=0.4125954721423124, w = [-1.48793635  0.29144089 -0.5134835  -0.87589652 -0.1082649   0.71133048\n",
      "  0.70387207 -0.30529698  0.09241944  0.30401301 -0.00420973 -0.26649185\n",
      "  0.02235897 -0.08365776  0.09737816]\n",
      "Gradient Descent(438/999): loss=0.412566687992678, w = [-1.4881907   0.29134708 -0.51348704 -0.87714999 -0.10819567  0.71224179\n",
      "  0.70419508 -0.30558886  0.09234868  0.30410669 -0.00421126 -0.2660679\n",
      "  0.02236293 -0.08352874  0.09739147]\n",
      "Gradient Descent(439/999): loss=0.41253796170558527, w = [-1.48844376  0.29125293 -0.51349012 -0.87840222 -0.10812623  0.71315225\n",
      "  0.70451768 -0.30588066  0.09227789  0.30419996 -0.00421276 -0.26564417\n",
      "  0.02236686 -0.08339966  0.09740469]\n",
      "Gradient Descent(440/999): loss=0.41250929304899053, w = [-1.48869556  0.29115843 -0.51349275 -0.8796532  -0.10805658  0.71406186\n",
      "  0.70483988 -0.3061724   0.09220709  0.30429282 -0.00421422 -0.26522067\n",
      "  0.02237077 -0.08327053  0.09741781]\n",
      "Gradient Descent(441/999): loss=0.41248068179295283, w = [-1.48894609  0.29106359 -0.51349493 -0.88090293 -0.10798673  0.71497063\n",
      "  0.70516168 -0.30646407  0.09213628  0.30438528 -0.00421565 -0.2647974\n",
      "  0.02237465 -0.08314136  0.09743086]\n",
      "Gradient Descent(442/999): loss=0.41245212770959533, w = [-1.48919537  0.29096842 -0.51349666 -0.88215142 -0.10791668  0.71587855\n",
      "  0.70548308 -0.30675567  0.09206545  0.30447733 -0.00421705 -0.26437435\n",
      "  0.0223785  -0.08301213  0.09744381]\n",
      "Gradient Descent(443/999): loss=0.4124236305730917, w = [-1.48944341  0.2908729  -0.51349796 -0.88339867 -0.10784643  0.71678564\n",
      "  0.70580408 -0.30704721  0.0919946   0.30456898 -0.00421842 -0.26395153\n",
      "  0.02238233 -0.08288286  0.09745667]\n",
      "Gradient Descent(444/999): loss=0.41239519015961995, w = [-1.48969023  0.29077706 -0.51349882 -0.88464468 -0.10777598  0.71769188\n",
      "  0.70612467 -0.30733868  0.09192375  0.30466023 -0.00421975 -0.26352894\n",
      "  0.02238613 -0.08275355  0.09746945]\n",
      "Gradient Descent(445/999): loss=0.41236680624734545, w = [-1.48993582  0.29068088 -0.51349924 -0.88588946 -0.10770533  0.71859728\n",
      "  0.70644487 -0.30763009  0.09185288  0.30475108 -0.00422106 -0.26310657\n",
      "  0.0223899  -0.0826242   0.09748213]\n",
      "Gradient Descent(446/999): loss=0.41233847861638295, w = [-1.49018021  0.29058437 -0.51349924 -0.887133   -0.10763449  0.71950184\n",
      "  0.70676468 -0.30792144  0.091782    0.30484154 -0.00422234 -0.26268443\n",
      "  0.02239365 -0.08249481  0.09749473]\n",
      "Gradient Descent(447/999): loss=0.4123102070487751, w = [-1.49042341  0.29048753 -0.51349882 -0.8883753  -0.10756346  0.72040556\n",
      "  0.70708409 -0.30821273  0.09171111  0.3049316  -0.00422358 -0.26226252\n",
      "  0.02239737 -0.08236539  0.09750724]\n",
      "Gradient Descent(448/999): loss=0.4122819913284605, w = [-1.49066541  0.29039036 -0.51349797 -0.88961638 -0.10749224  0.72130845\n",
      "  0.7074031  -0.30850395  0.09164021  0.30502127 -0.0042248  -0.26184083\n",
      "  0.02240107 -0.08223593  0.09751966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(449/999): loss=0.4122538312412501, w = [-1.49090625  0.29029286 -0.51349671 -0.89085623 -0.10742083  0.72221051\n",
      "  0.70772173 -0.30879512  0.09156931  0.30511054 -0.00422598 -0.26141937\n",
      "  0.02240473 -0.08210645  0.097532  ]\n",
      "Gradient Descent(450/999): loss=0.4122257265747965, w = [-1.49114591  0.29019504 -0.51349504 -0.89209485 -0.10734923  0.72311173\n",
      "  0.70803996 -0.30908622  0.0914984   0.30519943 -0.00422714 -0.26099813\n",
      "  0.02240838 -0.08197694  0.09754424]\n",
      "Gradient Descent(451/999): loss=0.41219767711857724, w = [-1.49138443  0.29009689 -0.51349296 -0.89333225 -0.10727745  0.72401212\n",
      "  0.7083578  -0.30937727  0.09142748  0.30528794 -0.00422826 -0.26057712\n",
      "  0.02241199 -0.0818474   0.09755639]\n",
      "Gradient Descent(452/999): loss=0.4121696826638477, w = [-1.49162179  0.28999843 -0.51349047 -0.89456843 -0.10720548  0.72491168\n",
      "  0.70867525 -0.30966826  0.09135656  0.30537606 -0.00422936 -0.26015634\n",
      "  0.02241558 -0.08171784  0.09756846]\n",
      "Gradient Descent(453/999): loss=0.41214174300364603, w = [-1.49185802  0.28989964 -0.51348758 -0.89580338 -0.10713333  0.72581042\n",
      "  0.70899232 -0.3099592   0.09128563  0.30546379 -0.00423043 -0.25973578\n",
      "  0.02241915 -0.08158826  0.09758044]\n",
      "Gradient Descent(454/999): loss=0.4121138579327435, w = [-1.49209313  0.28980053 -0.51348429 -0.89703712 -0.107061    0.72670833\n",
      "  0.709309   -0.31025008  0.0912147   0.30555115 -0.00423147 -0.25931545\n",
      "  0.02242268 -0.08145866  0.09759232]\n",
      "Gradient Descent(455/999): loss=0.41208602724763005, w = [-1.49232712  0.28970111 -0.51348061 -0.89826964 -0.1069885   0.72760541\n",
      "  0.70962529 -0.3105409   0.09114377  0.30563812 -0.00423248 -0.25889535\n",
      "  0.02242619 -0.08132905  0.09760412]\n",
      "Gradient Descent(456/999): loss=0.4120582507464916, w = [-1.49256001  0.28960138 -0.51347654 -0.89950095 -0.10691582  0.72850167\n",
      "  0.7099412  -0.31083167  0.09107284  0.30572472 -0.00423346 -0.25847547\n",
      "  0.02242968 -0.08119942  0.09761583]\n",
      "Gradient Descent(457/999): loss=0.4120305282291761, w = [-1.49279179  0.28950132 -0.51347209 -0.90073104 -0.10684296  0.7293971\n",
      "  0.71025673 -0.31112238  0.09100191  0.30581094 -0.00423442 -0.25805582\n",
      "  0.02243314 -0.08106978  0.09762746]\n",
      "Gradient Descent(458/999): loss=0.41200285949718857, w = [-1.49302249  0.28940096 -0.51346725 -0.90195992 -0.10676993  0.73029171\n",
      "  0.71057188 -0.31141305  0.09093098  0.30589679 -0.00423535 -0.25763639\n",
      "  0.02243657 -0.08094013  0.09763899]\n",
      "Gradient Descent(459/999): loss=0.411975244353654, w = [-1.49325212  0.28930029 -0.51346203 -0.9031876  -0.10669673  0.73118551\n",
      "  0.71088664 -0.31170366  0.09086005  0.30598227 -0.00423624 -0.25721719\n",
      "  0.02243998 -0.08081048  0.09765043]\n",
      "Gradient Descent(460/999): loss=0.411947682603295, w = [-1.49348067  0.2891993  -0.51345643 -0.90441407 -0.10662335  0.73207848\n",
      "  0.71120103 -0.31199422  0.09078912  0.30606738 -0.00423712 -0.25679822\n",
      "  0.02244336 -0.08068081  0.09766179]\n",
      "Gradient Descent(461/999): loss=0.41192017405242154, w = [-1.49370817  0.28909801 -0.51345047 -0.90563933 -0.10654982  0.73297064\n",
      "  0.71151504 -0.31228473  0.0907182   0.30615211 -0.00423796 -0.25637947\n",
      "  0.02244672 -0.08055115  0.09767306]\n",
      "Gradient Descent(462/999): loss=0.4118927185088935, w = [-1.49393462  0.28899642 -0.51344413 -0.90686339 -0.10647611  0.73386198\n",
      "  0.71182867 -0.31257519  0.09064728  0.30623648 -0.00423878 -0.25596095\n",
      "  0.02245005 -0.08042148  0.09768424]\n",
      "Gradient Descent(463/999): loss=0.4118653157821072, w = [-1.49416002  0.28889452 -0.51343743 -0.90808625 -0.10640224  0.73475251\n",
      "  0.71214193 -0.3128656   0.09057636  0.30632049 -0.00423957 -0.25554265\n",
      "  0.02245336 -0.08029182  0.09769533]\n",
      "Gradient Descent(464/999): loss=0.41183796568298336, w = [-1.4943844   0.28879232 -0.51343036 -0.90930791 -0.10632821  0.73564222\n",
      "  0.71245481 -0.31315597  0.09050546  0.30640413 -0.00424034 -0.25512458\n",
      "  0.02245664 -0.08016216  0.09770633]\n",
      "Gradient Descent(465/999): loss=0.41181066802393135, w = [-1.49460775  0.28868981 -0.51342294 -0.91052837 -0.10625401  0.73653112\n",
      "  0.71276732 -0.31344628  0.09043455  0.30648741 -0.00424108 -0.25470673\n",
      "  0.02245989 -0.0800325   0.09771725]\n",
      "Gradient Descent(466/999): loss=0.41178342261883355, w = [-1.49483009  0.28858701 -0.51341516 -0.91174764 -0.10617966  0.73741921\n",
      "  0.71307945 -0.31373655  0.09036366  0.30657034 -0.00424179 -0.25428911\n",
      "  0.02246312 -0.07990285  0.09772807]\n",
      "Gradient Descent(467/999): loss=0.41175622928303357, w = [-1.49505142  0.28848391 -0.51340703 -0.91296572 -0.10610514  0.73830649\n",
      "  0.71339122 -0.31402678  0.09029277  0.3066529  -0.00424247 -0.25387172\n",
      "  0.02246632 -0.07977321  0.09773881]\n",
      "Gradient Descent(468/999): loss=0.41172908783330514, w = [-1.49527175  0.28838051 -0.51339855 -0.9141826  -0.10603047  0.73919297\n",
      "  0.71370261 -0.31431696  0.09022189  0.3067351  -0.00424313 -0.25345455\n",
      "  0.0224695  -0.07964358  0.09774946]\n",
      "Gradient Descent(469/999): loss=0.41170199808783825, w = [-1.4954911   0.28827681 -0.51338973 -0.9153983  -0.10595565  0.74007863\n",
      "  0.71401364 -0.31460709  0.09015102  0.30681695 -0.00424377 -0.2530376\n",
      "  0.02247265 -0.07951397  0.09776002]\n",
      "Gradient Descent(470/999): loss=0.41167495986622255, w = [-1.49570946  0.28817283 -0.51338056 -0.91661281 -0.10588067  0.74096349\n",
      "  0.7143243  -0.31489718  0.09008016  0.30689845 -0.00424438 -0.25262088\n",
      "  0.02247578 -0.07938437  0.0977705 ]\n",
      "Gradient Descent(471/999): loss=0.4116479729894264, w = [-1.49592686  0.28806855 -0.51337105 -0.91782613 -0.10580554  0.74184755\n",
      "  0.71463459 -0.31518722  0.09000931  0.3069796  -0.00424496 -0.25220438\n",
      "  0.02247889 -0.07925478  0.09778089]\n",
      "Gradient Descent(472/999): loss=0.41162103727977895, w = [-1.49614329  0.28796398 -0.51336121 -0.91903827 -0.10573026  0.7427308\n",
      "  0.71494451 -0.31547722  0.08993848  0.30706039 -0.00424552 -0.25178811\n",
      "  0.02248197 -0.07912522  0.09779119]\n",
      "Gradient Descent(473/999): loss=0.4115941525609485, w = [-1.49635877  0.28785912 -0.51335104 -0.92024923 -0.10565483  0.74361325\n",
      "  0.71525408 -0.31576718  0.08986765  0.30714084 -0.00424606 -0.25137207\n",
      "  0.02248502 -0.07899567  0.0978014 ]\n",
      "Gradient Descent(474/999): loss=0.4115673186579352, w = [-1.4965733   0.28775398 -0.51334053 -0.92145901 -0.10557925  0.7444949\n",
      "  0.71556327 -0.3160571   0.08979684  0.30722094 -0.00424657 -0.25095625\n",
      "  0.02248805 -0.07886615  0.09781153]\n",
      "Gradient Descent(475/999): loss=0.4115405353970368, w = [-1.49678689  0.28764855 -0.5133297  -0.92266762 -0.10550353  0.74537575\n",
      "  0.71587211 -0.31634697  0.08972604  0.30730069 -0.00424705 -0.25054065\n",
      "  0.02249105 -0.07873666  0.09782157]\n",
      "Gradient Descent(476/999): loss=0.4115138026058512, w = [-1.49699955  0.28754283 -0.51331854 -0.92387505 -0.10542766  0.74625581\n",
      "  0.71618058 -0.31663681  0.08965526  0.3073801  -0.00424751 -0.25012528\n",
      "  0.02249403 -0.07860718  0.09783152]\n",
      "Gradient Descent(477/999): loss=0.4114871201132427, w = [-1.49721129  0.28743683 -0.51330706 -0.9250813  -0.10535164  0.74713506\n",
      "  0.7164887  -0.3169266   0.08958449  0.30745916 -0.00424795 -0.24971013\n",
      "  0.02249699 -0.07847774  0.09784138]\n",
      "Gradient Descent(478/999): loss=0.4114604877493367, w = [-1.49742212  0.28733055 -0.51329527 -0.92628638 -0.10527549  0.74801352\n",
      "  0.71679645 -0.31721636  0.08951374  0.30753789 -0.00424836 -0.2492952\n",
      "  0.02249992 -0.07834833  0.09785116]\n",
      "Gradient Descent(479/999): loss=0.411433905345499, w = [-1.49763203  0.28722399 -0.51328316 -0.9274903  -0.1051992   0.74889119\n",
      "  0.71710385 -0.31750608  0.089443    0.30761628 -0.00424875 -0.2488805\n",
      "  0.02250282 -0.07821895  0.09786086]\n",
      "Gradient Descent(480/999): loss=0.4114073727343181, w = [-1.49784105  0.28711715 -0.51327074 -0.92869304 -0.10512276  0.74976806\n",
      "  0.71741089 -0.31779575  0.08937229  0.30769432 -0.00424912 -0.24846603\n",
      "  0.02250571 -0.0780896   0.09787046]\n",
      "Gradient Descent(481/999): loss=0.4113808897495972, w = [-1.49804918  0.28701004 -0.51325801 -0.92989462 -0.10504619  0.75064414\n",
      "  0.71771757 -0.31808539  0.08930158  0.30777203 -0.00424946 -0.24805177\n",
      "  0.02250856 -0.07796029  0.09787999]\n",
      "Gradient Descent(482/999): loss=0.4113544562263261, w = [-1.49825642  0.28690265 -0.51324497 -0.93109504 -0.10496949  0.75151943\n",
      "  0.7180239  -0.31837499  0.0892309   0.30784941 -0.00424978 -0.24763774\n",
      "  0.0225114  -0.07783101  0.09788942]\n",
      "Gradient Descent(483/999): loss=0.4113280720006786, w = [-1.49846279  0.28679498 -0.51323163 -0.93229429 -0.10489265  0.75239393\n",
      "  0.71832988 -0.31866456  0.08916024  0.30792645 -0.00425008 -0.24722394\n",
      "  0.02251421 -0.07770178  0.09789877]\n",
      "Gradient Descent(484/999): loss=0.41130173690998983, w = [-1.49866828  0.28668704 -0.51321799 -0.93349239 -0.10481567  0.75326764\n",
      "  0.7186355  -0.31895409  0.08908959  0.30800316 -0.00425035 -0.24681036\n",
      "  0.02251699 -0.07757258  0.09790804]\n",
      "Gradient Descent(485/999): loss=0.41127545079274447, w = [-1.49887292  0.28657883 -0.51320405 -0.93468932 -0.10473857  0.75414057\n",
      "  0.71894077 -0.31924358  0.08901897  0.30807954 -0.0042506  -0.246397\n",
      "  0.02251976 -0.07744342  0.09791722]\n",
      "Gradient Descent(486/999): loss=0.4112492134885593, w = [-1.4990767   0.28647035 -0.51318982 -0.9358851  -0.10466133  0.75501271\n",
      "  0.71924569 -0.31953303  0.08894836  0.30815559 -0.00425083 -0.24598386\n",
      "  0.0225225  -0.07731431  0.09792631]\n",
      "Gradient Descent(487/999): loss=0.4112230248381767, w = [-1.49927964  0.28636161 -0.51317529 -0.93707973 -0.10458397  0.75588406\n",
      "  0.71955026 -0.31982245  0.08887778  0.30823131 -0.00425103 -0.24557094\n",
      "  0.02252521 -0.07718525  0.09793532]\n",
      "Gradient Descent(488/999): loss=0.41119688468344306, w = [-1.49948173  0.28625259 -0.51316048 -0.9382732  -0.10450647  0.75675463\n",
      "  0.71985448 -0.32011184  0.08880722  0.30830671 -0.00425121 -0.24515825\n",
      "  0.0225279  -0.07705623  0.09794424]\n",
      "Gradient Descent(489/999): loss=0.41117079286729163, w = [-1.49968299  0.28614331 -0.51314538 -0.93946552 -0.10442885  0.75762442\n",
      "  0.72015835 -0.32040119  0.08873668  0.30838178 -0.00425137 -0.24474578\n",
      "  0.02253057 -0.07692726  0.09795309]\n",
      "Gradient Descent(490/999): loss=0.4111447492337431, w = [-1.49988343  0.28603377 -0.51313    -0.9406567  -0.10435111  0.75849342\n",
      "  0.72046187 -0.32069051  0.08866616  0.30845653 -0.00425151 -0.24433354\n",
      "  0.02253321 -0.07679834  0.09796184]\n",
      "Gradient Descent(491/999): loss=0.4111187536278796, w = [-1.50008304  0.28592396 -0.51311433 -0.94184672 -0.10427324  0.75936165\n",
      "  0.72076505 -0.32097979  0.08859567  0.30853095 -0.00425163 -0.24392151\n",
      "  0.02253583 -0.07666947  0.09797052]\n",
      "Gradient Descent(492/999): loss=0.41109280589583436, w = [-1.50028185  0.28581389 -0.51309839 -0.94303561 -0.10419525  0.7602291\n",
      "  0.72106789 -0.32126904  0.0885252   0.30860505 -0.00425172 -0.24350971\n",
      "  0.02253843 -0.07654065  0.09797911]\n",
      "Gradient Descent(493/999): loss=0.41106690588478545, w = [-1.50047985  0.28570357 -0.51308217 -0.94422335 -0.10411714  0.76109577\n",
      "  0.72137038 -0.32155826  0.08845476  0.30867884 -0.0042518  -0.24309813\n",
      "  0.02254101 -0.07641189  0.09798761]\n",
      "Gradient Descent(494/999): loss=0.41104105344293346, w = [-1.50067705  0.28559298 -0.51306567 -0.94540995 -0.1040389   0.76196166\n",
      "  0.72167253 -0.32184745  0.08838434  0.3087523  -0.00425185 -0.24268677\n",
      "  0.02254356 -0.07628319  0.09799603]\n",
      "Gradient Descent(495/999): loss=0.41101524841949316, w = [-1.50087346  0.28548214 -0.51304891 -0.9465954  -0.10396055  0.76282677\n",
      "  0.72197433 -0.3221366   0.08831394  0.30882545 -0.00425188 -0.24227563\n",
      "  0.02254609 -0.07615454  0.09800437]\n",
      "Gradient Descent(496/999): loss=0.4109894906646835, w = [-1.50106909  0.28537104 -0.51303188 -0.94777973 -0.10388208  0.76369112\n",
      "  0.7222758  -0.32242572  0.08824358  0.30889828 -0.00425189 -0.24186472\n",
      "  0.02254859 -0.07602595  0.09801263]\n",
      "Gradient Descent(497/999): loss=0.4109637800297144, w = [-1.50126393  0.28525969 -0.51301458 -0.94896291 -0.1038035   0.76455468\n",
      "  0.72257692 -0.32271482  0.08817324  0.3089708  -0.00425187 -0.24145402\n",
      "  0.02255108 -0.07589743  0.09802081]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(498/999): loss=0.41093811636676914, w = [-1.50145801  0.28514808 -0.51299702 -0.95014497 -0.1037248   0.76541748\n",
      "  0.7228777  -0.32300388  0.08810292  0.30904301 -0.00425184 -0.24104355\n",
      "  0.02255354 -0.07576896  0.0980289 ]\n",
      "Gradient Descent(499/999): loss=0.4109124995290056, w = [-1.50165132  0.28503622 -0.51297919 -0.95132589 -0.10364599  0.76627951\n",
      "  0.72317815 -0.32329291  0.08803264  0.3091149  -0.00425179 -0.24063329\n",
      "  0.02255598 -0.07564056  0.09803691]\n",
      "Gradient Descent(500/999): loss=0.4108869293705285, w = [-1.50184387  0.28492412 -0.51296111 -0.95250568 -0.10356706  0.76714076\n",
      "  0.72347826 -0.32358191  0.08796238  0.30918648 -0.00425171 -0.24022326\n",
      "  0.02255839 -0.07551222  0.09804484]\n",
      "Gradient Descent(501/999): loss=0.4108614057463885, w = [-1.50203567  0.28481176 -0.51294278 -0.95368434 -0.10348802  0.76800125\n",
      "  0.72377803 -0.32387088  0.08789215  0.30925776 -0.00425162 -0.23981345\n",
      "  0.02256078 -0.07538395  0.09805268]\n",
      "Gradient Descent(502/999): loss=0.41083592851257145, w = [-1.50222672  0.28469916 -0.51292419 -0.95486188 -0.10340888  0.76886096\n",
      "  0.72407747 -0.32415982  0.08782194  0.30932872 -0.00425151 -0.23940386\n",
      "  0.02256315 -0.07525575  0.09806045]\n",
      "Gradient Descent(503/999): loss=0.410810497525983, w = [-1.50241704  0.28458631 -0.51290535 -0.95603829 -0.10332962  0.76971991\n",
      "  0.72437658 -0.32444873  0.08775177  0.30939938 -0.00425137 -0.23899449\n",
      "  0.0225655  -0.07512762  0.09806813]\n",
      "Gradient Descent(504/999): loss=0.41078511264443573, w = [-1.50260661  0.28447321 -0.51288626 -0.95721358 -0.10325026  0.7705781\n",
      "  0.72467535 -0.32473762  0.08768163  0.30946973 -0.00425122 -0.23858533\n",
      "  0.02256783 -0.07499956  0.09807574]\n",
      "Gradient Descent(505/999): loss=0.4107597737266508, w = [-1.50279547  0.28435987 -0.51286692 -0.95838775 -0.10317079  0.77143552\n",
      "  0.72497378 -0.32502647  0.08761152  0.30953978 -0.00425104 -0.2381764\n",
      "  0.02257013 -0.07487157  0.09808326]\n",
      "Gradient Descent(506/999): loss=0.4107344806322327, w = [-1.5029836   0.28424629 -0.51284734 -0.95956081 -0.10309122  0.77229218\n",
      "  0.72527189 -0.3253153   0.08754144  0.30960953 -0.00425085 -0.23776769\n",
      "  0.02257242 -0.07474365  0.09809071]\n",
      "Gradient Descent(507/999): loss=0.410709233221663, w = [-1.50317101  0.28413247 -0.51282752 -0.96073274 -0.10301154  0.77314807\n",
      "  0.72556966 -0.3256041   0.08747139  0.30967897 -0.00425063 -0.2373592\n",
      "  0.02257468 -0.07461581  0.09809807]\n",
      "Gradient Descent(508/999): loss=0.4106840313562971, w = [-1.50335772  0.28401841 -0.51280746 -0.96190356 -0.10293176  0.7740032\n",
      "  0.72586711 -0.32589287  0.08740137  0.30974812 -0.0042504  -0.23695092\n",
      "  0.02257691 -0.07448804  0.09810535]\n",
      "Gradient Descent(509/999): loss=0.41065887489834724, w = [-1.50354372  0.28390411 -0.51278717 -0.96307327 -0.10285187  0.77485758\n",
      "  0.72616422 -0.32618161  0.08733138  0.30981696 -0.00425014 -0.23654287\n",
      "  0.02257913 -0.07436036  0.09811256]\n",
      "Gradient Descent(510/999): loss=0.41063376371087823, w = [-1.50372903  0.28378958 -0.51276663 -0.96424186 -0.10277189  0.77571119\n",
      "  0.72646101 -0.32647033  0.08726143  0.30988551 -0.00424987 -0.23613503\n",
      "  0.02258133 -0.07423275  0.09811969]\n",
      "Gradient Descent(511/999): loss=0.41060869765778657, w = [-1.50391364  0.28367481 -0.51274587 -0.96540935 -0.1026918   0.77656405\n",
      "  0.72675747 -0.32675902  0.0871915   0.30995376 -0.00424958 -0.23572742\n",
      "  0.0225835  -0.07410522  0.09812673]\n",
      "Gradient Descent(512/999): loss=0.4105836766038087, w = [-1.50409757  0.2835598  -0.51272488 -0.96657573 -0.10261162  0.77741614\n",
      "  0.7270536  -0.32704769  0.08712162  0.31002171 -0.00424927 -0.23532002\n",
      "  0.02258565 -0.07397777  0.0981337 ]\n",
      "Gradient Descent(513/999): loss=0.41055870041449655, w = [-1.50428081  0.28344457 -0.51270365 -0.967741   -0.10253134  0.77826749\n",
      "  0.72734941 -0.32733633  0.08705176  0.31008937 -0.00424894 -0.23491284\n",
      "  0.02258778 -0.0738504   0.09814059]\n",
      "Gradient Descent(514/999): loss=0.4105337689562084, w = [-1.50446339  0.2833291  -0.5126822  -0.96890517 -0.10245097  0.77911807\n",
      "  0.72764489 -0.32762494  0.08698194  0.31015674 -0.00424859 -0.23450588\n",
      "  0.02258989 -0.07372312  0.0981474 ]\n",
      "Gradient Descent(515/999): loss=0.41050888209611425, w = [-1.50464529  0.2832134  -0.51266053 -0.97006823 -0.1023705   0.7799679\n",
      "  0.72794005 -0.32791353  0.08691215  0.31022381 -0.00424822 -0.23409913\n",
      "  0.02259198 -0.07359592  0.09815414]\n",
      "Gradient Descent(516/999): loss=0.41048403970217245, w = [-1.50482653  0.28309747 -0.51263864 -0.9712302  -0.10228993  0.78081698\n",
      "  0.72823489 -0.32820209  0.0868424   0.31029059 -0.00424784 -0.2336926\n",
      "  0.02259405 -0.07346881  0.0981608 ]\n",
      "Gradient Descent(517/999): loss=0.41045924164312714, w = [-1.50500711  0.28298132 -0.51261652 -0.97239107 -0.10220928  0.78166531\n",
      "  0.7285294  -0.32849063  0.08677268  0.31035709 -0.00424743 -0.23328629\n",
      "  0.0225961  -0.07334178  0.09816738]\n",
      "Gradient Descent(518/999): loss=0.41043448778849506, w = [-1.50518704  0.28286493 -0.51259419 -0.97355084 -0.10212853  0.78251289\n",
      "  0.72882359 -0.32877914  0.086703    0.31042329 -0.00424701 -0.2328802\n",
      "  0.02259813 -0.07321485  0.09817388]\n",
      "Gradient Descent(519/999): loss=0.41040977800856543, w = [-1.50536632  0.28274833 -0.51257165 -0.97470951 -0.10204769  0.78335972\n",
      "  0.72911747 -0.32906763  0.08663335  0.31048921 -0.00424657 -0.23247433\n",
      "  0.02260013 -0.073088    0.09818031]\n",
      "Gradient Descent(520/999): loss=0.41038511217438317, w = [-1.50554496  0.2826315  -0.51254889 -0.97586709 -0.10196676  0.78420579\n",
      "  0.72941102 -0.32935609  0.08656374  0.31055483 -0.00424611 -0.23206867\n",
      "  0.02260212 -0.07296125  0.09818666]\n",
      "Gradient Descent(521/999): loss=0.4103604901577413, w = [-1.50572297  0.28251444 -0.51252592 -0.97702358 -0.10188574  0.78505112\n",
      "  0.72970425 -0.32964453  0.08649417  0.31062018 -0.00424564 -0.23166323\n",
      "  0.02260408 -0.07283458  0.09819293]\n",
      "Gradient Descent(522/999): loss=0.4103359118311829, w = [-1.50590034  0.28239717 -0.51250274 -0.97817899 -0.10180463  0.78589571\n",
      "  0.72999717 -0.32993295  0.08642463  0.31068524 -0.00424514 -0.231258\n",
      "  0.02260602 -0.07270801  0.09819913]\n",
      "Gradient Descent(523/999): loss=0.41031137706797405, w = [-1.50607708  0.28227967 -0.51247935 -0.9793333  -0.10172344  0.78673954\n",
      "  0.73028977 -0.33022134  0.08635513  0.31075001 -0.00424463 -0.23085299\n",
      "  0.02260795 -0.07258154  0.09820526]\n",
      "Gradient Descent(524/999): loss=0.4102868857421207, w = [-1.50625321  0.28216196 -0.51245576 -0.98048653 -0.10164216  0.78758263\n",
      "  0.73058205 -0.33050971  0.08628567  0.31081451 -0.0042441  -0.2304482\n",
      "  0.02260985 -0.07245516  0.09821131]\n",
      "Gradient Descent(525/999): loss=0.41026243772833393, w = [-1.50642872  0.28204403 -0.51243196 -0.98163867 -0.1015608   0.78842498\n",
      "  0.73087402 -0.33079806  0.08621625  0.31087872 -0.00424355 -0.23004363\n",
      "  0.02261173 -0.07232887  0.09821728]\n",
      "Gradient Descent(526/999): loss=0.41023803290204325, w = [-1.50660362  0.28192588 -0.51240796 -0.98278973 -0.10147936  0.78926659\n",
      "  0.73116567 -0.33108638  0.08614686  0.31094265 -0.00424299 -0.22963926\n",
      "  0.0226136  -0.07220269  0.09822318]\n",
      "Gradient Descent(527/999): loss=0.4102136711393838, w = [-1.50677791  0.28180752 -0.51238377 -0.98393971 -0.10139783  0.79010745\n",
      "  0.73145701 -0.33137468  0.08607751  0.3110063  -0.00424241 -0.22923512\n",
      "  0.02261544 -0.0720766   0.09822901]\n",
      "Gradient Descent(528/999): loss=0.41018935231717824, w = [-1.50695161  0.28168894 -0.51235937 -0.98508861 -0.10131622  0.79094757\n",
      "  0.73174804 -0.33166295  0.0860082   0.31106968 -0.00424181 -0.22883119\n",
      "  0.02261726 -0.07195061  0.09823476]\n",
      "Gradient Descent(529/999): loss=0.4101650763129447, w = [-1.50712471  0.28157015 -0.51233479 -0.98623644 -0.10123453  0.79178696\n",
      "  0.73203875 -0.33195121  0.08593893  0.31113277 -0.00424119 -0.22842747\n",
      "  0.02261907 -0.07182472  0.09824044]\n",
      "Gradient Descent(530/999): loss=0.4101408430048781, w = [-1.50729721  0.28145115 -0.51231    -0.98738318 -0.10115276  0.7926256\n",
      "  0.73232916 -0.33223944  0.0858697   0.3111956  -0.00424056 -0.22802397\n",
      "  0.02262085 -0.07169894  0.09824604]\n",
      "Gradient Descent(531/999): loss=0.4101166522718503, w = [-1.50746913  0.28133194 -0.51228503 -0.98852886 -0.10107091  0.79346351\n",
      "  0.73261925 -0.33252765  0.08580051  0.31125814 -0.00423991 -0.22762069\n",
      "  0.02262261 -0.07157326  0.09825158]\n",
      "Gradient Descent(532/999): loss=0.41009250399340114, w = [-1.50764047  0.28121252 -0.51225986 -0.98967346 -0.10098899  0.79430067\n",
      "  0.73290903 -0.33281584  0.08573136  0.31132042 -0.00423924 -0.22721761\n",
      "  0.02262436 -0.07144768  0.09825704]\n",
      "Gradient Descent(533/999): loss=0.41006839804972783, w = [-1.50781124  0.28109289 -0.51223451 -0.990817   -0.10090698  0.79513711\n",
      "  0.73319851 -0.333104    0.08566225  0.31138242 -0.00423856 -0.22681476\n",
      "  0.02262608 -0.0713222   0.09826243]\n",
      "Gradient Descent(534/999): loss=0.4100443343216791, w = [-1.50798143  0.28097306 -0.51220897 -0.99195946 -0.10082491  0.7959728\n",
      "  0.73348767 -0.33339215  0.08559318  0.31144415 -0.00423786 -0.22641211\n",
      "  0.02262779 -0.07119683  0.09826774]\n",
      "Gradient Descent(535/999): loss=0.41002031269075667, w = [-1.50815105  0.28085302 -0.51218325 -0.99310086 -0.10074275  0.79680777\n",
      "  0.73377653 -0.33368027  0.08552415  0.3115056  -0.00423714 -0.22600968\n",
      "  0.02262948 -0.07107157  0.09827299]\n",
      "Gradient Descent(536/999): loss=0.4099963330390961, w = [-1.50832012  0.28073278 -0.51215735 -0.99424119 -0.10066053  0.797642\n",
      "  0.73406508 -0.33396837  0.08545516  0.31156679 -0.00423641 -0.22560747\n",
      "  0.02263114 -0.07094642  0.09827816]\n",
      "Gradient Descent(537/999): loss=0.40997239524946893, w = [-1.50848862  0.28061233 -0.51213126 -0.99538046 -0.10057823  0.7984755\n",
      "  0.73435333 -0.33425645  0.08538621  0.31162771 -0.00423566 -0.22520546\n",
      "  0.02263279 -0.07082137  0.09828326]\n",
      "Gradient Descent(538/999): loss=0.4099484992052752, w = [-1.50865658  0.28049168 -0.512105   -0.99651867 -0.10049586  0.79930826\n",
      "  0.73464127 -0.33454451  0.0853173   0.31168836 -0.0042349  -0.22480367\n",
      "  0.02263442 -0.07069644  0.0982883 ]\n",
      "Gradient Descent(539/999): loss=0.40992464479053176, w = [-1.50882398  0.28037083 -0.51207855 -0.99765582 -0.10041341  0.8001403\n",
      "  0.7349289  -0.33483255  0.08524844  0.31174874 -0.00423411 -0.2244021\n",
      "  0.02263603 -0.07057161  0.09829326]\n",
      "Gradient Descent(540/999): loss=0.40990083188987536, w = [-1.50899084  0.28024978 -0.51205194 -0.99879191 -0.1003309   0.80097161\n",
      "  0.73521624 -0.33512056  0.08517961  0.31180886 -0.00423332 -0.22400073\n",
      "  0.02263762 -0.0704469   0.09829815]\n",
      "Gradient Descent(541/999): loss=0.4098770603885424, w = [-1.50915716  0.28012853 -0.51202515 -0.99992695 -0.10024832  0.80180218\n",
      "  0.73550327 -0.33540856  0.08511083  0.31186872 -0.00423251 -0.22359958\n",
      "  0.0226392  -0.0703223   0.09830297]\n",
      "Gradient Descent(542/999): loss=0.4098533301723816, w = [-1.50932294  0.28000709 -0.51199818 -1.00106093 -0.10016567  0.80263204\n",
      "  0.73579    -0.33569653  0.0850421   0.31192831 -0.00423168 -0.22319864\n",
      "  0.02264075 -0.07019781  0.09830773]\n",
      "Gradient Descent(543/999): loss=0.40982964112782866, w = [-1.5094882   0.27988545 -0.51197105 -1.00219386 -0.10008295  0.80346116\n",
      "  0.73607642 -0.33598449  0.0849734   0.31198763 -0.00423083 -0.22279791\n",
      "  0.02264229 -0.07007344  0.09831241]\n",
      "Gradient Descent(544/999): loss=0.4098059931419197, w = [-1.50965292  0.27976361 -0.51194375 -1.00332574 -0.10000017  0.80428956\n",
      "  0.73636255 -0.33627242  0.08490475  0.3120467  -0.00422997 -0.2223974\n",
      "  0.0226438  -0.06994918  0.09831703]\n",
      "Gradient Descent(545/999): loss=0.409782386102265, w = [-1.50981713  0.27964158 -0.51191628 -1.00445657 -0.09991732  0.80511723\n",
      "  0.73664838 -0.33656034  0.08483614  0.3121055  -0.0042291  -0.22199709\n",
      "  0.0226453  -0.06982504  0.09832157]\n",
      "Gradient Descent(546/999): loss=0.40975881989705915, w = [-1.50998081  0.27951936 -0.51188864 -1.00558635 -0.09983441  0.80594419\n",
      "  0.7369339  -0.33684823  0.08476757  0.31216404 -0.00422821 -0.221597\n",
      "  0.02264678 -0.06970101  0.09832605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(547/999): loss=0.40973529441506734, w = [-1.51014399  0.27939695 -0.51186084 -1.00671508 -0.09975143  0.80677041\n",
      "  0.73721913 -0.3371361   0.08469905  0.31222233 -0.0042273  -0.22119712\n",
      "  0.02264825 -0.06957711  0.09833047]\n",
      "Gradient Descent(548/999): loss=0.40971180954562797, w = [-1.51030665  0.27927435 -0.51183288 -1.00784277 -0.09966839  0.80759592\n",
      "  0.73750406 -0.33742396  0.08463057  0.31228035 -0.00422638 -0.22079745\n",
      "  0.02264969 -0.06945332  0.09833481]\n",
      "Gradient Descent(549/999): loss=0.40968836517863516, w = [-1.51046881  0.27915155 -0.51180476 -1.00896942 -0.09958528  0.8084207\n",
      "  0.7377887  -0.33771179  0.08456214  0.31233812 -0.00422545 -0.22039798\n",
      "  0.02265112 -0.06932965  0.09833909]\n",
      "Gradient Descent(550/999): loss=0.40966496120454077, w = [-1.51063046  0.27902857 -0.51177648 -1.01009503 -0.09950212  0.80924477\n",
      "  0.73807304 -0.3379996   0.08449375  0.31239563 -0.0042245  -0.21999873\n",
      "  0.02265253 -0.0692061   0.0983433 ]\n",
      "Gradient Descent(551/999): loss=0.4096415975143508, w = [-1.51079162  0.27890541 -0.51174804 -1.0112196  -0.0994189   0.81006812\n",
      "  0.73835708 -0.3382874   0.08442541  0.31245289 -0.00422353 -0.21959969\n",
      "  0.02265392 -0.06908267  0.09834744]\n",
      "Gradient Descent(552/999): loss=0.4096182739996171, w = [-1.51095229  0.27878205 -0.51171945 -1.01234313 -0.09933561  0.81089074\n",
      "  0.73864083 -0.33857517  0.0843571   0.31250989 -0.00422255 -0.21920086\n",
      "  0.02265529 -0.06895937  0.09835152]\n",
      "Gradient Descent(553/999): loss=0.4095949905524282, w = [-1.51111246  0.27865852 -0.5116907  -1.01346563 -0.09925227  0.81171265\n",
      "  0.73892429 -0.33886293  0.08428885  0.31256664 -0.00422156 -0.21880224\n",
      "  0.02265664 -0.06883618  0.09835553]\n",
      "Gradient Descent(554/999): loss=0.40957174706541266, w = [-1.51127215  0.27853479 -0.5116618  -1.01458709 -0.09916887  0.81253385\n",
      "  0.73920745 -0.33915066  0.08422064  0.31262313 -0.00422055 -0.21840383\n",
      "  0.02265798 -0.06871312  0.09835948]\n",
      "Gradient Descent(555/999): loss=0.4095485434317271, w = [-1.51143136  0.27841089 -0.51163275 -1.01570752 -0.09908541  0.81335433\n",
      "  0.73949032 -0.33943838  0.08415247  0.31267937 -0.00421952 -0.21800563\n",
      "  0.0226593  -0.06859019  0.09836336]\n",
      "Gradient Descent(556/999): loss=0.4095253795450559, w = [-1.51159009  0.27828681 -0.51160355 -1.01682692 -0.09900189  0.81417409\n",
      "  0.7397729  -0.33972607  0.08408436  0.31273536 -0.00421848 -0.21760764\n",
      "  0.02266061 -0.06846738  0.09836718]\n",
      "Gradient Descent(557/999): loss=0.4095022552996056, w = [-1.51174835  0.27816254 -0.5115742  -1.01794529 -0.09891832  0.81499314\n",
      "  0.74005518 -0.34001375  0.08401628  0.3127911  -0.00421743 -0.21720986\n",
      "  0.02266189 -0.06834469  0.09837093]\n",
      "Gradient Descent(558/999): loss=0.4094791705900951, w = [-1.51190613  0.2780381  -0.5115447  -1.01906264 -0.0988347   0.81581148\n",
      "  0.74033718 -0.34030141  0.08394825  0.3128466  -0.00421636 -0.21681228\n",
      "  0.02266316 -0.06822213  0.09837462]\n",
      "Gradient Descent(559/999): loss=0.4094561253117567, w = [-1.51206345  0.27791347 -0.51151506 -1.02017895 -0.09875102  0.81662911\n",
      "  0.74061888 -0.34058904  0.08388027  0.31290184 -0.00421528 -0.21641492\n",
      "  0.02266441 -0.0680997   0.09837825]\n",
      "Gradient Descent(560/999): loss=0.40943311936032883, w = [-1.51222031  0.27778867 -0.51148528 -1.02129425 -0.09866729  0.81744602\n",
      "  0.7409003  -0.34087666  0.08381234  0.31295684 -0.00421419 -0.21601776\n",
      "  0.02266565 -0.0679774   0.09838181]\n",
      "Gradient Descent(561/999): loss=0.4094101526320519, w = [-1.51237671  0.2776637  -0.51145535 -1.02240852 -0.0985835   0.81826223\n",
      "  0.74118143 -0.34116426  0.08374445  0.31301158 -0.00421308 -0.21562081\n",
      "  0.02266687 -0.06785523  0.09838531]\n",
      "Gradient Descent(562/999): loss=0.40938722502366476, w = [-1.51253265  0.27753855 -0.51142528 -1.02352177 -0.09849967  0.81907772\n",
      "  0.74146227 -0.34145184  0.08367661  0.31306609 -0.00421196 -0.21522407\n",
      "  0.02266807 -0.06773318  0.09838874]\n",
      "Gradient Descent(563/999): loss=0.4093643364323956, w = [-1.51268815  0.27741322 -0.51139507 -1.024634   -0.09841578  0.81989251\n",
      "  0.74174283 -0.34173941  0.08360881  0.31312034 -0.00421082 -0.21482753\n",
      "  0.02266925 -0.06761127  0.09839212]\n",
      "Gradient Descent(564/999): loss=0.4093414867559668, w = [-1.51284319  0.27728773 -0.51136472 -1.02574522 -0.09833184  0.82070659\n",
      "  0.7420231  -0.34202695  0.08354106  0.31317436 -0.00420967 -0.2144312\n",
      "  0.02267042 -0.06748948  0.09839543]\n",
      "Gradient Descent(565/999): loss=0.4093186758925799, w = [-1.51299779  0.27716206 -0.51133424 -1.02685542 -0.09824786  0.82151996\n",
      "  0.74230308 -0.34231447  0.08347336  0.31322813 -0.0042085  -0.21403508\n",
      "  0.02267157 -0.06736783  0.09839868]\n",
      "Gradient Descent(566/999): loss=0.409295903740919, w = [-1.51315195  0.27703622 -0.51130362 -1.02796461 -0.09816382  0.82233263\n",
      "  0.74258278 -0.34260198  0.08340571  0.31328165 -0.00420733 -0.21363917\n",
      "  0.0226727  -0.06724631  0.09840187]\n",
      "Gradient Descent(567/999): loss=0.40927317020014103, w = [-1.51330567  0.27691021 -0.51127287 -1.02907278 -0.09807974  0.82314459\n",
      "  0.74286219 -0.34288946  0.0833381   0.31333494 -0.00420614 -0.21324346\n",
      "  0.02267382 -0.06712493  0.09840499]\n",
      "Gradient Descent(568/999): loss=0.4092504751698765, w = [-1.51345896  0.27678403 -0.51124199 -1.03017994 -0.09799561  0.82395585\n",
      "  0.74314132 -0.34317693  0.08327054  0.31338798 -0.00420493 -0.21284796\n",
      "  0.02267492 -0.06700367  0.09840806]\n",
      "Gradient Descent(569/999): loss=0.4092278185502225, w = [-1.51361182  0.27665769 -0.51121097 -1.0312861  -0.09791144  0.8247664\n",
      "  0.74342017 -0.34346438  0.08320303  0.31344079 -0.00420371 -0.21245267\n",
      "  0.02267601 -0.06688255  0.09841107]\n",
      "Gradient Descent(570/999): loss=0.40920520024173795, w = [-1.51376425  0.27653118 -0.51117983 -1.03239125 -0.09782722  0.82557626\n",
      "  0.74369874 -0.34375181  0.08313557  0.31349335 -0.00420248 -0.21205758\n",
      "  0.02267708 -0.06676157  0.09841401]\n",
      "Gradient Descent(571/999): loss=0.4091826201454445, w = [-1.51391626  0.2764045  -0.51114855 -1.03349539 -0.09774295  0.82638541\n",
      "  0.74397702 -0.34403922  0.08306815  0.31354568 -0.00420124 -0.21166269\n",
      "  0.02267813 -0.06664072  0.0984169 ]\n",
      "Gradient Descent(572/999): loss=0.40916007816281175, w = [-1.51406785  0.27627766 -0.51111715 -1.03459852 -0.09765864  0.82719386\n",
      "  0.74425502 -0.34432661  0.08300079  0.31359777 -0.00419998 -0.21126802\n",
      "  0.02267917 -0.06652001  0.09841973]\n",
      "Gradient Descent(573/999): loss=0.4091375741957648, w = [-1.51421902  0.27615065 -0.51108562 -1.03570066 -0.09757429  0.82800161\n",
      "  0.74453274 -0.34461398  0.08293347  0.31364962 -0.00419871 -0.21087354\n",
      "  0.02268019 -0.06639943  0.0984225 ]\n",
      "Gradient Descent(574/999): loss=0.4091151081466782, w = [-1.51436978  0.27602349 -0.51105397 -1.03680179 -0.09748989  0.82880866\n",
      "  0.74481019 -0.34490134  0.0828662   0.31370124 -0.00419743 -0.21047928\n",
      "  0.02268119 -0.06627899  0.0984252 ]\n",
      "Gradient Descent(575/999): loss=0.40909267991836695, w = [-1.51452013  0.27589616 -0.51102219 -1.03790193 -0.09740545  0.82961502\n",
      "  0.74508735 -0.34518867  0.08279898  0.31375262 -0.00419613 -0.21008521\n",
      "  0.02268218 -0.06615869  0.09842785]\n",
      "Gradient Descent(576/999): loss=0.4090702894140827, w = [-1.51467007  0.27576867 -0.5109903  -1.03900106 -0.09732097  0.83042067\n",
      "  0.74536424 -0.34547599  0.0827318   0.31380376 -0.00419483 -0.20969136\n",
      "  0.02268316 -0.06603853  0.09843045]\n",
      "Gradient Descent(577/999): loss=0.40904793653751714, w = [-1.51481961  0.27564102 -0.51095828 -1.0400992  -0.09723645  0.83122563\n",
      "  0.74564085 -0.34576329  0.08266468  0.31385467 -0.0041935  -0.2092977\n",
      "  0.02268412 -0.06591851  0.09843298]\n",
      "Gradient Descent(578/999): loss=0.40902562119279506, w = [-1.51496874  0.27551322 -0.51092614 -1.04119635 -0.09715189  0.8320299\n",
      "  0.74591718 -0.34605057  0.0825976   0.31390535 -0.00419217 -0.20890425\n",
      "  0.02268506 -0.06579862  0.09843546]\n",
      "Gradient Descent(579/999): loss=0.40900334328446686, w = [-1.51511748  0.27538526 -0.51089388 -1.0422925  -0.09706729  0.83283347\n",
      "  0.74619323 -0.34633783  0.08253058  0.3139558  -0.00419083 -0.20851101\n",
      "  0.02268599 -0.06567888  0.09843788]\n",
      "Gradient Descent(580/999): loss=0.40898110271750765, w = [-1.51526583  0.27525714 -0.51086151 -1.04338767 -0.09698265  0.83363635\n",
      "  0.74646901 -0.34662507  0.0824636   0.31400601 -0.00418947 -0.20811797\n",
      "  0.0226869  -0.06555928  0.09844024]\n",
      "Gradient Descent(581/999): loss=0.408958899397313, w = [-1.51541378  0.27512886 -0.51082902 -1.04448184 -0.09689798  0.83443853\n",
      "  0.74674452 -0.34691229  0.08239667  0.314056   -0.0041881  -0.20772513\n",
      "  0.0226878  -0.06543982  0.09844255]\n",
      "Gradient Descent(582/999): loss=0.4089367332297013, w = [-1.51556135  0.27500043 -0.51079642 -1.04557502 -0.09681327  0.83524002\n",
      "  0.74701975 -0.3471995   0.0823298   0.31410575 -0.00418671 -0.20733249\n",
      "  0.02268868 -0.0653205   0.0984448 ]\n",
      "Gradient Descent(583/999): loss=0.40891460412090075, w = [-1.51570853  0.27487185 -0.5107637  -1.04666722 -0.09672852  0.83604082\n",
      "  0.7472947  -0.34748668  0.08226297  0.31415527 -0.00418532 -0.20694006\n",
      "  0.02268954 -0.06520132  0.098447  ]\n",
      "Gradient Descent(584/999): loss=0.4088925119775512, w = [-1.51585533  0.27474312 -0.51073087 -1.04775844 -0.09664373  0.83684093\n",
      "  0.74756938 -0.34777385  0.08219619  0.31420457 -0.00418391 -0.20654783\n",
      "  0.0226904  -0.06508229  0.09844913]\n",
      "Gradient Descent(585/999): loss=0.408870456706702, w = [-1.51600175  0.27461423 -0.51069793 -1.04884867 -0.09655891  0.83764035\n",
      "  0.74784379 -0.34806099  0.08212946  0.31425364 -0.00418249 -0.2061558\n",
      "  0.02269123 -0.0649634   0.09845122]\n",
      "Gradient Descent(586/999): loss=0.4088484382158073, w = [-1.51614779  0.27448519 -0.51066488 -1.04993792 -0.09647406  0.83843909\n",
      "  0.74811793 -0.34834812  0.08206278  0.31430248 -0.00418106 -0.20576398\n",
      "  0.02269205 -0.06484465  0.09845325]\n",
      "Gradient Descent(587/999): loss=0.4088264564127164, w = [-1.51629346  0.27435601 -0.51063172 -1.05102618 -0.09638917  0.83923713\n",
      "  0.7483918  -0.34863523  0.08199615  0.3143511  -0.00417962 -0.20537235\n",
      "  0.02269286 -0.06472605  0.09845522]\n",
      "Gradient Descent(588/999): loss=0.40880451120568306, w = [-1.51643877  0.27422668 -0.51059845 -1.05211347 -0.09630425  0.84003449\n",
      "  0.7486654  -0.34892232  0.08192958  0.31439949 -0.00417816 -0.20498093\n",
      "  0.02269365 -0.0646076   0.09845714]\n",
      "Gradient Descent(589/999): loss=0.40878260250335335, w = [-1.5165837   0.27409719 -0.51056508 -1.05319979 -0.09621929  0.84083116\n",
      "  0.74893872 -0.3492094   0.08186305  0.31444765 -0.0041767  -0.20458971\n",
      "  0.02269443 -0.06448929  0.09845901]\n",
      "Gradient Descent(590/999): loss=0.40876073021476544, w = [-1.51672827  0.27396757 -0.5105316  -1.05428512 -0.09613431  0.84162714\n",
      "  0.74921178 -0.34949645  0.08179657  0.31449559 -0.00417522 -0.20419869\n",
      "  0.0226952  -0.06437112  0.09846082]\n",
      "Gradient Descent(591/999): loss=0.4087388942493441, w = [-1.51687248  0.27383779 -0.51049802 -1.05536949 -0.09604929  0.84242244\n",
      "  0.74948456 -0.34978348  0.08173014  0.31454331 -0.00417373 -0.20380788\n",
      "  0.02269594 -0.0642531   0.09846258]\n",
      "Gradient Descent(592/999): loss=0.4087170945168997, w = [-1.51701633  0.27370787 -0.51046433 -1.05645288 -0.09596424  0.84321706\n",
      "  0.74975708 -0.3500705   0.08166376  0.3145908  -0.00417223 -0.20341726\n",
      "  0.02269668 -0.06413523  0.09846429]\n",
      "Gradient Descent(593/999): loss=0.40869533092762705, w = [-1.51715982  0.27357781 -0.51043054 -1.0575353  -0.09587916  0.84401099\n",
      "  0.75002934 -0.35035749  0.08159744  0.31463807 -0.00417071 -0.20302685\n",
      "  0.0226974  -0.06401751  0.09846594]\n",
      "Gradient Descent(594/999): loss=0.4086736033920983, w = [-1.51730296  0.27344761 -0.51039665 -1.05861675 -0.09579406  0.84480424\n",
      "  0.75030132 -0.35064447  0.08153116  0.31468513 -0.00416919 -0.20263663\n",
      "  0.0226981  -0.06389993  0.09846755]\n",
      "Gradient Descent(595/999): loss=0.40865191182126037, w = [-1.51744576  0.27331726 -0.51036266 -1.05969723 -0.09570892  0.84559681\n",
      "  0.75057304 -0.35093143  0.08146494  0.31473196 -0.00416765 -0.20224662\n",
      "  0.0226988  -0.06378251  0.0984691 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(596/999): loss=0.40863025612643766, w = [-1.5175882   0.27318678 -0.51032857 -1.06077675 -0.09562376  0.8463887\n",
      "  0.75084449 -0.35121837  0.08139876  0.31477857 -0.00416611 -0.2018568\n",
      "  0.02269947 -0.06366523  0.09847059]\n",
      "Gradient Descent(597/999): loss=0.40860863621932403, w = [-1.5177303   0.27305615 -0.51029439 -1.06185531 -0.09553856  0.84717991\n",
      "  0.75111568 -0.35150529  0.08133264  0.31482496 -0.00416455 -0.20146719\n",
      "  0.02270014 -0.0635481   0.09847204]\n",
      "Gradient Descent(598/999): loss=0.40858705201197526, w = [-1.51787205  0.27292538 -0.5102601  -1.0629329  -0.09545334  0.84797044\n",
      "  0.7513866  -0.35179219  0.08126657  0.31487113 -0.00416298 -0.20107777\n",
      "  0.02270079 -0.06343112  0.09847344]\n",
      "Gradient Descent(599/999): loss=0.4085655034168199, w = [-1.51801347  0.27279448 -0.51022572 -1.06400952 -0.0953681   0.84876029\n",
      "  0.75165725 -0.35207907  0.08120054  0.31491709 -0.0041614  -0.20068855\n",
      "  0.02270142 -0.06331429  0.09847478]\n",
      "Gradient Descent(600/999): loss=0.40854399034664274, w = [-1.51815455  0.27266344 -0.51019125 -1.06508519 -0.09528283  0.84954947\n",
      "  0.75192765 -0.35236593  0.08113457  0.31496283 -0.00415981 -0.20029954\n",
      "  0.02270205 -0.06319761  0.09847608]\n",
      "Gradient Descent(601/999): loss=0.4085225127145923, w = [-1.51829529  0.27253226 -0.51015668 -1.0661599  -0.09519753  0.85033797\n",
      "  0.75219778 -0.35265277  0.08106866  0.31500835 -0.00415821 -0.19991072\n",
      "  0.02270265 -0.06308108  0.09847732]\n",
      "Gradient Descent(602/999): loss=0.40850107043416967, w = [-1.51843571  0.27240095 -0.51012202 -1.06723366 -0.09511221  0.85112579\n",
      "  0.75246765 -0.35293959  0.08100279  0.31505365 -0.00415659 -0.1995221\n",
      "  0.02270325 -0.06296471  0.09847852]\n",
      "Gradient Descent(603/999): loss=0.4084796634192315, w = [-1.51857579  0.2722695  -0.51008727 -1.06830646 -0.09502686  0.85191294\n",
      "  0.75273725 -0.3532264   0.08093697  0.31509874 -0.00415497 -0.19913368\n",
      "  0.02270383 -0.06284848  0.09847966]\n",
      "Gradient Descent(604/999): loss=0.40845829158398717, w = [-1.51871555  0.27213792 -0.51005243 -1.0693783  -0.09494149  0.85269941\n",
      "  0.7530066  -0.35351318  0.08087121  0.31514362 -0.00415334 -0.19874546\n",
      "  0.0227044  -0.06273241  0.09848076]\n",
      "Gradient Descent(605/999): loss=0.40843695484299186, w = [-1.51885499  0.27200621 -0.5100175  -1.07044919 -0.0948561   0.85348521\n",
      "  0.75327568 -0.35379994  0.0808055   0.31518828 -0.00415169 -0.19835743\n",
      "  0.02270495 -0.06261649  0.09848181]\n",
      "Gradient Descent(606/999): loss=0.4084156531111483, w = [-1.5189941   0.27187436 -0.50998248 -1.07151913 -0.09477069  0.85427033\n",
      "  0.75354451 -0.35408669  0.08073983  0.31523273 -0.00415004 -0.1979696\n",
      "  0.02270549 -0.06250073  0.09848281]\n",
      "Gradient Descent(607/999): loss=0.40839438630370295, w = [-1.51913289  0.27174239 -0.50994737 -1.07258812 -0.09468525  0.85505479\n",
      "  0.75381307 -0.35437341  0.08067422  0.31527697 -0.00414837 -0.19758197\n",
      "  0.02270602 -0.06238511  0.09848376]\n",
      "Gradient Descent(608/999): loss=0.4083731543362404, w = [-1.51927137  0.27161028 -0.50991218 -1.07365617 -0.09459979  0.85583857\n",
      "  0.75408138 -0.35466012  0.08060867  0.315321   -0.00414669 -0.19719454\n",
      "  0.02270653 -0.06226965  0.09848466]\n",
      "Gradient Descent(609/999): loss=0.40835195712468936, w = [-1.51940953  0.27147805 -0.5098769  -1.07472327 -0.09451432  0.85662169\n",
      "  0.75434943 -0.3549468   0.08054316  0.31536481 -0.00414501 -0.19680731\n",
      "  0.02270704 -0.06215435  0.09848552]\n",
      "Gradient Descent(610/999): loss=0.4083307945853082, w = [-1.51954738  0.27134568 -0.50984153 -1.07578942 -0.09442882  0.85740413\n",
      "  0.75461722 -0.35523347  0.08047771  0.31540841 -0.00414331 -0.19642027\n",
      "  0.02270752 -0.0620392   0.09848633]\n",
      "Gradient Descent(611/999): loss=0.4083096666346941, w = [-1.51968492  0.27121319 -0.50980608 -1.07685463 -0.0943433   0.85818591\n",
      "  0.75488475 -0.35552011  0.0804123   0.31545181 -0.0041416  -0.19603343\n",
      "  0.022708   -0.0619242   0.09848709]\n",
      "Gradient Descent(612/999): loss=0.40828857318976963, w = [-1.51982215  0.27108058 -0.50977055 -1.0779189  -0.09425776  0.85896701\n",
      "  0.75515202 -0.35580674  0.08034695  0.31549499 -0.00413989 -0.19564678\n",
      "  0.02270846 -0.06180936  0.0984878 ]\n",
      "Gradient Descent(613/999): loss=0.40826751416779217, w = [-1.51995908  0.27094783 -0.50973493 -1.07898222 -0.09417221  0.85974745\n",
      "  0.75541904 -0.35609334  0.08028166  0.31553797 -0.00413816 -0.19526033\n",
      "  0.02270891 -0.06169468  0.09848847]\n",
      "Gradient Descent(614/999): loss=0.40824648948634235, w = [-1.52009571  0.27081497 -0.50969924 -1.08004461 -0.09408664  0.86052723\n",
      "  0.75568581 -0.35637993  0.08021641  0.31558074 -0.00413642 -0.19487408\n",
      "  0.02270935 -0.06158015  0.09848909]\n",
      "Gradient Descent(615/999): loss=0.40822549906332195, w = [-1.52023204  0.27068198 -0.50966346 -1.08110606 -0.09400104  0.86130634\n",
      "  0.75595231 -0.35666649  0.08015122  0.3156233  -0.00413468 -0.19448802\n",
      "  0.02270977 -0.06146577  0.09848967]\n",
      "Gradient Descent(616/999): loss=0.4082045428169632, w = [-1.52036807  0.27054886 -0.50962761 -1.08216658 -0.09391544  0.86208478\n",
      "  0.75621857 -0.35695304  0.08008607  0.31566565 -0.00413292 -0.19410216\n",
      "  0.02271018 -0.06135156  0.0984902 ]\n",
      "Gradient Descent(617/999): loss=0.408183620665807, w = [-1.5205038   0.27041563 -0.50959168 -1.08322616 -0.09382981  0.86286256\n",
      "  0.75648457 -0.35723956  0.08002098  0.3157078  -0.00413115 -0.19371649\n",
      "  0.02271058 -0.0612375   0.09849069]\n",
      "Gradient Descent(618/999): loss=0.4081627325287217, w = [-1.52063924  0.27028227 -0.50955567 -1.08428481 -0.09374417  0.86363967\n",
      "  0.75675031 -0.35752606  0.07995595  0.31574974 -0.00412938 -0.19333102\n",
      "  0.02271097 -0.06112359  0.09849113]\n",
      "Gradient Descent(619/999): loss=0.4081418783248812, w = [-1.52077439  0.27014879 -0.50951958 -1.08534252 -0.09365852  0.86441613\n",
      "  0.75701581 -0.35781255  0.07989096  0.31579148 -0.00412759 -0.19294574\n",
      "  0.02271134 -0.06100985  0.09849152]\n",
      "Gradient Descent(620/999): loss=0.40812105797378107, w = [-1.52090925  0.27001519 -0.50948341 -1.08639931 -0.09357285  0.86519192\n",
      "  0.75728105 -0.35809901  0.07982603  0.31583302 -0.00412579 -0.19256066\n",
      "  0.02271171 -0.06089626  0.09849188]\n",
      "Gradient Descent(621/999): loss=0.40810027139522037, w = [-1.52104382  0.26988148 -0.50944718 -1.08745517 -0.09348716  0.86596705\n",
      "  0.75754603 -0.35838545  0.07976115  0.31587435 -0.00412399 -0.19217577\n",
      "  0.02271206 -0.06078283  0.09849218]\n",
      "Gradient Descent(622/999): loss=0.4080795185093095, w = [-1.52117811  0.26974764 -0.50941086 -1.0885101  -0.09340147  0.86674152\n",
      "  0.75781077 -0.35867187  0.07969632  0.31591547 -0.00412217 -0.19179108\n",
      "  0.02271239 -0.06066956  0.09849245]\n",
      "Gradient Descent(623/999): loss=0.40805879923646743, w = [-1.52131212  0.26961369 -0.50937448 -1.0895641  -0.09331575  0.86751533\n",
      "  0.75807526 -0.35895827  0.07963155  0.3159564  -0.00412035 -0.19140658\n",
      "  0.02271272 -0.06055645  0.09849267]\n",
      "Gradient Descent(624/999): loss=0.4080381134974129, w = [-1.52144585  0.26947963 -0.50933802 -1.09061718 -0.09323003  0.86828848\n",
      "  0.75833949 -0.35924465  0.07956683  0.31599712 -0.00411851 -0.19102227\n",
      "  0.02271303 -0.06044349  0.09849285]\n",
      "Gradient Descent(625/999): loss=0.40801746121317145, w = [-1.5215793   0.26934544 -0.50930149 -1.09166934 -0.09314429  0.86906097\n",
      "  0.75860348 -0.35953101  0.07950216  0.31603765 -0.00411667 -0.19063816\n",
      "  0.02271333 -0.0603307   0.09849299]\n",
      "Gradient Descent(626/999): loss=0.40799684230506605, w = [-1.52171247  0.26921115 -0.50926489 -1.09272058 -0.09305854  0.86983281\n",
      "  0.75886722 -0.35981734  0.07943754  0.31607797 -0.00411482 -0.19025424\n",
      "  0.02271362 -0.06021806  0.09849308]\n",
      "Gradient Descent(627/999): loss=0.4079762566947192, w = [-1.52184537  0.26907674 -0.50922822 -1.0937709  -0.09297278  0.87060399\n",
      "  0.75913071 -0.36010366  0.07937297  0.31611809 -0.00411296 -0.18987052\n",
      "  0.0227139  -0.06010558  0.09849313]\n",
      "Gradient Descent(628/999): loss=0.40795570430404743, w = [-1.521978    0.26894221 -0.50919148 -1.09482029 -0.09288701  0.87137451\n",
      "  0.75939394 -0.36038995  0.07930846  0.31615801 -0.00411108 -0.18948698\n",
      "  0.02271416 -0.05999327  0.09849314]\n",
      "Gradient Descent(629/999): loss=0.4079351850552621, w = [-1.52211035  0.26880758 -0.50915468 -1.09586878 -0.09280123  0.87214438\n",
      "  0.75965694 -0.36067622  0.079244    0.31619774 -0.0041092  -0.18910364\n",
      "  0.02271442 -0.05988111  0.09849311]\n",
      "Gradient Descent(630/999): loss=0.40791469887087073, w = [-1.52224244  0.26867283 -0.5091178  -1.09691634 -0.09271544  0.8729136\n",
      "  0.75991968 -0.36096248  0.0791796   0.31623726 -0.00410731 -0.1887205\n",
      "  0.02271466 -0.05976912  0.09849303]\n",
      "Gradient Descent(631/999): loss=0.4078942456736638, w = [-1.52237427  0.26853797 -0.50908086 -1.09796299 -0.09262964  0.87368216\n",
      "  0.76018218 -0.3612487   0.07911524  0.31627659 -0.00410542 -0.18833754\n",
      "  0.02271489 -0.05965728  0.09849292]\n",
      "Gradient Descent(632/999): loss=0.4078738253867271, w = [-1.52250583  0.268403   -0.50904385 -1.09900873 -0.09254383  0.87445007\n",
      "  0.76044443 -0.36153491  0.07905094  0.31631572 -0.00410351 -0.18795478\n",
      "  0.02271511 -0.05954561  0.09849276]\n",
      "Gradient Descent(633/999): loss=0.4078534379334293, w = [-1.52263713  0.26826793 -0.50900678 -1.10005356 -0.09245801  0.87521732\n",
      "  0.76070644 -0.3618211   0.07898669  0.31635466 -0.00410159 -0.1875722\n",
      "  0.02271532 -0.0594341   0.09849257]\n",
      "Gradient Descent(634/999): loss=0.40783308323742407, w = [-1.52276816  0.26813274 -0.50896964 -1.10109748 -0.09237219  0.87598393\n",
      "  0.7609682  -0.36210726  0.0789225   0.31639339 -0.00409967 -0.18718982\n",
      "  0.02271551 -0.05932275  0.09849233]\n",
      "Gradient Descent(635/999): loss=0.4078127612226471, w = [-1.52289894  0.26799745 -0.50893244 -1.10214049 -0.09228635  0.87674988\n",
      "  0.76122971 -0.3623934   0.07885836  0.31643194 -0.00409773 -0.18680764\n",
      "  0.0227157  -0.05921156  0.09849205]\n",
      "Gradient Descent(636/999): loss=0.40779247181331835, w = [-1.52302947  0.26786205 -0.50889518 -1.10318259 -0.09220051  0.87751519\n",
      "  0.76149099 -0.36267952  0.07879427  0.31647029 -0.00409579 -0.18642564\n",
      "  0.02271587 -0.05910053  0.09849174]\n",
      "Gradient Descent(637/999): loss=0.4077722149339324, w = [-1.52315974  0.26772655 -0.50885785 -1.10422379 -0.09211467  0.87827985\n",
      "  0.76175202 -0.36296562  0.07873023  0.31650844 -0.00409384 -0.18604383\n",
      "  0.02271603 -0.05898966  0.09849138]\n",
      "Gradient Descent(638/999): loss=0.40775199050926325, w = [-1.52328976  0.26759094 -0.50882047 -1.10526408 -0.09202881  0.87904385\n",
      "  0.7620128  -0.3632517   0.07866625  0.3165464  -0.00409188 -0.18566222\n",
      "  0.02271618 -0.05887896  0.09849099]\n",
      "Gradient Descent(639/999): loss=0.4077317984643598, w = [-1.52341953  0.26745523 -0.50878302 -1.10630347 -0.09194295  0.87980721\n",
      "  0.76227334 -0.36353775  0.07860232  0.31658417 -0.00408991 -0.18528079\n",
      "  0.02271632 -0.05876842  0.09849056]\n",
      "Gradient Descent(640/999): loss=0.40771163872454674, w = [-1.52354905  0.26731941 -0.50874551 -1.10734196 -0.09185709  0.88056993\n",
      "  0.76253364 -0.36382378  0.07853844  0.31662175 -0.00408794 -0.18489956\n",
      "  0.02271645 -0.05865804  0.09849009]\n",
      "Gradient Descent(641/999): loss=0.40769151121541974, w = [-1.52367832  0.26718349 -0.50870795 -1.10837955 -0.09177122  0.88133199\n",
      "  0.7627937  -0.36410979  0.07847462  0.31665913 -0.00408595 -0.18451851\n",
      "  0.02271657 -0.05854782  0.09848958]\n",
      "Gradient Descent(642/999): loss=0.40767141586284156, w = [-1.52380735  0.26704747 -0.50867032 -1.10941624 -0.09168535  0.88209342\n",
      "  0.76305352 -0.36439578  0.07841085  0.31669632 -0.00408396 -0.18413766\n",
      "  0.02271668 -0.05843777  0.09848903]\n",
      "Gradient Descent(643/999): loss=0.4076513525929483, w = [-1.52393613  0.26691135 -0.50863264 -1.11045203 -0.09159947  0.88285419\n",
      "  0.7633131  -0.36468174  0.07834713  0.31673333 -0.00408196 -0.18375699\n",
      "  0.02271677 -0.05832788  0.09848845]\n",
      "Gradient Descent(644/999): loss=0.40763132133213986, w = [-1.52406468  0.26677513 -0.5085949  -1.11148693 -0.09151359  0.88361433\n",
      "  0.76357243 -0.36496768  0.07828346  0.31677014 -0.00407995 -0.18337651\n",
      "  0.02271686 -0.05821816  0.09848782]\n",
      "Gradient Descent(645/999): loss=0.40761132200708106, w = [-1.52419298  0.2666388  -0.5085571  -1.11252093 -0.0914277   0.88437382\n",
      "  0.76383153 -0.3652536   0.07821985  0.31680676 -0.00407793 -0.18299623\n",
      "  0.02271693 -0.05810859  0.09848716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(646/999): loss=0.407591354544705, w = [-1.52432105  0.26650238 -0.50851925 -1.11355404 -0.09134182  0.88513266\n",
      "  0.76409039 -0.36553949  0.07815629  0.31684319 -0.0040759  -0.18261613\n",
      "  0.022717   -0.05799919  0.09848647]\n",
      "Gradient Descent(647/999): loss=0.4075714188722011, w = [-1.52444888  0.26636586 -0.50848135 -1.11458626 -0.09125593  0.88589087\n",
      "  0.764349   -0.36582536  0.07809279  0.31687944 -0.00407387 -0.18223622\n",
      "  0.02271705 -0.05788996  0.09848574]\n",
      "Gradient Descent(648/999): loss=0.4075515149170208, w = [-1.52457648  0.26622925 -0.50844339 -1.11561758 -0.09117003  0.88664843\n",
      "  0.76460738 -0.36611121  0.07802934  0.31691549 -0.00407182 -0.1818565\n",
      "  0.02271709 -0.05778089  0.09848497]\n",
      "Gradient Descent(649/999): loss=0.40753164260687635, w = [-1.52470385  0.26609254 -0.50840537 -1.11664802 -0.09108414  0.88740536\n",
      "  0.76486553 -0.36639703  0.07796594  0.31695136 -0.00406977 -0.18147697\n",
      "  0.02271713 -0.05767198  0.09848416]\n",
      "Gradient Descent(650/999): loss=0.40751180186973435, w = [-1.52483098  0.26595573 -0.5083673  -1.11767757 -0.09099825  0.88816164\n",
      "  0.76512343 -0.36668283  0.07790259  0.31698704 -0.00406772 -0.18109763\n",
      "  0.02271715 -0.05756324  0.09848332]\n",
      "Gradient Descent(651/999): loss=0.4074919926338191, w = [-1.52495789  0.26581882 -0.50832918 -1.11870624 -0.09091235  0.88891729\n",
      "  0.7653811  -0.36696861  0.0778393   0.31702254 -0.00406565 -0.18071847\n",
      "  0.02271716 -0.05745466  0.09848245]\n",
      "Gradient Descent(652/999): loss=0.40747221482760804, w = [-1.52508456  0.26568183 -0.50829101 -1.11973402 -0.09082646  0.8896723\n",
      "  0.76563853 -0.36725436  0.07777606  0.31705785 -0.00406357 -0.1803395\n",
      "  0.02271716 -0.05734625  0.09848153]\n",
      "Gradient Descent(653/999): loss=0.4074524683798301, w = [-1.52521102  0.26554473 -0.50825279 -1.12076091 -0.09074056  0.89042667\n",
      "  0.76589573 -0.36754009  0.07771287  0.31709297 -0.00406149 -0.17996072\n",
      "  0.02271715 -0.057238    0.09848059]\n",
      "Gradient Descent(654/999): loss=0.40743275321946837, w = [-1.52533725  0.26540755 -0.50821452 -1.12178693 -0.09065467  0.8911804\n",
      "  0.76615269 -0.3678258   0.07764974  0.31712791 -0.0040594  -0.17958213\n",
      "  0.02271713 -0.05712992  0.09847961]\n",
      "Gradient Descent(655/999): loss=0.4074130692757497, w = [-1.52546325  0.26527027 -0.5081762  -1.12281206 -0.09056878  0.8919335\n",
      "  0.76640941 -0.36811148  0.07758666  0.31716267 -0.00405731 -0.17920373\n",
      "  0.0227171  -0.057022    0.09847859]\n",
      "Gradient Descent(656/999): loss=0.4073934164781556, w = [-1.52558904  0.2651329  -0.50813783 -1.12383631 -0.09048288  0.89268596\n",
      "  0.7666659  -0.36839714  0.07752363  0.31719724 -0.0040552  -0.17882551\n",
      "  0.02271707 -0.05691425  0.09847754]\n",
      "Gradient Descent(657/999): loss=0.40737379475640945, w = [-1.5257146   0.26499545 -0.50809941 -1.12485969 -0.090397    0.89343779\n",
      "  0.76692216 -0.36868277  0.07746066  0.31723163 -0.00405309 -0.17844748\n",
      "  0.02271702 -0.05680666  0.09847646]\n",
      "Gradient Descent(658/999): loss=0.4073542040404818, w = [-1.52583995  0.2648579  -0.50806094 -1.12588219 -0.09031111  0.89418898\n",
      "  0.76717818 -0.36896838  0.07739774  0.31726583 -0.00405097 -0.17806963\n",
      "  0.02271696 -0.05669924  0.09847534]\n",
      "Gradient Descent(659/999): loss=0.40733464426058646, w = [-1.52596508  0.26472026 -0.50802243 -1.12690381 -0.09022522  0.89493954\n",
      "  0.76743397 -0.36925396  0.07733487  0.31729985 -0.00404884 -0.17769197\n",
      "  0.02271689 -0.05659198  0.09847419]\n",
      "Gradient Descent(660/999): loss=0.4073151153471774, w = [-1.52609     0.26458253 -0.50798387 -1.12792456 -0.09013934  0.89568947\n",
      "  0.76768953 -0.36953952  0.07727206  0.3173337  -0.0040467  -0.1773145\n",
      "  0.02271681 -0.05648489  0.09847301]\n",
      "Gradient Descent(661/999): loss=0.40729561723095276, w = [-1.52621471  0.26444472 -0.50794526 -1.12894444 -0.09005346  0.89643877\n",
      "  0.76794485 -0.36982505  0.07720929  0.31736736 -0.00404456 -0.17693721\n",
      "  0.02271672 -0.05637797  0.09847179]\n",
      "Gradient Descent(662/999): loss=0.407276149842849, w = [-1.5263392   0.26430682 -0.50790661 -1.12996344 -0.08996759  0.89718743\n",
      "  0.76819995 -0.37011056  0.07714659  0.31740084 -0.00404241 -0.17656011\n",
      "  0.02271662 -0.05627121  0.09847055]\n",
      "Gradient Descent(663/999): loss=0.4072567131140377, w = [-1.52646349  0.26416883 -0.50786792 -1.13098158 -0.08988172  0.89793547\n",
      "  0.76845481 -0.37039605  0.07708393  0.31743413 -0.00404025 -0.1761832\n",
      "  0.02271651 -0.05616462  0.09846926]\n",
      "Gradient Descent(664/999): loss=0.4072373069759303, w = [-1.52658756  0.26403076 -0.50782918 -1.13199885 -0.08979585  0.89868288\n",
      "  0.76870944 -0.37068151  0.07702133  0.31746725 -0.00403809 -0.17580647\n",
      "  0.0227164  -0.05605819  0.09846795]\n",
      "Gradient Descent(665/999): loss=0.4072179313601749, w = [-1.52671143  0.2638926  -0.50779039 -1.13301524 -0.08970999  0.89942965\n",
      "  0.76896384 -0.37096694  0.07695878  0.31750019 -0.00403592 -0.17542992\n",
      "  0.02271627 -0.05595193  0.09846661]\n",
      "Gradient Descent(666/999): loss=0.40719858619864613, w = [-1.5268351   0.26375435 -0.50775157 -1.13403078 -0.08962413  0.9001758\n",
      "  0.76921801 -0.37125235  0.07689629  0.31753295 -0.00403374 -0.17505356\n",
      "  0.02271613 -0.05584583  0.09846523]\n",
      "Gradient Descent(667/999): loss=0.40717927142346055, w = [-1.52695856  0.26361603 -0.5077127  -1.13504545 -0.08953828  0.90092132\n",
      "  0.76947195 -0.37153774  0.07683385  0.31756554 -0.00403155 -0.17467739\n",
      "  0.02271598 -0.05573991  0.09846382]\n",
      "Gradient Descent(668/999): loss=0.4071599869669588, w = [-1.52708181  0.26347762 -0.50767379 -1.13605925 -0.08945243  0.90166622\n",
      "  0.76972566 -0.37182309  0.07677146  0.31759794 -0.00402936 -0.1743014\n",
      "  0.02271583 -0.05563415  0.09846238]\n",
      "Gradient Descent(669/999): loss=0.40714073276171275, w = [-1.52720487  0.26333913 -0.50763483 -1.13707219 -0.0893666   0.90241049\n",
      "  0.76997914 -0.37210842  0.07670912  0.31763017 -0.00402716 -0.17392559\n",
      "  0.02271566 -0.05552855  0.09846091]\n",
      "Gradient Descent(670/999): loss=0.4071215087405286, w = [-1.52732773  0.26320055 -0.50759584 -1.13808428 -0.08928076  0.90315413\n",
      "  0.7702324  -0.37239373  0.07664684  0.31766222 -0.00402495 -0.17354997\n",
      "  0.02271549 -0.05542312  0.09845941]\n",
      "Gradient Descent(671/999): loss=0.4071023148364316, w = [-1.52745039  0.2630619  -0.50755681 -1.1390955  -0.08919494  0.90389715\n",
      "  0.77048542 -0.37267901  0.07658461  0.3176941  -0.00402274 -0.17317453\n",
      "  0.02271531 -0.05531786  0.09845788]\n",
      "Gradient Descent(672/999): loss=0.40708315098267894, w = [-1.52757285  0.26292316 -0.50751774 -1.14010586 -0.08910912  0.90463954\n",
      "  0.77073822 -0.37296427  0.07652244  0.3177258  -0.00402052 -0.17279928\n",
      "  0.02271511 -0.05521277  0.09845632]\n",
      "Gradient Descent(673/999): loss=0.40706401711274914, w = [-1.52769511  0.26278435 -0.50747863 -1.14111537 -0.08902331  0.90538131\n",
      "  0.77099079 -0.37324949  0.07646031  0.31775732 -0.00401829 -0.17242421\n",
      "  0.02271491 -0.05510784  0.09845473]\n",
      "Gradient Descent(674/999): loss=0.4070449131603485, w = [-1.52781719  0.26264546 -0.50743948 -1.14212402 -0.08893751  0.90612246\n",
      "  0.77124314 -0.3735347   0.07639824  0.31778867 -0.00401606 -0.17204932\n",
      "  0.0227147  -0.05500308  0.09845311]\n",
      "Gradient Descent(675/999): loss=0.4070258390593995, w = [-1.52793907  0.26250648 -0.50740029 -1.14313182 -0.08885172  0.90686299\n",
      "  0.77149526 -0.37381987  0.07633623  0.31781985 -0.00401381 -0.17167462\n",
      "  0.02271448 -0.05489849  0.09845146]\n",
      "Gradient Descent(676/999): loss=0.40700679474404994, w = [-1.52806076  0.26236744 -0.50736107 -1.14413876 -0.08876593  0.90760289\n",
      "  0.77174715 -0.37410502  0.07627427  0.31785085 -0.00401157 -0.1713001\n",
      "  0.02271425 -0.05479406  0.09844979]\n",
      "Gradient Descent(677/999): loss=0.40698778014866616, w = [-1.52818226  0.26222831 -0.50732181 -1.14514485 -0.08868015  0.90834218\n",
      "  0.77199882 -0.37439014  0.07621236  0.31788168 -0.00400931 -0.17092576\n",
      "  0.02271401 -0.0546898   0.09844808]\n",
      "Gradient Descent(678/999): loss=0.40696879520783513, w = [-1.52830357  0.26208911 -0.50728251 -1.14615009 -0.08859439  0.90908084\n",
      "  0.77225027 -0.37467524  0.0761505   0.31791233 -0.00400705 -0.1705516\n",
      "  0.02271377 -0.05458571  0.09844634]\n",
      "Gradient Descent(679/999): loss=0.4069498398563585, w = [-1.52842469  0.26194983 -0.50724318 -1.14715449 -0.08850863  0.90981888\n",
      "  0.77250149 -0.3749603   0.0760887   0.31794281 -0.00400478 -0.17017763\n",
      "  0.02271351 -0.05448178  0.09844458]\n",
      "Gradient Descent(680/999): loss=0.4069309140292563, w = [-1.52854563  0.26181048 -0.50720381 -1.14815803 -0.08842289  0.91055631\n",
      "  0.77275248 -0.37524535  0.07602694  0.31797312 -0.00400251 -0.16980384\n",
      "  0.02271325 -0.05437802  0.09844278]\n",
      "Gradient Descent(681/999): loss=0.40691201766176144, w = [-1.52866639  0.26167105 -0.50716441 -1.14916073 -0.08833715  0.91129312\n",
      "  0.77300325 -0.37553036  0.07596525  0.31800326 -0.00400023 -0.16943023\n",
      "  0.02271297 -0.05427443  0.09844096]\n",
      "Gradient Descent(682/999): loss=0.4068931506893224, w = [-1.52878696  0.26153155 -0.50712497 -1.15016258 -0.08825142  0.91202931\n",
      "  0.7732538  -0.37581535  0.0759036   0.31803323 -0.00399794 -0.1690568\n",
      "  0.02271269 -0.05417101  0.09843912]\n",
      "Gradient Descent(683/999): loss=0.4068743130476004, w = [-1.52890734  0.26139198 -0.5070855  -1.15116359 -0.08816571  0.91276489\n",
      "  0.77350413 -0.3761003   0.07584201  0.31806303 -0.00399565 -0.16868356\n",
      "  0.0227124  -0.05406775  0.09843724]\n",
      "Gradient Descent(684/999): loss=0.406855504672468, w = [-1.52902755  0.26125233 -0.507046   -1.15216375 -0.08808001  0.91349985\n",
      "  0.77375423 -0.37638524  0.07578047  0.31809266 -0.00399335 -0.16831049\n",
      "  0.0227121  -0.05396466  0.09843534]\n",
      "Gradient Descent(685/999): loss=0.4068367255000082, w = [-1.52914758  0.26111262 -0.50700646 -1.15316308 -0.08799432  0.91423419\n",
      "  0.77400412 -0.37667014  0.07571899  0.31812211 -0.00399104 -0.16793761\n",
      "  0.0227118  -0.05386174  0.09843341]\n",
      "Gradient Descent(686/999): loss=0.4068179754665149, w = [-1.52926743  0.26097283 -0.50696689 -1.15416156 -0.08790864  0.91496792\n",
      "  0.77425378 -0.37695502  0.07565755  0.3181514  -0.00398873 -0.16756491\n",
      "  0.02271148 -0.05375899  0.09843145]\n",
      "Gradient Descent(687/999): loss=0.40679925450848575, w = [-1.52938711  0.26083297 -0.50692729 -1.15515921 -0.08782297  0.91570104\n",
      "  0.77450322 -0.37723986  0.07559617  0.31818052 -0.00398641 -0.16719239\n",
      "  0.02271116 -0.0536564   0.09842947]\n",
      "Gradient Descent(688/999): loss=0.4067805625626311, w = [-1.5295066   0.26069304 -0.50688766 -1.15615601 -0.08773732  0.91643354\n",
      "  0.77475244 -0.37752468  0.07553485  0.31820947 -0.00398409 -0.16682005\n",
      "  0.02271082 -0.05355398  0.09842746]\n",
      "Gradient Descent(689/999): loss=0.40676189956586595, w = [-1.52962593  0.26055304 -0.506848   -1.15715198 -0.08765168  0.91716543\n",
      "  0.77500143 -0.37780948  0.07547357  0.31823826 -0.00398176 -0.16644789\n",
      "  0.02271048 -0.05345173  0.09842542]\n",
      "Gradient Descent(690/999): loss=0.4067432654553062, w = [-1.52974508  0.26041298 -0.50680831 -1.15814712 -0.08756605  0.91789671\n",
      "  0.77525021 -0.37809424  0.07541235  0.31826687 -0.00397942 -0.16607591\n",
      "  0.02271013 -0.05334965  0.09842336]\n",
      "Gradient Descent(691/999): loss=0.40672466016827674, w = [-1.52986406  0.26027284 -0.50676858 -1.15914142 -0.08748044  0.91862738\n",
      "  0.77549877 -0.37837897  0.07535119  0.31829532 -0.00397708 -0.16570411\n",
      "  0.02270978 -0.05324773  0.09842127]\n",
      "Gradient Descent(692/999): loss=0.4067060836423042, w = [-1.52998286  0.26013264 -0.50672883 -1.16013488 -0.08739484  0.91935744\n",
      "  0.77574711 -0.37866368  0.07529007  0.31832361 -0.00397473 -0.16533249\n",
      "  0.02270941 -0.05314598  0.09841916]\n",
      "Gradient Descent(693/999): loss=0.4066875358151157, w = [-1.5301015   0.25999238 -0.50668905 -1.16112752 -0.08730925  0.92008689\n",
      "  0.77599524 -0.37894836  0.07522901  0.31835173 -0.00397237 -0.16496105\n",
      "  0.02270904 -0.0530444   0.09841702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(694/999): loss=0.4066690166246391, w = [-1.53021997  0.25985204 -0.50664924 -1.16211933 -0.08722368  0.92081573\n",
      "  0.77624314 -0.37923301  0.075168    0.31837968 -0.00397001 -0.16458979\n",
      "  0.02270866 -0.05294299  0.09841486]\n",
      "Gradient Descent(695/999): loss=0.4066505260090033, w = [-1.53033827  0.25971164 -0.50660941 -1.1631103  -0.08713812  0.92154396\n",
      "  0.77649083 -0.37951762  0.07510704  0.31840746 -0.00396765 -0.16421871\n",
      "  0.02270827 -0.05284174  0.09841267]\n",
      "Gradient Descent(696/999): loss=0.40663206390653545, w = [-1.53045641  0.25957118 -0.50656954 -1.16410045 -0.08705258  0.92227158\n",
      "  0.7767383  -0.37980222  0.07504614  0.31843509 -0.00396527 -0.16384781\n",
      "  0.02270787 -0.05274066  0.09841046]\n",
      "Gradient Descent(697/999): loss=0.4066136302557609, w = [-1.53057438  0.25943065 -0.50652965 -1.16508977 -0.08696706  0.9229986\n",
      "  0.77698555 -0.38008678  0.07498529  0.31846254 -0.0039629  -0.16347709\n",
      "  0.02270746 -0.05263975  0.09840822]\n",
      "Gradient Descent(698/999): loss=0.40659522499539996, w = [-1.53069219  0.25929006 -0.50648974 -1.16607827 -0.08688154  0.923725\n",
      "  0.77723258 -0.38037131  0.07492449  0.31848984 -0.00396051 -0.16310654\n",
      "  0.02270705 -0.052539    0.09840596]\n",
      "Gradient Descent(699/999): loss=0.40657684806437094, w = [-1.53080983  0.2591494  -0.50644979 -1.16706594 -0.08679605  0.92445081\n",
      "  0.7774794  -0.38065581  0.07486375  0.31851697 -0.00395812 -0.16273618\n",
      "  0.02270663 -0.05243843  0.09840368]\n",
      "Gradient Descent(700/999): loss=0.40655849940178296, w = [-1.53092731  0.25900869 -0.50640982 -1.16805279 -0.08671057  0.92517601\n",
      "  0.77772601 -0.38094028  0.07480305  0.31854394 -0.00395573 -0.16236599\n",
      "  0.0227062  -0.05233802  0.09840137]\n",
      "Gradient Descent(701/999): loss=0.4065401789469458, w = [-1.53104463  0.25886791 -0.50636983 -1.16903882 -0.08662511  0.9259006\n",
      "  0.77797239 -0.38122473  0.07474241  0.31857074 -0.00395333 -0.16199598\n",
      "  0.02270576 -0.05223778  0.09839904]\n",
      "Gradient Descent(702/999): loss=0.40652188663935435, w = [-1.5311618   0.25872707 -0.50632981 -1.17002402 -0.08653967  0.92662459\n",
      "  0.77821857 -0.38150914  0.07468183  0.31859739 -0.00395092 -0.16162615\n",
      "  0.02270532 -0.0521377   0.09839668]\n",
      "Gradient Descent(703/999): loss=0.40650362241870003, w = [-1.5312788   0.25858617 -0.50628977 -1.17100841 -0.08645424  0.92734798\n",
      "  0.77846453 -0.38179352  0.07462129  0.31862387 -0.00394851 -0.1612565\n",
      "  0.02270487 -0.0520378   0.09839431]\n",
      "Gradient Descent(704/999): loss=0.40648538622486424, w = [-1.53139565  0.25844521 -0.5062497  -1.17199198 -0.08636883  0.92807077\n",
      "  0.77871027 -0.38207787  0.07456081  0.31865019 -0.00394609 -0.16088703\n",
      "  0.02270441 -0.05193806  0.09839191]\n",
      "Gradient Descent(705/999): loss=0.40646717799791754, w = [-1.53151234  0.25830419 -0.50620961 -1.17297473 -0.08628344  0.92879295\n",
      "  0.7789558  -0.3823622   0.07450038  0.31867635 -0.00394367 -0.16051773\n",
      "  0.02270394 -0.05183848  0.09838948]\n",
      "Gradient Descent(706/999): loss=0.4064489976781191, w = [-1.53162887  0.25816311 -0.50616949 -1.17395667 -0.08619806  0.92951453\n",
      "  0.77920112 -0.38264649  0.07444001  0.31870235 -0.00394124 -0.16014861\n",
      "  0.02270347 -0.05173908  0.09838704]\n",
      "Gradient Descent(707/999): loss=0.40643084520591605, w = [-1.53174525  0.25802197 -0.50612936 -1.17493779 -0.08611271  0.93023552\n",
      "  0.77944622 -0.38293075  0.07437968  0.31872819 -0.0039388  -0.15977967\n",
      "  0.02270298 -0.05163984  0.09838457]\n",
      "Gradient Descent(708/999): loss=0.4064127205219447, w = [-1.53186148  0.25788078 -0.5060892  -1.1759181  -0.08602737  0.9309559\n",
      "  0.77969111 -0.38321498  0.07431941  0.31875387 -0.00393637 -0.1594109\n",
      "  0.02270249 -0.05154077  0.09838208]\n",
      "Gradient Descent(709/999): loss=0.4063946235670265, w = [-1.53197755  0.25773952 -0.50604901 -1.1768976  -0.08594205  0.93167568\n",
      "  0.77993579 -0.38349918  0.07425919  0.31877939 -0.00393392 -0.15904231\n",
      "  0.022702   -0.05144186  0.09837957]\n",
      "Gradient Descent(710/999): loss=0.40637655428216435, w = [-1.53209348  0.25759822 -0.50600881 -1.17787628 -0.08585675  0.93239487\n",
      "  0.78018026 -0.38378335  0.07419902  0.31880475 -0.00393147 -0.1586739\n",
      "  0.02270149 -0.05134313  0.09837703]\n",
      "Gradient Descent(711/999): loss=0.40635851260855244, w = [-1.53220925  0.25745685 -0.50596859 -1.17885416 -0.08577147  0.93311346\n",
      "  0.78042452 -0.38406748  0.07413891  0.31882995 -0.00392902 -0.15830567\n",
      "  0.02270098 -0.05124456  0.09837448]\n",
      "Gradient Descent(712/999): loss=0.4063404984875625, w = [-1.53232488  0.25731543 -0.50592834 -1.17983122 -0.08568621  0.93383145\n",
      "  0.78066856 -0.38435159  0.07407885  0.318855   -0.00392656 -0.15793761\n",
      "  0.02270046 -0.05114616  0.0983719 ]\n",
      "Gradient Descent(713/999): loss=0.4063225118607512, w = [-1.53244035  0.25717396 -0.50588808 -1.18080748 -0.08560097  0.93454884\n",
      "  0.78091239 -0.38463566  0.07401884  0.31887989 -0.00392409 -0.15756973\n",
      "  0.02269994 -0.05104792  0.0983693 ]\n",
      "Gradient Descent(714/999): loss=0.4063045526698578, w = [-1.53255568  0.25703243 -0.50584779 -1.18178294 -0.08551575  0.93526564\n",
      "  0.78115602 -0.38491971  0.07395888  0.31890462 -0.00392162 -0.15720202\n",
      "  0.0226994  -0.05094985  0.09836669]\n",
      "Gradient Descent(715/999): loss=0.40628662085680084, w = [-1.53267087  0.25689085 -0.50580749 -1.18275759 -0.08543055  0.93598184\n",
      "  0.78139943 -0.38520372  0.07389898  0.3189292  -0.00391914 -0.15683449\n",
      "  0.02269886 -0.05085195  0.09836405]\n",
      "Gradient Descent(716/999): loss=0.406268716363678, w = [-1.53278591  0.25674921 -0.50576717 -1.18373143 -0.08534537  0.93669745\n",
      "  0.78164263 -0.3854877   0.07383912  0.31895361 -0.00391666 -0.15646713\n",
      "  0.02269832 -0.05075422  0.09836139]\n",
      "Gradient Descent(717/999): loss=0.4062508391327678, w = [-1.5329008   0.25660752 -0.50572682 -1.18470447 -0.08526022  0.93741246\n",
      "  0.78188563 -0.38577165  0.07377932  0.31897788 -0.00391418 -0.15609995\n",
      "  0.02269776 -0.05065665  0.09835871]\n",
      "Gradient Descent(718/999): loss=0.40623298910652667, w = [-1.53301555  0.25646578 -0.50568646 -1.18567671 -0.08517508  0.93812689\n",
      "  0.78212841 -0.38605556  0.07371957  0.31900199 -0.00391169 -0.15573295\n",
      "  0.0226972  -0.05055925  0.09835601]\n",
      "Gradient Descent(719/999): loss=0.40621516622758835, w = [-1.53313016  0.25632399 -0.50564608 -1.18664815 -0.08508996  0.93884071\n",
      "  0.78237099 -0.38633945  0.07365988  0.31902594 -0.00390919 -0.15536612\n",
      "  0.02269663 -0.05046201  0.09835329]\n",
      "Gradient Descent(720/999): loss=0.40619737043876003, w = [-1.53324463  0.25618215 -0.50560569 -1.18761879 -0.08500487  0.93955395\n",
      "  0.78261336 -0.3866233   0.07360024  0.31904974 -0.00390669 -0.15499946\n",
      "  0.02269606 -0.05036494  0.09835055]\n",
      "Gradient Descent(721/999): loss=0.40617960168303086, w = [-1.53335896  0.25604026 -0.50556527 -1.18858864 -0.0849198   0.9402666\n",
      "  0.78285552 -0.38690712  0.07354064  0.31907338 -0.00390419 -0.15463298\n",
      "  0.02269547 -0.05026804  0.09834779]\n",
      "Gradient Descent(722/999): loss=0.4061618599035579, w = [-1.53347315  0.25589831 -0.50552484 -1.18955768 -0.08483475  0.94097865\n",
      "  0.78309747 -0.3871909   0.0734811   0.31909687 -0.00390167 -0.15426668\n",
      "  0.02269489 -0.05017131  0.09834502]\n",
      "Gradient Descent(723/999): loss=0.40614414504367913, w = [-1.53358719  0.25575632 -0.5054844  -1.19052593 -0.08474972  0.94169012\n",
      "  0.78333922 -0.38747465  0.07342162  0.31912021 -0.00389916 -0.15390054\n",
      "  0.02269429 -0.05007474  0.09834222]\n",
      "Gradient Descent(724/999): loss=0.40612645704689765, w = [-1.53370111  0.25561428 -0.50544393 -1.19149339 -0.08466471  0.94240099\n",
      "  0.78358076 -0.38775838  0.07336218  0.31914339 -0.00389664 -0.15353459\n",
      "  0.02269369 -0.04997833  0.0983394 ]\n",
      "Gradient Descent(725/999): loss=0.4061087958568972, w = [-1.53381488  0.25547219 -0.50540345 -1.19246005 -0.08457973  0.94311128\n",
      "  0.78382209 -0.38804206  0.0733028   0.31916643 -0.00389412 -0.1531688\n",
      "  0.02269308 -0.0498821   0.09833657]\n",
      "Gradient Descent(726/999): loss=0.4060911614175275, w = [-1.53392852  0.25533006 -0.50536296 -1.19342592 -0.08449477  0.94382098\n",
      "  0.78406322 -0.38832572  0.07324347  0.31918931 -0.00389159 -0.15280319\n",
      "  0.02269246 -0.04978603  0.09833372]\n",
      "Gradient Descent(727/999): loss=0.4060735536728109, w = [-1.53404202  0.25518787 -0.50532245 -1.194391   -0.08440984  0.94453009\n",
      "  0.78430414 -0.38860934  0.07318419  0.31921203 -0.00388905 -0.15243776\n",
      "  0.02269184 -0.04969012  0.09833085]\n",
      "Gradient Descent(728/999): loss=0.40605597256693954, w = [-1.53415539  0.25504564 -0.50528192 -1.19535529 -0.08432493  0.94523862\n",
      "  0.78454486 -0.38889293  0.07312496  0.31923461 -0.00388651 -0.1520725\n",
      "  0.02269121 -0.04959438  0.09832796]\n",
      "Gradient Descent(729/999): loss=0.4060384180442757, w = [-1.53426863  0.25490337 -0.50524139 -1.19631879 -0.08424004  0.94594656\n",
      "  0.78478537 -0.38917648  0.07306578  0.31925704 -0.00388397 -0.15170741\n",
      "  0.02269058 -0.04949881  0.09832505]\n",
      "Gradient Descent(730/999): loss=0.40602089004934805, w = [-1.53438173  0.25476105 -0.50520083 -1.1972815  -0.08415517  0.94665391\n",
      "  0.78502568 -0.38946     0.07300666  0.31927931 -0.00388142 -0.15134249\n",
      "  0.02268993 -0.0494034   0.09832213]\n",
      "Gradient Descent(731/999): loss=0.406003388526854, w = [-1.53449471  0.25461868 -0.50516027 -1.19824342 -0.08407033  0.94736068\n",
      "  0.78526578 -0.38974349  0.07294759  0.31930144 -0.00387887 -0.15097775\n",
      "  0.02268929 -0.04930816  0.09831918]\n",
      "Gradient Descent(732/999): loss=0.40598591342165863, w = [-1.53460755  0.25447627 -0.50511968 -1.19920456 -0.08398552  0.94806686\n",
      "  0.78550568 -0.39002694  0.07288857  0.31932341 -0.00387631 -0.15061318\n",
      "  0.02268863 -0.04921309  0.09831622]\n",
      "Gradient Descent(733/999): loss=0.40596846467879294, w = [-1.53472026  0.25433381 -0.50507909 -1.20016492 -0.08390073  0.94877246\n",
      "  0.78574537 -0.39031036  0.0728296   0.31934524 -0.00387375 -0.15024878\n",
      "  0.02268797 -0.04911818  0.09831325]\n",
      "Gradient Descent(734/999): loss=0.40595104224345185, w = [-1.53483284  0.25419132 -0.50503848 -1.20112449 -0.08381596  0.94947748\n",
      "  0.78598487 -0.39059374  0.07277068  0.31936691 -0.00387119 -0.14988456\n",
      "  0.0226873  -0.04902343  0.09831025]\n",
      "Gradient Descent(735/999): loss=0.40593364606099536, w = [-1.5349453   0.25404877 -0.50499786 -1.20208328 -0.08373122  0.95018191\n",
      "  0.78622416 -0.39087709  0.07271182  0.31938844 -0.00386862 -0.14952051\n",
      "  0.02268663 -0.04892885  0.09830724]\n",
      "Gradient Descent(736/999): loss=0.4059162760769478, w = [-1.53505762  0.25390619 -0.50495723 -1.20304129 -0.08364651  0.95088577\n",
      "  0.78646324 -0.39116041  0.07265301  0.31940982 -0.00386604 -0.14915663\n",
      "  0.02268595 -0.04883444  0.09830421]\n",
      "Gradient Descent(737/999): loss=0.4058989322369963, w = [-1.53516983  0.25376357 -0.50491659 -1.20399852 -0.08356182  0.95158904\n",
      "  0.78670213 -0.39144369  0.07259424  0.31943105 -0.00386346 -0.14879292\n",
      "  0.02268526 -0.04874019  0.09830117]\n",
      "Gradient Descent(738/999): loss=0.40588161448699195, w = [-1.5352819   0.2536209  -0.50487594 -1.20495497 -0.08347716  0.95229173\n",
      "  0.78694081 -0.39172693  0.07253553  0.31945214 -0.00386088 -0.14842939\n",
      "  0.02268457 -0.04864611  0.09829811]\n",
      "Gradient Descent(739/999): loss=0.4058643227729447, w = [-1.53539385  0.25347819 -0.50483527 -1.20591065 -0.08339252  0.95299385\n",
      "  0.78717929 -0.39201014  0.07247688  0.31947307 -0.00385829 -0.14806602\n",
      "  0.02268387 -0.04855219  0.09829503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(740/999): loss=0.4058470570410278, w = [-1.53550568  0.25333544 -0.50479459 -1.20686555 -0.08330791  0.95369538\n",
      "  0.78741757 -0.39229332  0.07241827  0.31949386 -0.0038557  -0.14770283\n",
      "  0.02268317 -0.04845843  0.09829194]\n",
      "Gradient Descent(741/999): loss=0.4058298172375727, w = [-1.53561738  0.25319266 -0.5047539  -1.20781967 -0.08322332  0.95439633\n",
      "  0.78765565 -0.39257646  0.07235971  0.31951451 -0.00385311 -0.14733981\n",
      "  0.02268246 -0.04836484  0.09828883]\n",
      "Gradient Descent(742/999): loss=0.4058126033090702, w = [-1.53572896  0.25304983 -0.50471321 -1.20877301 -0.08313877  0.95509671\n",
      "  0.78789353 -0.39285957  0.07230121  0.31953501 -0.00385051 -0.14697696\n",
      "  0.02268174 -0.04827142  0.09828571]\n",
      "Gradient Descent(743/999): loss=0.4057954152021741, w = [-1.53584042  0.25290696 -0.5046725  -1.20972559 -0.08305423  0.95579651\n",
      "  0.78813121 -0.39314264  0.07224276  0.31955536 -0.0038479  -0.14661428\n",
      "  0.02268102 -0.04817816  0.09828257]\n",
      "Gradient Descent(744/999): loss=0.4057782528636909, w = [-1.53595175  0.25276406 -0.50463178 -1.21067739 -0.08296973  0.95649573\n",
      "  0.78836869 -0.39342567  0.07218435  0.31957556 -0.00384529 -0.14625177\n",
      "  0.02268029 -0.04808506  0.09827941]\n",
      "Gradient Descent(745/999): loss=0.40576111624058725, w = [-1.53606297  0.25262112 -0.50459105 -1.21162842 -0.08288526  0.95719438\n",
      "  0.78860597 -0.39370867  0.072126    0.31959563 -0.00384268 -0.14588943\n",
      "  0.02267955 -0.04799213  0.09827624]\n",
      "Gradient Descent(746/999): loss=0.4057440052799858, w = [-1.53617407  0.25247814 -0.50455032 -1.21257869 -0.08280081  0.95789245\n",
      "  0.78884305 -0.39399163  0.07206771  0.31961554 -0.00384006 -0.14552727\n",
      "  0.02267881 -0.04789936  0.09827306]\n",
      "Gradient Descent(747/999): loss=0.405726919929165, w = [-1.53628504  0.25233512 -0.50450957 -1.21352818 -0.08271639  0.95858995\n",
      "  0.78907994 -0.39427456  0.07200946  0.31963532 -0.00383744 -0.14516527\n",
      "  0.02267807 -0.04780676  0.09826986]\n",
      "Gradient Descent(748/999): loss=0.40570986013555943, w = [-1.5363959   0.25219207 -0.50446882 -1.2144769  -0.08263199  0.95928687\n",
      "  0.78931662 -0.39455745  0.07195126  0.31965495 -0.00383482 -0.14480345\n",
      "  0.02267731 -0.04771432  0.09826664]\n",
      "Gradient Descent(749/999): loss=0.40569282584675664, w = [-1.53650665  0.25204898 -0.50442806 -1.21542486 -0.08254763  0.95998322\n",
      "  0.78955311 -0.39484031  0.07189312  0.31967443 -0.00383219 -0.14444179\n",
      "  0.02267655 -0.04762204  0.09826341]\n",
      "Gradient Descent(750/999): loss=0.40567581701050087, w = [-1.53661727  0.25190586 -0.50438729 -1.21637206 -0.08246329  0.960679\n",
      "  0.78978939 -0.39512313  0.07183502  0.31969378 -0.00382956 -0.14408031\n",
      "  0.02267579 -0.04752993  0.09826017]\n",
      "Gradient Descent(751/999): loss=0.40565883357468674, w = [-1.53672778  0.2517627  -0.50434651 -1.21731848 -0.08237898  0.9613742\n",
      "  0.79002549 -0.39540591  0.07177698  0.31971298 -0.00382692 -0.14371899\n",
      "  0.02267502 -0.04743798  0.09825691]\n",
      "Gradient Descent(752/999): loss=0.4056418754873634, w = [-1.53683818  0.25161951 -0.50430573 -1.21826415 -0.0822947   0.96206883\n",
      "  0.79026138 -0.39568865  0.07171899  0.31973204 -0.00382428 -0.14335784\n",
      "  0.02267425 -0.04734619  0.09825364]\n",
      "Gradient Descent(753/999): loss=0.4056249426967305, w = [-1.53694846  0.25147628 -0.50426494 -1.21920905 -0.08221045  0.96276289\n",
      "  0.79049708 -0.39597136  0.07166105  0.31975095 -0.00382164 -0.14299687\n",
      "  0.02267347 -0.04725457  0.09825035]\n",
      "Gradient Descent(754/999): loss=0.405608035151139, w = [-1.53705862  0.25133302 -0.50422414 -1.2201532  -0.08212623  0.96345638\n",
      "  0.79073258 -0.39625403  0.07160316  0.31976973 -0.00381899 -0.14263606\n",
      "  0.02267268 -0.04716311  0.09824705]\n",
      "Gradient Descent(755/999): loss=0.4055911527990936, w = [-1.53716868  0.25118972 -0.50418333 -1.22109658 -0.08204204  0.9641493\n",
      "  0.79096788 -0.39653667  0.07154532  0.31978836 -0.00381634 -0.14227542\n",
      "  0.02267189 -0.04707181  0.09824374]\n",
      "Gradient Descent(756/999): loss=0.4055742955892436, w = [-1.53727862  0.2510464  -0.50414252 -1.2220392  -0.08195788  0.96484166\n",
      "  0.79120299 -0.39681926  0.07148753  0.31980685 -0.00381368 -0.14191495\n",
      "  0.02267109 -0.04698068  0.09824041]\n",
      "Gradient Descent(757/999): loss=0.40555746347039257, w = [-1.53738845  0.25090304 -0.5041017  -1.22298107 -0.08187375  0.96553344\n",
      "  0.79143791 -0.39710182  0.07142979  0.3198252  -0.00381103 -0.14155465\n",
      "  0.02267029 -0.04688971  0.09823707]\n",
      "Gradient Descent(758/999): loss=0.4055406563914901, w = [-1.53749817  0.25075965 -0.50406088 -1.22392218 -0.08178965  0.96622465\n",
      "  0.79167263 -0.39738435  0.0713721   0.31984341 -0.00380836 -0.14119452\n",
      "  0.02266948 -0.0467989   0.09823372]\n",
      "Gradient Descent(759/999): loss=0.40552387430163483, w = [-1.53760777  0.25061623 -0.50402005 -1.22486253 -0.08170557  0.9669153\n",
      "  0.79190715 -0.39766683  0.07131447  0.31986149 -0.0038057  -0.14083456\n",
      "  0.02266866 -0.04670825  0.09823035]\n",
      "Gradient Descent(760/999): loss=0.40550711715007165, w = [-1.53771727  0.25047278 -0.50397922 -1.22580213 -0.08162153  0.96760538\n",
      "  0.79214148 -0.39794928  0.07125688  0.31987942 -0.00380303 -0.14047476\n",
      "  0.02266785 -0.04661777  0.09822698]\n",
      "Gradient Descent(761/999): loss=0.40549038488619427, w = [-1.53782666  0.2503293  -0.50393838 -1.22674097 -0.08153752  0.9682949\n",
      "  0.79237562 -0.39823169  0.07119935  0.31989721 -0.00380035 -0.14011514\n",
      "  0.02266702 -0.04652745  0.09822359]\n",
      "Gradient Descent(762/999): loss=0.40547367745954, w = [-1.53793594  0.25018578 -0.50389753 -1.22767906 -0.08145354  0.96898385\n",
      "  0.79260956 -0.39851406  0.07114187  0.31991487 -0.00379768 -0.13975568\n",
      "  0.02266619 -0.04643729  0.09822018]\n",
      "Gradient Descent(763/999): loss=0.40545699481979475, w = [-1.53804512  0.25004224 -0.50385669 -1.2286164  -0.08136959  0.96967223\n",
      "  0.79284331 -0.39879639  0.07108443  0.31993238 -0.003795   -0.13939639\n",
      "  0.02266536 -0.04634729  0.09821677]\n",
      "Gradient Descent(764/999): loss=0.40544033691678605, w = [-1.53815418  0.24989867 -0.50381583 -1.22955299 -0.08128567  0.97036005\n",
      "  0.79307686 -0.39907869  0.07102705  0.31994976 -0.00379231 -0.13903726\n",
      "  0.02266451 -0.04625745  0.09821334]\n",
      "Gradient Descent(765/999): loss=0.4054237037004886, w = [-1.53826314  0.24975508 -0.50377498 -1.23048883 -0.08120178  0.97104731\n",
      "  0.79331022 -0.39936094  0.07096972  0.319967   -0.00378962 -0.13867831\n",
      "  0.02266367 -0.04616778  0.0982099 ]\n",
      "Gradient Descent(766/999): loss=0.40540709512101913, w = [-1.538372    0.24961145 -0.50373412 -1.23142392 -0.08111793  0.971734\n",
      "  0.79354339 -0.39964316  0.07091244  0.3199841  -0.00378693 -0.13831952\n",
      "  0.02266282 -0.04607826  0.09820644]\n",
      "Gradient Descent(767/999): loss=0.40539051112863916, w = [-1.53848074  0.2494678  -0.50369325 -1.23235827 -0.0810341   0.97242013\n",
      "  0.79377637 -0.39992534  0.07085521  0.32000107 -0.00378424 -0.1379609\n",
      "  0.02266196 -0.04598891  0.09820298]\n",
      "Gradient Descent(768/999): loss=0.40537395167374995, w = [-1.53858939  0.24932412 -0.50365239 -1.23329187 -0.08095031  0.9731057\n",
      "  0.79400916 -0.40020748  0.07079803  0.3200179  -0.00378154 -0.13760245\n",
      "  0.0226611  -0.04589972  0.09819951]\n",
      "Gradient Descent(769/999): loss=0.4053574167068973, w = [-1.53869793  0.24918041 -0.50361152 -1.23422472 -0.08086655  0.97379071\n",
      "  0.79424175 -0.40048958  0.0707409   0.32003459 -0.00377884 -0.13724416\n",
      "  0.02266023 -0.04581069  0.09819602]\n",
      "Gradient Descent(770/999): loss=0.4053409061787663, w = [-1.53880637  0.24903668 -0.50357064 -1.23515683 -0.08078282  0.97447516\n",
      "  0.79447415 -0.40077165  0.07068382  0.32005114 -0.00377614 -0.13688604\n",
      "  0.02265936 -0.04572182  0.09819252]\n",
      "Gradient Descent(771/999): loss=0.4053244200401852, w = [-1.5389147   0.24889292 -0.50352977 -1.23608819 -0.08069912  0.97515904\n",
      "  0.79470637 -0.40105367  0.07062679  0.32006756 -0.00377343 -0.13652809\n",
      "  0.02265849 -0.04563311  0.09818901]\n",
      "Gradient Descent(772/999): loss=0.4053079582421209, w = [-1.53902293  0.24874913 -0.50348889 -1.23701882 -0.08061546  0.97584237\n",
      "  0.79493839 -0.40133565  0.07056981  0.32008385 -0.00377072 -0.13617031\n",
      "  0.02265761 -0.04554456  0.09818549]\n",
      "Gradient Descent(773/999): loss=0.40529152073567937, w = [-1.53913106  0.24860532 -0.50344801 -1.2379487  -0.08053183  0.97652514\n",
      "  0.79517022 -0.4016176   0.07051288  0.3201     -0.003768   -0.13581269\n",
      "  0.02265672 -0.04545618  0.09818196]\n",
      "Gradient Descent(774/999): loss=0.40527510747210777, w = [-1.53923909  0.24846149 -0.50340713 -1.23887784 -0.08044823  0.97720735\n",
      "  0.79540186 -0.4018995   0.070456    0.32011602 -0.00376528 -0.13545523\n",
      "  0.02265583 -0.04536795  0.09817842]\n",
      "Gradient Descent(775/999): loss=0.4052587184027888, w = [-1.53934702  0.24831763 -0.50336625 -1.23980624 -0.08036466  0.977889\n",
      "  0.79563331 -0.40218137  0.07039917  0.3201319  -0.00376256 -0.13509795\n",
      "  0.02265493 -0.04527988  0.09817486]\n",
      "Gradient Descent(776/999): loss=0.40524235347924403, w = [-1.53945485  0.24817375 -0.50332537 -1.24073391 -0.08028113  0.9785701\n",
      "  0.79586457 -0.4024632   0.07034239  0.32014764 -0.00375984 -0.13474083\n",
      "  0.02265403 -0.04519197  0.0981713 ]\n",
      "Gradient Descent(777/999): loss=0.40522601265313474, w = [-1.53956258  0.24802985 -0.50328448 -1.24166084 -0.08019763  0.97925064\n",
      "  0.79609565 -0.40274498  0.07028566  0.32016326 -0.00375711 -0.13438387\n",
      "  0.02265312 -0.04510422  0.09816773]\n",
      "Gradient Descent(778/999): loss=0.4052096958762557, w = [-1.53967021  0.24788592 -0.50324359 -1.24258703 -0.08011416  0.97993062\n",
      "  0.79632653 -0.40302673  0.07022898  0.32017874 -0.00375438 -0.13402708\n",
      "  0.02265221 -0.04501664  0.09816414]\n",
      "Gradient Descent(779/999): loss=0.40519340310053886, w = [-1.53977775  0.24774198 -0.50320271 -1.24351248 -0.08003073  0.98061005\n",
      "  0.79655723 -0.40330843  0.07017236  0.32019408 -0.00375165 -0.13367046\n",
      "  0.0226513  -0.04492921  0.09816055]\n",
      "Gradient Descent(780/999): loss=0.40517713427805346, w = [-1.53988519  0.24759801 -0.50316182 -1.24443721 -0.07994733  0.98128893\n",
      "  0.79678773 -0.4035901   0.07011578  0.3202093  -0.00374891 -0.133314\n",
      "  0.02265038 -0.04484194  0.09815695]\n",
      "Gradient Descent(781/999): loss=0.4051608893610013, w = [-1.53999253  0.24745401 -0.50312093 -1.2453612  -0.07986396  0.98196725\n",
      "  0.79701805 -0.40387172  0.07005925  0.32022438 -0.00374617 -0.13295771\n",
      "  0.02264945 -0.04475483  0.09815333]\n",
      "Gradient Descent(782/999): loss=0.40514466830172, w = [-1.54009977  0.24731    -0.50308005 -1.24628445 -0.07978063  0.98264501\n",
      "  0.79724818 -0.40415331  0.07000277  0.32023933 -0.00374343 -0.13260158\n",
      "  0.02264852 -0.04466787  0.09814971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(783/999): loss=0.4051284710526821, w = [-1.54020692  0.24716597 -0.50303916 -1.24720698 -0.07969734  0.98332223\n",
      "  0.79747813 -0.40443485  0.06994634  0.32025415 -0.00374068 -0.13224562\n",
      "  0.02264759 -0.04458108  0.09814608]\n",
      "Gradient Descent(784/999): loss=0.4051122975664903, w = [-1.54031397  0.24702191 -0.50299827 -1.24812878 -0.07961407  0.98399889\n",
      "  0.79770789 -0.40471635  0.06988996  0.32026884 -0.00373793 -0.13188982\n",
      "  0.02264665 -0.04449444  0.09814243]\n",
      "Gradient Descent(785/999): loss=0.40509614779588543, w = [-1.54042093  0.24687784 -0.50295739 -1.24904985 -0.07953085  0.984675\n",
      "  0.79793746 -0.40499781  0.06983363  0.32028339 -0.00373518 -0.13153419\n",
      "  0.02264571 -0.04440797  0.09813878]\n",
      "Gradient Descent(786/999): loss=0.4050800216937349, w = [-1.54052779  0.24673375 -0.5029165  -1.24997019 -0.07944765  0.98535056\n",
      "  0.79816684 -0.40527923  0.06977735  0.32029782 -0.00373243 -0.13117872\n",
      "  0.02264476 -0.04432165  0.09813512]\n",
      "Gradient Descent(787/999): loss=0.40506391921304347, w = [-1.54063456  0.24658964 -0.50287562 -1.2508898  -0.07936449  0.98602556\n",
      "  0.79839604 -0.40556061  0.06972112  0.32031211 -0.00372967 -0.13082342\n",
      "  0.0226438  -0.04423549  0.09813145]\n",
      "Gradient Descent(788/999): loss=0.4050478403069419, w = [-1.54074124  0.24644551 -0.50283473 -1.25180869 -0.07928137  0.98670002\n",
      "  0.79862505 -0.40584195  0.06966494  0.32032628 -0.00372691 -0.13046828\n",
      "  0.02264285 -0.04414949  0.09812778]\n",
      "Gradient Descent(789/999): loss=0.40503178492869674, w = [-1.54084782  0.24630136 -0.50279385 -1.25272685 -0.07919828  0.98737393\n",
      "  0.79885388 -0.40612325  0.06960881  0.32034031 -0.00372414 -0.1301133\n",
      "  0.02264189 -0.04406364  0.09812409]\n",
      "Gradient Descent(790/999): loss=0.4050157530317009, w = [-1.54095431  0.2461572  -0.50275297 -1.25364429 -0.07911523  0.98804729\n",
      "  0.79908252 -0.4064045   0.06955273  0.32035422 -0.00372138 -0.12975849\n",
      "  0.02264092 -0.04397795  0.09812039]\n",
      "Gradient Descent(791/999): loss=0.4049997445694793, w = [-1.54106071  0.24601301 -0.50271209 -1.254561   -0.07903221  0.9887201\n",
      "  0.79931098 -0.40668571  0.06949669  0.32036799 -0.00371861 -0.12940384\n",
      "  0.02263995 -0.04389242  0.09811669]\n",
      "Gradient Descent(792/999): loss=0.40498375949568516, w = [-1.54116702  0.24586882 -0.50267122 -1.255477   -0.07894923  0.98939237\n",
      "  0.79953925 -0.40696688  0.06944071  0.32038164 -0.00371584 -0.12904936\n",
      "  0.02263897 -0.04380705  0.09811297]\n",
      "Gradient Descent(793/999): loss=0.40496779776410063, w = [-1.54127324  0.2457246  -0.50263034 -1.25639227 -0.07886628  0.99006409\n",
      "  0.79976734 -0.40724801  0.06938478  0.32039516 -0.00371306 -0.12869504\n",
      "  0.02263799 -0.04372183  0.09810925]\n",
      "Gradient Descent(794/999): loss=0.40495185932863614, w = [-1.54137937  0.24558037 -0.50258947 -1.25730682 -0.07878337  0.99073526\n",
      "  0.79999524 -0.4075291   0.06932889  0.32040855 -0.00371029 -0.12834088\n",
      "  0.02263701 -0.04363677  0.09810552]\n",
      "Gradient Descent(795/999): loss=0.40493594414333006, w = [-1.54148541  0.24543612 -0.5025486  -1.25822066 -0.0787005   0.99140588\n",
      "  0.80022296 -0.40781014  0.06927306  0.32042181 -0.0037075  -0.12798689\n",
      "  0.02263602 -0.04355187  0.09810179]\n",
      "Gradient Descent(796/999): loss=0.40492005216234667, w = [-1.54159136  0.24529186 -0.50250774 -1.25913377 -0.07861766  0.99207596\n",
      "  0.8004505  -0.40809114  0.06921727  0.32043495 -0.00370472 -0.12763306\n",
      "  0.02263503 -0.04346712  0.09809804]\n",
      "Gradient Descent(797/999): loss=0.4049041833399796, w = [-1.54169722  0.24514758 -0.50246687 -1.26004617 -0.07853485  0.9927455\n",
      "  0.80067785 -0.4083721   0.06916154  0.32044796 -0.00370194 -0.12727939\n",
      "  0.02263403 -0.04338253  0.09809429]\n",
      "Gradient Descent(798/999): loss=0.40488833763064597, w = [-1.541803    0.24500329 -0.50242601 -1.26095785 -0.07845209  0.99341449\n",
      "  0.80090502 -0.40865302  0.06910585  0.32046084 -0.00369915 -0.12692589\n",
      "  0.02263303 -0.04329809  0.09809053]\n",
      "Gradient Descent(799/999): loss=0.40487251498889093, w = [-1.54190869  0.24485898 -0.50238516 -1.26186882 -0.07836936  0.99408294\n",
      "  0.80113201 -0.40893389  0.06905021  0.32047359 -0.00369636 -0.12657255\n",
      "  0.02263203 -0.04321381  0.09808676]\n",
      "Gradient Descent(800/999): loss=0.4048567153693825, w = [-1.54201428  0.24471466 -0.50234431 -1.26277907 -0.07828667  0.99475085\n",
      "  0.80135882 -0.40921473  0.06899462  0.32048622 -0.00369356 -0.12621937\n",
      "  0.02263102 -0.04312969  0.09808299]\n",
      "Gradient Descent(801/999): loss=0.40484093872691407, w = [-1.5421198   0.24457033 -0.50230346 -1.26368861 -0.07820401  0.99541821\n",
      "  0.80158544 -0.40949551  0.06893908  0.32049872 -0.00369077 -0.12586635\n",
      "  0.02263    -0.04304572  0.0980792 ]\n",
      "Gradient Descent(802/999): loss=0.4048251850164066, w = [-1.54222522  0.24442599 -0.50226261 -1.26459744 -0.07812139  0.99608503\n",
      "  0.80181188 -0.40977626  0.06888359  0.3205111  -0.00368797 -0.1255135\n",
      "  0.02262899 -0.04296191  0.09807541]\n",
      "Gradient Descent(803/999): loss=0.40480945419289943, w = [-1.54233057  0.24428163 -0.50222177 -1.26550556 -0.07803881  0.99675131\n",
      "  0.80203814 -0.41005696  0.06882815  0.32052335 -0.00368517 -0.12516081\n",
      "  0.02262796 -0.04287825  0.09807162]\n",
      "Gradient Descent(804/999): loss=0.4047937462115578, w = [-1.54243582  0.24413726 -0.50218093 -1.26641297 -0.07795626  0.99741705\n",
      "  0.80226422 -0.41033762  0.06877275  0.32053548 -0.00368236 -0.12480828\n",
      "  0.02262694 -0.04279475  0.09806781]\n",
      "Gradient Descent(805/999): loss=0.40477806102767094, w = [-1.54254099  0.24399288 -0.5021401  -1.26731966 -0.07787376  0.99808225\n",
      "  0.80249012 -0.41061823  0.06871741  0.32054748 -0.00367956 -0.12445591\n",
      "  0.02262591 -0.0427114   0.098064  ]\n",
      "Gradient Descent(806/999): loss=0.4047623985966489, w = [-1.54264608  0.24384848 -0.50209927 -1.26822565 -0.07779129  0.99874692\n",
      "  0.80271584 -0.4108988   0.06866211  0.32055936 -0.00367675 -0.12410371\n",
      "  0.02262487 -0.04262821  0.09806018]\n",
      "Gradient Descent(807/999): loss=0.404746758874024, w = [-1.54275108  0.24370408 -0.50205845 -1.26913093 -0.07770885  0.99941104\n",
      "  0.80294137 -0.41117933  0.06860687  0.32057111 -0.00367394 -0.12375166\n",
      "  0.02262384 -0.04254517  0.09805636]\n",
      "Gradient Descent(808/999): loss=0.40473114181545056, w = [-1.542856    0.24355967 -0.50201763 -1.27003551 -0.07762646  1.00007462\n",
      "  0.80316673 -0.41145981  0.06855167  0.32058274 -0.00367113 -0.12339978\n",
      "  0.02262279 -0.04246228  0.09805253]\n",
      "Gradient Descent(809/999): loss=0.4047155473767015, w = [-1.54296084  0.24341524 -0.50197682 -1.27093938 -0.0775441   1.00073767\n",
      "  0.80339191 -0.41174025  0.06849652  0.32059425 -0.00366831 -0.12304806\n",
      "  0.02262175 -0.04237955  0.09804869]\n",
      "Gradient Descent(810/999): loss=0.4046999755136728, w = [-1.54306559  0.24327081 -0.50193601 -1.27184254 -0.07746178  1.00140018\n",
      "  0.80361691 -0.41202065  0.06844142  0.32060563 -0.00366549 -0.1226965\n",
      "  0.0226207  -0.04229697  0.09804485]\n",
      "Gradient Descent(811/999): loss=0.4046844261823795, w = [-1.54317026  0.24312636 -0.50189521 -1.272745   -0.0773795   1.00206215\n",
      "  0.80384172 -0.412301    0.06838636  0.32061689 -0.00366267 -0.1223451\n",
      "  0.02261965 -0.04221455  0.098041  ]\n",
      "Gradient Descent(812/999): loss=0.4046688993389565, w = [-1.54327485  0.24298191 -0.50185442 -1.27364676 -0.07729726  1.00272359\n",
      "  0.80406636 -0.4125813   0.06833136  0.32062802 -0.00365985 -0.12199387\n",
      "  0.02261859 -0.04213227  0.09803714]\n",
      "Gradient Descent(813/999): loss=0.4046533949396576, w = [-1.54337936  0.24283745 -0.50181362 -1.27454782 -0.07721506  1.00338449\n",
      "  0.80429082 -0.41286156  0.0682764   0.32063904 -0.00365702 -0.12164279\n",
      "  0.02261753 -0.04205015  0.09803328]\n",
      "Gradient Descent(814/999): loss=0.40463791294085383, w = [-1.54348379  0.24269298 -0.50177284 -1.27544818 -0.07713289  1.00404486\n",
      "  0.80451511 -0.41314178  0.0682215   0.32064993 -0.00365419 -0.12129188\n",
      "  0.02261646 -0.04196819  0.09802942]\n",
      "Gradient Descent(815/999): loss=0.40462245329903734, w = [-1.54358814  0.2425485  -0.50173206 -1.27634783 -0.07705076  1.00470469\n",
      "  0.80473921 -0.41342195  0.06816664  0.3206607  -0.00365136 -0.12094112\n",
      "  0.02261539 -0.04188638  0.09802554]\n",
      "Gradient Descent(816/999): loss=0.40460701597081583, w = [-1.54369241  0.24240401 -0.50169129 -1.27724679 -0.07696867  1.00536399\n",
      "  0.80496314 -0.41370208  0.06811183  0.32067135 -0.00364853 -0.12059053\n",
      "  0.02261432 -0.04180472  0.09802166]\n",
      "Gradient Descent(817/999): loss=0.4045916009129151, w = [-1.5437966   0.24225952 -0.50165053 -1.27814505 -0.07688663  1.00602276\n",
      "  0.80518688 -0.41398216  0.06805707  0.32068188 -0.0036457  -0.1202401\n",
      "  0.02261324 -0.04172321  0.09801778]\n",
      "Gradient Descent(818/999): loss=0.40457620808217676, w = [-1.54390071  0.24211502 -0.50160977 -1.27904261 -0.07680461  1.00668099\n",
      "  0.80541046 -0.4142622   0.06800235  0.32069229 -0.00364286 -0.11988982\n",
      "  0.02261216 -0.04164185  0.09801389]\n",
      "Gradient Descent(819/999): loss=0.4045608374355603, w = [-1.54400474  0.24197051 -0.50156901 -1.27993948 -0.07672264  1.00733869\n",
      "  0.80563385 -0.41454219  0.06794769  0.32070257 -0.00364002 -0.11953971\n",
      "  0.02261108 -0.04156065  0.09801   ]\n",
      "Gradient Descent(820/999): loss=0.4045454889301397, w = [-1.54410869  0.241826   -0.50152827 -1.28083565 -0.07664071  1.00799587\n",
      "  0.80585707 -0.41482214  0.06789307  0.32071274 -0.00363718 -0.11918976\n",
      "  0.02260999 -0.04147959  0.0980061 ]\n",
      "Gradient Descent(821/999): loss=0.40453016252310714, w = [-1.54421256  0.24168148 -0.50148753 -1.28173113 -0.07655882  1.00865251\n",
      "  0.80608011 -0.41510204  0.0678385   0.32072279 -0.00363434 -0.11883996\n",
      "  0.0226089  -0.04139869  0.09800219]\n",
      "Gradient Descent(822/999): loss=0.40451485817176575, w = [-1.54431636  0.24153696 -0.5014468  -1.28262591 -0.07647696  1.00930862\n",
      "  0.80630297 -0.41538189  0.06778398  0.32073271 -0.00363149 -0.11849033\n",
      "  0.02260781 -0.04131794  0.09799828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(823/999): loss=0.40449957583353713, w = [-1.54442008  0.24139243 -0.50140608 -1.28352    -0.07639515  1.0099642\n",
      "  0.80652566 -0.4156617   0.06772951  0.32074252 -0.00362864 -0.11814086\n",
      "  0.02260671 -0.04123734  0.09799436]\n",
      "Gradient Descent(824/999): loss=0.40448431546595504, w = [-1.54452372  0.24124789 -0.50136536 -1.2844134  -0.07631337  1.01061925\n",
      "  0.80674817 -0.41594146  0.06767508  0.32075221 -0.0036258  -0.11779154\n",
      "  0.0226056  -0.04115689  0.09799044]\n",
      "Gradient Descent(825/999): loss=0.40446907702666757, w = [-1.54462729  0.24110335 -0.50132465 -1.28530611 -0.07623164  1.01127377\n",
      "  0.80697051 -0.41622118  0.0676207   0.32076178 -0.00362294 -0.11744239\n",
      "  0.0226045  -0.0410766   0.09798652]\n",
      "Gradient Descent(826/999): loss=0.40445386047343684, w = [-1.54473078  0.24095881 -0.50128395 -1.28619813 -0.07614994  1.01192777\n",
      "  0.80719267 -0.41650085  0.06756638  0.32077123 -0.00362009 -0.11709339\n",
      "  0.02260339 -0.04099645  0.09798259]\n",
      "Gradient Descent(827/999): loss=0.40443866576413756, w = [-1.54483419  0.24081427 -0.50124326 -1.28708946 -0.07606829  1.01258124\n",
      "  0.80741466 -0.41678048  0.06751209  0.32078056 -0.00361723 -0.11674456\n",
      "  0.02260228 -0.04091645  0.09797866]\n",
      "Gradient Descent(828/999): loss=0.4044234928567549, w = [-1.54493753  0.24066972 -0.50120258 -1.28798011 -0.07598667  1.01323418\n",
      "  0.80763647 -0.41706006  0.06745786  0.32078978 -0.00361438 -0.11639588\n",
      "  0.02260116 -0.0408366   0.09797472]\n",
      "Gradient Descent(829/999): loss=0.4044083417093907, w = [-1.5450408   0.24052516 -0.5011619  -1.28887007 -0.0759051   1.01388659\n",
      "  0.80785811 -0.41733959  0.06740368  0.32079887 -0.00361152 -0.11604736\n",
      "  0.02260004 -0.04075691  0.09797078]\n",
      "Gradient Descent(830/999): loss=0.4043932122802536, w = [-1.54514399  0.24038061 -0.50112124 -1.28975934 -0.07582357  1.01453848\n",
      "  0.80807957 -0.41761907  0.06734954  0.32080785 -0.00360865 -0.11569901\n",
      "  0.02259892 -0.04067736  0.09796683]\n",
      "Gradient Descent(831/999): loss=0.4043781045276669, w = [-1.54524711  0.24023605 -0.50108058 -1.29064792 -0.07574207  1.01518985\n",
      "  0.80830086 -0.41789851  0.06729545  0.32081672 -0.00360579 -0.1153508\n",
      "  0.02259779 -0.04059796  0.09796288]\n",
      "Gradient Descent(832/999): loss=0.40436301841006395, w = [-1.54535015  0.24009149 -0.50103993 -1.29153582 -0.07566062  1.01584069\n",
      "  0.80852198 -0.4181779   0.0672414   0.32082546 -0.00360293 -0.11500276\n",
      "  0.02259666 -0.04051871  0.09795892]\n",
      "Gradient Descent(833/999): loss=0.4043479538859875, w = [-1.54545312  0.23994693 -0.50099929 -1.29242304 -0.0755792   1.016491\n",
      "  0.80874292 -0.41845725  0.06718741  0.32083409 -0.00360006 -0.11465488\n",
      "  0.02259553 -0.04043961  0.09795496]\n",
      "Gradient Descent(834/999): loss=0.4043329109140916, w = [-1.54555601  0.23980237 -0.50095865 -1.29330958 -0.07549783  1.0171408\n",
      "  0.80896369 -0.41873655  0.06713346  0.3208426  -0.00359719 -0.11430715\n",
      "  0.02259439 -0.04036066  0.097951  ]\n",
      "Gradient Descent(835/999): loss=0.4043178894531395, w = [-1.54565884  0.2396578  -0.50091803 -1.29419543 -0.0754165   1.01779007\n",
      "  0.80918429 -0.4190158   0.06707956  0.320851   -0.00359432 -0.11395959\n",
      "  0.02259325 -0.04028185  0.09794703]\n",
      "Gradient Descent(836/999): loss=0.40430288946200404, w = [-1.54576159  0.23951324 -0.50087742 -1.29508061 -0.07533521  1.01843881\n",
      "  0.80940471 -0.419295    0.06702571  0.32085928 -0.00359144 -0.11361218\n",
      "  0.02259211 -0.0402032   0.09794306]\n",
      "Gradient Descent(837/999): loss=0.40428791089966604, w = [-1.54586427  0.23936868 -0.50083681 -1.2959651  -0.07525396  1.01908704\n",
      "  0.80962496 -0.41957415  0.0669719   0.32086745 -0.00358857 -0.11326493\n",
      "  0.02259096 -0.04012469  0.09793909]\n",
      "Gradient Descent(838/999): loss=0.4042729537252165, w = [-1.54596688  0.23922411 -0.50079622 -1.29684891 -0.07517275  1.01973474\n",
      "  0.80984504 -0.41985326  0.06691815  0.3208755  -0.00358569 -0.11291784\n",
      "  0.02258981 -0.04004633  0.09793511]\n",
      "Gradient Descent(839/999): loss=0.4042580178978523, w = [-1.54606941  0.23907955 -0.50075563 -1.29773205 -0.07509158  1.02038193\n",
      "  0.81006495 -0.42013232  0.06686444  0.32088344 -0.00358281 -0.1125709\n",
      "  0.02258866 -0.03996812  0.09793113]\n",
      "Gradient Descent(840/999): loss=0.40424310337687974, w = [-1.54617188  0.23893498 -0.50071506 -1.29861451 -0.07501046  1.02102859\n",
      "  0.81028468 -0.42041134  0.06681077  0.32089126 -0.00357993 -0.11222412\n",
      "  0.0225875  -0.03989006  0.09792715]\n",
      "Gradient Descent(841/999): loss=0.404228210121711, w = [-1.54627428  0.23879042 -0.50067449 -1.2994963  -0.07492937  1.02167474\n",
      "  0.81050425 -0.4206903   0.06675716  0.32089896 -0.00357705 -0.1118775\n",
      "  0.02258634 -0.03981214  0.09792316]\n",
      "Gradient Descent(842/999): loss=0.4042133380918675, w = [-1.5463766   0.23864586 -0.50063394 -1.30037741 -0.07484833  1.02232036\n",
      "  0.81072364 -0.42096922  0.06670359  0.32090656 -0.00357416 -0.11153104\n",
      "  0.02258518 -0.03973437  0.09791917]\n",
      "Gradient Descent(843/999): loss=0.4041984872469742, w = [-1.54647886  0.2385013  -0.50059339 -1.30125784 -0.07476733  1.02296547\n",
      "  0.81094286 -0.42124809  0.06665007  0.32091404 -0.00357128 -0.11118473\n",
      "  0.02258402 -0.03965674  0.09791518]\n",
      "Gradient Descent(844/999): loss=0.40418365754676466, w = [-1.54658104  0.23835674 -0.50055286 -1.3021376  -0.07468637  1.02361006\n",
      "  0.81116192 -0.42152691  0.06659659  0.3209214  -0.00356839 -0.11083859\n",
      "  0.02258285 -0.03957927  0.09791118]\n",
      "Gradient Descent(845/999): loss=0.40416884895107563, w = [-1.54668316  0.23821219 -0.50051233 -1.30301669 -0.07460545  1.02425414\n",
      "  0.8113808  -0.42180568  0.06654316  0.32092865 -0.0035655  -0.11049259\n",
      "  0.02258168 -0.03950194  0.09790718]\n",
      "Gradient Descent(846/999): loss=0.4041540614198524, w = [-1.54678521  0.23806763 -0.50047182 -1.30389511 -0.07452458  1.02489769\n",
      "  0.81159951 -0.4220844   0.06648978  0.32093579 -0.00356261 -0.11014676\n",
      "  0.0225805  -0.03942475  0.09790318]\n",
      "Gradient Descent(847/999): loss=0.4041392949131425, w = [-1.54688718  0.23792308 -0.50043132 -1.30477286 -0.07444374  1.02554073\n",
      "  0.81181805 -0.42236308  0.06643645  0.32094282 -0.00355972 -0.10980108\n",
      "  0.02257932 -0.03934771  0.09789917]\n",
      "Gradient Descent(848/999): loss=0.40412454939109915, w = [-1.54698909  0.23777854 -0.50039082 -1.30564993 -0.07436295  1.02618326\n",
      "  0.81203642 -0.4226417   0.06638316  0.32094974 -0.00355682 -0.10945556\n",
      "  0.02257814 -0.03927082  0.09789517]\n",
      "Gradient Descent(849/999): loss=0.40410982481398067, w = [-1.54709094  0.23763399 -0.50035034 -1.30652634 -0.0742822   1.02682526\n",
      "  0.81225462 -0.42292028  0.06632992  0.32095654 -0.00355393 -0.1091102\n",
      "  0.02257696 -0.03919407  0.09789116]\n",
      "Gradient Descent(850/999): loss=0.404095121142147, w = [-1.54719271  0.23748945 -0.50030987 -1.30740208 -0.07420149  1.02746676\n",
      "  0.81247266 -0.42319881  0.06627673  0.32096323 -0.00355103 -0.10876499\n",
      "  0.02257577 -0.03911747  0.09788714]\n",
      "Gradient Descent(851/999): loss=0.40408043833606366, w = [-1.54729442  0.23734492 -0.50026941 -1.30827715 -0.07412083  1.02810774\n",
      "  0.81269052 -0.42347729  0.06622359  0.32096981 -0.00354813 -0.10841994\n",
      "  0.02257458 -0.03904101  0.09788313]\n",
      "Gradient Descent(852/999): loss=0.40406577635629887, w = [-1.54739606  0.23720039 -0.50022896 -1.30915156 -0.0740402   1.02874821\n",
      "  0.81290822 -0.42375572  0.06617049  0.32097628 -0.00354523 -0.10807504\n",
      "  0.02257339 -0.0389647   0.09787911]\n",
      "Gradient Descent(853/999): loss=0.404051135163522, w = [-1.54749763  0.23705586 -0.50018853 -1.3100253  -0.07395962  1.02938816\n",
      "  0.81312574 -0.4240341   0.06611743  0.32098263 -0.00354233 -0.1077303\n",
      "  0.02257219 -0.03888853  0.09787509]\n",
      "Gradient Descent(854/999): loss=0.40403651471850704, w = [-1.54759914  0.23691134 -0.5001481  -1.31089837 -0.07387908  1.03002761\n",
      "  0.8133431  -0.42431243  0.06606443  0.32098888 -0.00353942 -0.10738572\n",
      "  0.02257099 -0.03881251  0.09787107]\n",
      "Gradient Descent(855/999): loss=0.4040219149821279, w = [-1.54770058  0.23676683 -0.50010769 -1.31177078 -0.07379859  1.03066654\n",
      "  0.81356029 -0.42459071  0.06601147  0.32099501 -0.00353652 -0.10704129\n",
      "  0.02256979 -0.03873663  0.09786705]\n",
      "Gradient Descent(856/999): loss=0.4040073359153629, w = [-1.54780196  0.23662232 -0.50006729 -1.31264253 -0.07371814  1.03130496\n",
      "  0.81377732 -0.42486894  0.06595856  0.32100104 -0.00353361 -0.10669702\n",
      "  0.02256858 -0.03866089  0.09786302]\n",
      "Gradient Descent(857/999): loss=0.40399277747928847, w = [-1.54790327  0.23647781 -0.5000269  -1.31351362 -0.07363773  1.03194286\n",
      "  0.81399417 -0.42514712  0.06590569  0.32100696 -0.0035307  -0.1063529\n",
      "  0.02256738 -0.0385853   0.09785899]\n",
      "Gradient Descent(858/999): loss=0.4039782396350839, w = [-1.54800451  0.23633332 -0.49998652 -1.31438404 -0.07355736  1.03258026\n",
      "  0.81421086 -0.42542526  0.06585287  0.32101276 -0.00352779 -0.10600894\n",
      "  0.02256617 -0.03850985  0.09785497]\n",
      "Gradient Descent(859/999): loss=0.4039637223440287, w = [-1.54810569  0.23618883 -0.49994615 -1.31525381 -0.07347704  1.03321715\n",
      "  0.81442738 -0.42570334  0.0658001   0.32101846 -0.00352488 -0.10566514\n",
      "  0.02256495 -0.03843454  0.09785093]\n",
      "Gradient Descent(860/999): loss=0.40394922556750185, w = [-1.54820681  0.23604434 -0.4999058  -1.31612291 -0.07339676  1.03385353\n",
      "  0.81464374 -0.42598137  0.06574737  0.32102404 -0.00352197 -0.10532149\n",
      "  0.02256374 -0.03835938  0.0978469 ]\n",
      "Gradient Descent(861/999): loss=0.40393474926698375, w = [-1.54830786  0.23589987 -0.49986546 -1.31699136 -0.07331652  1.03448941\n",
      "  0.81485993 -0.42625935  0.06569469  0.32102952 -0.00351905 -0.10497799\n",
      "  0.02256252 -0.03828436  0.09784287]\n",
      "Gradient Descent(862/999): loss=0.40392029340405305, w = [-1.54840885  0.2357554  -0.49982513 -1.31785914 -0.07323632  1.03512477\n",
      "  0.81507595 -0.42653729  0.06564206  0.32103489 -0.00351614 -0.10463465\n",
      "  0.0225613  -0.03820948  0.09783883]\n",
      "Gradient Descent(863/999): loss=0.40390585794038725, w = [-1.54850977  0.23561094 -0.49978481 -1.31872628 -0.07315617  1.03575963\n",
      "  0.81529181 -0.42681517  0.06558947  0.32104015 -0.00351322 -0.10429147\n",
      "  0.02256007 -0.03813474  0.0978348 ]\n",
      "Gradient Descent(864/999): loss=0.40389144283776346, w = [-1.54861063  0.23546648 -0.49974451 -1.31959275 -0.07307607  1.03639398\n",
      "  0.8155075  -0.427093    0.06553693  0.3210453  -0.0035103  -0.10394844\n",
      "  0.02255884 -0.03806015  0.09783076]\n",
      "Gradient Descent(865/999): loss=0.4038770480580577, w = [-1.54871143  0.23532204 -0.49970422 -1.32045857 -0.072996    1.03702782\n",
      "  0.81572302 -0.42737078  0.06548443  0.32105035 -0.00350738 -0.10360556\n",
      "  0.02255761 -0.0379857   0.09782672]\n",
      "Gradient Descent(866/999): loss=0.4038626735632431, w = [-1.54881216  0.2351776  -0.49966394 -1.32132373 -0.07291598  1.03766116\n",
      "  0.81593838 -0.42764851  0.06543198  0.32105529 -0.00350446 -0.10326284\n",
      "  0.02255638 -0.03791138  0.09782268]\n",
      "Gradient Descent(867/999): loss=0.4038483193153906, w = [-1.54891283  0.23503318 -0.49962367 -1.32218825 -0.072836    1.03829399\n",
      "  0.81615358 -0.42792619  0.06537958  0.32106012 -0.00350153 -0.10292028\n",
      "  0.02255515 -0.03783722  0.09781863]\n",
      "Gradient Descent(868/999): loss=0.4038339852766691, w = [-1.54901344  0.23488876 -0.49958342 -1.3230521  -0.07275607  1.03892632\n",
      "  0.81636861 -0.42820381  0.06532722  0.32106484 -0.00349861 -0.10257787\n",
      "  0.02255391 -0.03776319  0.09781459]\n",
      "Gradient Descent(869/999): loss=0.40381967140934466, w = [-1.54911399  0.23474435 -0.49954318 -1.32391531 -0.07267618  1.03955814\n",
      "  0.81658347 -0.42848139  0.06527491  0.32106946 -0.00349569 -0.10223561\n",
      "  0.02255267 -0.0376893   0.09781055]\n",
      "Gradient Descent(870/999): loss=0.4038053776757793, w = [-1.54921447  0.23459995 -0.49950296 -1.32477787 -0.07259634  1.04018946\n",
      "  0.81679817 -0.42875891  0.06522264  0.32107396 -0.00349276 -0.10189351\n",
      "  0.02255142 -0.03761555  0.0978065 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(871/999): loss=0.4037911040384321, w = [-1.54931489  0.23445556 -0.49946274 -1.32563977 -0.07251654  1.04082028\n",
      "  0.81701271 -0.42903639  0.06517042  0.32107837 -0.00348983 -0.10155156\n",
      "  0.02255018 -0.03754195  0.09780246]\n",
      "Gradient Descent(872/999): loss=0.4037768504598585, w = [-1.54941526  0.23431119 -0.49942254 -1.32650103 -0.07243678  1.04145059\n",
      "  0.81722708 -0.42931381  0.06511825  0.32108266 -0.0034869  -0.10120976\n",
      "  0.02254893 -0.03746848  0.09779841]\n",
      "Gradient Descent(873/999): loss=0.40376261690270837, w = [-1.54951556  0.23416682 -0.49938236 -1.32736163 -0.07235707  1.04208041\n",
      "  0.81744129 -0.42959118  0.06506612  0.32108686 -0.00348397 -0.10086812\n",
      "  0.02254768 -0.03739515  0.09779436]\n",
      "Gradient Descent(874/999): loss=0.4037484033297288, w = [-1.5496158   0.23402246 -0.49934219 -1.32822159 -0.0722774   1.04270972\n",
      "  0.81765534 -0.4298685   0.06501404  0.32109094 -0.00348104 -0.10052663\n",
      "  0.02254643 -0.03732197  0.09779031]\n",
      "Gradient Descent(875/999): loss=0.40373420970376, w = [-1.54971598  0.23387812 -0.49930203 -1.3290809  -0.07219777  1.04333853\n",
      "  0.81786922 -0.43014577  0.064962    0.32109492 -0.0034781  -0.1001853\n",
      "  0.02254517 -0.03724892  0.09778627]\n",
      "Gradient Descent(876/999): loss=0.4037200359877389, w = [-1.5498161   0.23373379 -0.49926188 -1.32993957 -0.07211819  1.04396684\n",
      "  0.81808294 -0.43042298  0.06491001  0.3210988  -0.00347517 -0.09984412\n",
      "  0.02254391 -0.03717601  0.09778222]\n",
      "Gradient Descent(877/999): loss=0.40370588214469577, w = [-1.54991615  0.23358947 -0.49922175 -1.33079759 -0.07203865  1.04459465\n",
      "  0.8182965  -0.43070015  0.06485807  0.32110257 -0.00347223 -0.09950309\n",
      "  0.02254265 -0.03710324  0.09777817]\n",
      "Gradient Descent(878/999): loss=0.4036917481377543, w = [-1.55001615  0.23344516 -0.49918163 -1.33165496 -0.07195916  1.04522196\n",
      "  0.81850989 -0.43097726  0.06480617  0.32110623 -0.0034693  -0.09916222\n",
      "  0.02254139 -0.03703061  0.09777412]\n",
      "Gradient Descent(879/999): loss=0.40367763393013334, w = [-1.55011609  0.23330086 -0.49914153 -1.3325117  -0.07187972  1.04584877\n",
      "  0.81872312 -0.43125432  0.06475431  0.32110979 -0.00346636 -0.0988215\n",
      "  0.02254012 -0.03695812  0.09777007]\n",
      "Gradient Descent(880/999): loss=0.4036635394851448, w = [-1.55021597  0.23315658 -0.49910144 -1.33336778 -0.07180031  1.04647508\n",
      "  0.81893619 -0.43153133  0.0647025   0.32111325 -0.00346342 -0.09848093\n",
      "  0.02253885 -0.03688577  0.09776602]\n",
      "Gradient Descent(881/999): loss=0.4036494647661929, w = [-1.55031579  0.23301231 -0.49906137 -1.33422323 -0.07172095  1.0471009\n",
      "  0.8191491  -0.43180828  0.06465074  0.3211166  -0.00346048 -0.09814052\n",
      "  0.02253758 -0.03681355  0.09776197]\n",
      "Gradient Descent(882/999): loss=0.40363540973677564, w = [-1.55041556  0.23286805 -0.49902131 -1.33507804 -0.07164164  1.04772622\n",
      "  0.81936185 -0.43208518  0.06459902  0.32111985 -0.00345754 -0.09780026\n",
      "  0.02253631 -0.03674148  0.09775791]\n",
      "Gradient Descent(883/999): loss=0.4036213743604832, w = [-1.55051526  0.2327238  -0.49898126 -1.3359322  -0.07156237  1.04835104\n",
      "  0.81957443 -0.43236203  0.06454735  0.321123   -0.0034546  -0.09746015\n",
      "  0.02253503 -0.03666954  0.09775386]\n",
      "Gradient Descent(884/999): loss=0.4036073586009973, w = [-1.55061491  0.23257957 -0.49894123 -1.33678573 -0.07148315  1.04897537\n",
      "  0.81978686 -0.43263883  0.06449572  0.32112604 -0.00345165 -0.0971202\n",
      "  0.02253375 -0.03659773  0.09774981]\n",
      "Gradient Descent(885/999): loss=0.40359336242209326, w = [-1.55071449  0.23243536 -0.49890122 -1.33763861 -0.07140397  1.0495992\n",
      "  0.81999912 -0.43291557  0.06444414  0.32112898 -0.00344871 -0.09678039\n",
      "  0.02253247 -0.03652607  0.09774576]\n",
      "Gradient Descent(886/999): loss=0.4035793857876353, w = [-1.55081402  0.23229116 -0.49886121 -1.33849086 -0.07132484  1.05022253\n",
      "  0.82021122 -0.43319227  0.0643926   0.32113182 -0.00344576 -0.09644074\n",
      "  0.02253119 -0.03645454  0.09774171]\n",
      "Gradient Descent(887/999): loss=0.4035654286615818, w = [-1.55091349  0.23214697 -0.49882123 -1.33934247 -0.07124575  1.05084537\n",
      "  0.82042316 -0.4334689   0.06434111  0.32113456 -0.00344281 -0.09610124\n",
      "  0.0225299  -0.03638315  0.09773766]\n",
      "Gradient Descent(888/999): loss=0.40355149100797955, w = [-1.55101291  0.2320028  -0.49878125 -1.34019345 -0.0711667   1.05146772\n",
      "  0.82063495 -0.43374549  0.06428967  0.32113719 -0.00343987 -0.0957619\n",
      "  0.02252862 -0.03631189  0.09773361]\n",
      "Gradient Descent(889/999): loss=0.4035375727909671, w = [-1.55111227  0.23185864 -0.4987413  -1.34104379 -0.07108771  1.05208957\n",
      "  0.82084657 -0.43402202  0.06423826  0.32113972 -0.00343692 -0.09542271\n",
      "  0.02252733 -0.03624077  0.09772956]\n",
      "Gradient Descent(890/999): loss=0.4035236739747741, w = [-1.55121157  0.2317145  -0.49870136 -1.34189349 -0.07100875  1.05271093\n",
      "  0.82105803 -0.4342985   0.06418691  0.32114215 -0.00343397 -0.09508366\n",
      "  0.02252604 -0.03616979  0.09772551]\n",
      "Gradient Descent(891/999): loss=0.40350979452371755, w = [-1.55131081  0.23157037 -0.49866143 -1.34274256 -0.07092985  1.0533318\n",
      "  0.82126933 -0.43457493  0.0641356   0.32114448 -0.00343102 -0.09474477\n",
      "  0.02252474 -0.03609894  0.09772146]\n",
      "Gradient Descent(892/999): loss=0.4034959344022078, w = [-1.55141     0.23142626 -0.49862152 -1.343591   -0.07085098  1.05395218\n",
      "  0.82148047 -0.4348513   0.06408433  0.32114671 -0.00342806 -0.09440604\n",
      "  0.02252344 -0.03602823  0.09771741]\n",
      "Gradient Descent(893/999): loss=0.40348209357474096, w = [-1.55150913  0.23128216 -0.49858162 -1.3444388  -0.07077217  1.05457206\n",
      "  0.82169146 -0.43512761  0.06403311  0.32114883 -0.00342511 -0.09406745\n",
      "  0.02252215 -0.03595765  0.09771337]\n",
      "Gradient Descent(894/999): loss=0.4034682720059054, w = [-1.5516082   0.23113809 -0.49854174 -1.34528597 -0.07069339  1.05519146\n",
      "  0.82190228 -0.43540388  0.06398193  0.32115086 -0.00342216 -0.09372902\n",
      "  0.02252084 -0.03588721  0.09770932]\n",
      "Gradient Descent(895/999): loss=0.4034544696603761, w = [-1.55170722  0.23099403 -0.49850187 -1.34613251 -0.07061467  1.05581036\n",
      "  0.82211294 -0.43568009  0.0639308   0.32115278 -0.0034192  -0.09339073\n",
      "  0.02251954 -0.0358169   0.09770527]\n",
      "Gradient Descent(896/999): loss=0.4034406865029171, w = [-1.55180618  0.23084998 -0.49846202 -1.34697843 -0.07053599  1.05642877\n",
      "  0.82232345 -0.43595625  0.06387971  0.32115461 -0.00341625 -0.0930526\n",
      "  0.02251824 -0.03574673  0.09770123]\n",
      "Gradient Descent(897/999): loss=0.40342692249838036, w = [-1.55190509  0.23070595 -0.49842219 -1.34782371 -0.07045735  1.0570467\n",
      "  0.8225338  -0.43623235  0.06382867  0.32115633 -0.00341329 -0.09271462\n",
      "  0.02251693 -0.03567669  0.09769718]\n",
      "Gradient Descent(898/999): loss=0.40341317761170653, w = [-1.55200395  0.23056194 -0.49838237 -1.34866836 -0.07037876  1.05766413\n",
      "  0.82274399 -0.4365084   0.06377768  0.32115796 -0.00341033 -0.0923768\n",
      "  0.02251562 -0.03560679  0.09769314]\n",
      "Gradient Descent(899/999): loss=0.4033994518079239, w = [-1.55210274  0.23041795 -0.49834257 -1.34951239 -0.07030022  1.05828108\n",
      "  0.82295402 -0.43678439  0.06372672  0.32115948 -0.00340737 -0.09203912\n",
      "  0.02251431 -0.03553702  0.09768909]\n",
      "Gradient Descent(900/999): loss=0.40338574505214736, w = [-1.55220149  0.23027398 -0.49830278 -1.35035579 -0.07022172  1.05889754\n",
      "  0.8231639  -0.43706033  0.06367581  0.32116091 -0.00340441 -0.09170159\n",
      "  0.02251299 -0.03546738  0.09768505]\n",
      "Gradient Descent(901/999): loss=0.4033720573095778, w = [-1.55230017  0.23013002 -0.49826301 -1.35119856 -0.07014327  1.05951351\n",
      "  0.82337362 -0.43733621  0.06362495  0.32116224 -0.00340145 -0.09136422\n",
      "  0.02251168 -0.03539788  0.09768101]\n",
      "Gradient Descent(902/999): loss=0.4033583885455055, w = [-1.55239881  0.22998608 -0.49822325 -1.35204071 -0.07006486  1.060129\n",
      "  0.82358318 -0.43761204  0.06357413  0.32116347 -0.00339849 -0.09102699\n",
      "  0.02251036 -0.03532851  0.09767697]\n",
      "Gradient Descent(903/999): loss=0.403344738725305, w = [-1.55249739  0.22984216 -0.49818351 -1.35288223 -0.0699865   1.060744\n",
      "  0.82379258 -0.43788782  0.06352336  0.3211646  -0.00339553 -0.09068992\n",
      "  0.02250904 -0.03525927  0.09767293]\n",
      "Gradient Descent(904/999): loss=0.4033311078144382, w = [-1.55259591  0.22969826 -0.49814379 -1.35372313 -0.06990819  1.06135851\n",
      "  0.82400183 -0.43816354  0.06347263  0.32116563 -0.00339256 -0.090353\n",
      "  0.02250771 -0.03519016  0.09766889]\n",
      "Gradient Descent(905/999): loss=0.40331749577845133, w = [-1.55269438  0.22955438 -0.49810408 -1.3545634  -0.06982992  1.06197254\n",
      "  0.82421092 -0.43843921  0.06342194  0.32116656 -0.0033896  -0.09001623\n",
      "  0.02250639 -0.03512119  0.09766486]\n",
      "Gradient Descent(906/999): loss=0.40330390258297755, w = [-1.5527928   0.22941052 -0.49806439 -1.35540306 -0.0697517   1.06258608\n",
      "  0.82441985 -0.43871482  0.0633713   0.3211674  -0.00338664 -0.0896796\n",
      "  0.02250506 -0.03505235  0.09766082]\n",
      "Gradient Descent(907/999): loss=0.40329032819373595, w = [-1.55289117  0.22926668 -0.49802472 -1.35624209 -0.06967353  1.06319914\n",
      "  0.82462863 -0.43899038  0.0633207   0.32116814 -0.00338367 -0.08934313\n",
      "  0.02250374 -0.03498364  0.09765679]\n",
      "Gradient Descent(908/999): loss=0.4032767725765279, w = [-1.55298948  0.22912286 -0.49798506 -1.3570805  -0.0695954   1.06381172\n",
      "  0.82483725 -0.43926588  0.06327015  0.32116878 -0.0033807  -0.08900681\n",
      "  0.0225024  -0.03491507  0.09765276]\n",
      "Gradient Descent(909/999): loss=0.40326323569724165, w = [-1.55308774  0.22897906 -0.49794542 -1.35791829 -0.06951731  1.06442381\n",
      "  0.82504571 -0.43954132  0.06321964  0.32116932 -0.00337774 -0.08867064\n",
      "  0.02250107 -0.03484662  0.09764872]\n",
      "Gradient Descent(910/999): loss=0.40324971752184996, w = [-1.55318594  0.22883528 -0.4979058  -1.35875546 -0.06943928  1.06503542\n",
      "  0.82525402 -0.43981671  0.06316918  0.32116977 -0.00337477 -0.08833462\n",
      "  0.02249974 -0.03477831  0.0976447 ]\n",
      "Gradient Descent(911/999): loss=0.4032362180164088, w = [-1.5532841   0.22869152 -0.49786619 -1.35959202 -0.06936129  1.06564655\n",
      "  0.82546218 -0.44009205  0.06311875  0.32117012 -0.0033718  -0.08799875\n",
      "  0.0224984  -0.03471012  0.09764067]\n",
      "Gradient Descent(912/999): loss=0.40322273714705775, w = [-1.5533822   0.22854778 -0.4978266  -1.36042795 -0.06928334  1.0662572\n",
      "  0.82567018 -0.44036733  0.06306838  0.32117037 -0.00336883 -0.08766303\n",
      "  0.02249706 -0.03464207  0.09763664]\n",
      "Gradient Descent(913/999): loss=0.40320927488002095, w = [-1.55348025  0.22840406 -0.49778702 -1.36126327 -0.06920545  1.06686737\n",
      "  0.82587802 -0.44064255  0.06301805  0.32117053 -0.00336586 -0.08732746\n",
      "  0.02249572 -0.03457415  0.09763262]\n",
      "Gradient Descent(914/999): loss=0.4031958311816057, w = [-1.55357824  0.22826036 -0.49774746 -1.36209797 -0.0691276   1.06747705\n",
      "  0.82608571 -0.44091772  0.06296776  0.32117059 -0.00336289 -0.08699204\n",
      "  0.02249438 -0.03450636  0.09762859]\n",
      "Gradient Descent(915/999): loss=0.403182406018202, w = [-1.55367619  0.22811668 -0.49770792 -1.36293206 -0.06904979  1.06808626\n",
      "  0.82629324 -0.44119283  0.06291751  0.32117056 -0.00335992 -0.08665676\n",
      "  0.02249304 -0.03443869  0.09762457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(916/999): loss=0.40316899935628214, w = [-1.55377408  0.22797303 -0.4976684  -1.36376553 -0.06897203  1.06869498\n",
      "  0.82650063 -0.44146789  0.06286731  0.32117043 -0.00335694 -0.08632164\n",
      "  0.02249169 -0.03437116  0.09762055]\n",
      "Gradient Descent(917/999): loss=0.4031556111624019, w = [-1.55387192  0.2278294  -0.49762889 -1.36459839 -0.06889432  1.06930323\n",
      "  0.82670785 -0.44174289  0.06281716  0.3211702  -0.00335397 -0.08598667\n",
      "  0.02249034 -0.03430376  0.09761653]\n",
      "Gradient Descent(918/999): loss=0.40314224140319904, w = [-1.55396971  0.22768579 -0.4975894  -1.36543064 -0.06881666  1.069911\n",
      "  0.82691492 -0.44201783  0.06276704  0.32116988 -0.003351   -0.08565185\n",
      "  0.02248899 -0.03423649  0.09761252]\n",
      "Gradient Descent(919/999): loss=0.4031288900453924, w = [-1.55406745  0.2275422  -0.49754993 -1.36626227 -0.06873904  1.07051829\n",
      "  0.82712184 -0.44229272  0.06271697  0.32116947 -0.00334802 -0.08531717\n",
      "  0.02248764 -0.03416934  0.0976085 ]\n",
      "Gradient Descent(920/999): loss=0.4031155570557842, w = [-1.55416514  0.22739864 -0.49751047 -1.36709329 -0.06866147  1.0711251\n",
      "  0.82732861 -0.44256755  0.06266695  0.32116896 -0.00334505 -0.08498265\n",
      "  0.02248629 -0.03410233  0.09760449]\n",
      "Gradient Descent(921/999): loss=0.4031022424012555, w = [-1.55426278  0.2272551  -0.49747104 -1.3679237  -0.06858395  1.07173144\n",
      "  0.82753522 -0.44284232  0.06261697  0.32116836 -0.00334207 -0.08464827\n",
      "  0.02248493 -0.03403544  0.09760048]\n",
      "Gradient Descent(922/999): loss=0.40308894604877094, w = [-1.55436036  0.22711158 -0.49743162 -1.3687535  -0.06850647  1.0723373\n",
      "  0.82774168 -0.44311704  0.06256703  0.32116766 -0.00333909 -0.08431404\n",
      "  0.02248357 -0.03396868  0.09759647]\n",
      "Gradient Descent(923/999): loss=0.4030756679653755, w = [-1.5544579   0.22696808 -0.49739221 -1.36958269 -0.06842904  1.07294268\n",
      "  0.82794799 -0.4433917   0.06251713  0.32116687 -0.00333612 -0.08397997\n",
      "  0.02248222 -0.03390205  0.09759247]\n",
      "Gradient Descent(924/999): loss=0.40306240811819344, w = [-1.55455539  0.22682461 -0.49735283 -1.37041127 -0.06835166  1.07354759\n",
      "  0.82815414 -0.44366631  0.06246728  0.32116598 -0.00333314 -0.08364604\n",
      "  0.02248086 -0.03383555  0.09758847]\n",
      "Gradient Descent(925/999): loss=0.40304916647443056, w = [-1.55465282  0.22668117 -0.49731346 -1.37123925 -0.06827432  1.07415202\n",
      "  0.82836014 -0.44394086  0.06241747  0.321165   -0.00333016 -0.08331226\n",
      "  0.02247949 -0.03376917  0.09758446]\n",
      "Gradient Descent(926/999): loss=0.4030359430013733, w = [-1.55475021  0.22653774 -0.49727411 -1.37206661 -0.06819704  1.07475598\n",
      "  0.82856599 -0.44421535  0.06236771  0.32116393 -0.00332718 -0.08297862\n",
      "  0.02247813 -0.03370293  0.09758047]\n",
      "Gradient Descent(927/999): loss=0.4030227376663862, w = [-1.55484754  0.22639435 -0.49723478 -1.37289337 -0.06811979  1.07535947\n",
      "  0.82877169 -0.44448978  0.06231799  0.32116276 -0.0033242  -0.08264514\n",
      "  0.02247676 -0.03363681  0.09757647]\n",
      "Gradient Descent(928/999): loss=0.4030095504369144, w = [-1.55494483  0.22625097 -0.49719546 -1.37371953 -0.0680426   1.07596248\n",
      "  0.82897723 -0.44476416  0.06226831  0.3211615  -0.00332122 -0.08231181\n",
      "  0.02247539 -0.03357081  0.09757247]\n",
      "Gradient Descent(929/999): loss=0.4029963812804825, w = [-1.55504207  0.22610762 -0.49715616 -1.37454508 -0.06796545  1.07656502\n",
      "  0.82918262 -0.44503848  0.06221868  0.32116015 -0.00331824 -0.08197862\n",
      "  0.02247403 -0.03350494  0.09756848]\n",
      "Gradient Descent(930/999): loss=0.4029832301646944, w = [-1.55513926  0.2259643  -0.49711688 -1.37537002 -0.06788835  1.07716708\n",
      "  0.82938787 -0.44531274  0.06216909  0.32115871 -0.00331526 -0.08164558\n",
      "  0.02247265 -0.0334392   0.09756449]\n",
      "Gradient Descent(931/999): loss=0.40297009705723097, w = [-1.55523639  0.225821   -0.49707762 -1.37619437 -0.0678113   1.07776868\n",
      "  0.82959296 -0.44558695  0.06211954  0.32115717 -0.00331228 -0.08131269\n",
      "  0.02247128 -0.03337359  0.09756051]\n",
      "Gradient Descent(932/999): loss=0.40295698192585416, w = [-1.55533348  0.22567773 -0.49703838 -1.37701811 -0.0677343   1.0783698\n",
      "  0.82979789 -0.44586109  0.06207004  0.32115554 -0.00330929 -0.08097995\n",
      "  0.02246991 -0.0333081   0.09755652]\n",
      "Gradient Descent(933/999): loss=0.4029438847384021, w = [-1.55543053  0.22553448 -0.49699915 -1.37784124 -0.06765734  1.07897045\n",
      "  0.83000268 -0.44613518  0.06202058  0.32115382 -0.00330631 -0.08064735\n",
      "  0.02246853 -0.03324274  0.09755254]\n",
      "Gradient Descent(934/999): loss=0.40293080546279214, w = [-1.55552752  0.22539126 -0.49695995 -1.37866378 -0.06758043  1.07957063\n",
      "  0.83020732 -0.44640922  0.06197116  0.32115201 -0.00330333 -0.08031491\n",
      "  0.02246715 -0.0331775   0.09754856]\n",
      "Gradient Descent(935/999): loss=0.40291774406701913, w = [-1.55562446  0.22524806 -0.49692076 -1.37948571 -0.06750356  1.08017035\n",
      "  0.83041181 -0.44668319  0.06192178  0.32115011 -0.00330034 -0.07998261\n",
      "  0.02246577 -0.03311239  0.09754458]\n",
      "Gradient Descent(936/999): loss=0.40290470051915545, w = [-1.55572136  0.22510489 -0.49688158 -1.38030705 -0.06742675  1.08076959\n",
      "  0.83061614 -0.44695711  0.06187245  0.32114812 -0.00329736 -0.07965045\n",
      "  0.02246439 -0.0330474   0.09754061]\n",
      "Gradient Descent(937/999): loss=0.4028916747873503, w = [-1.5558182   0.22496175 -0.49684243 -1.38112779 -0.06734998  1.08136836\n",
      "  0.83082033 -0.44723097  0.06182316  0.32114603 -0.00329437 -0.07931845\n",
      "  0.02246301 -0.03298254  0.09753664]\n",
      "Gradient Descent(938/999): loss=0.4028786668398308, w = [-1.555915    0.22481863 -0.4968033  -1.38194792 -0.06727326  1.08196667\n",
      "  0.83102437 -0.44750477  0.06177392  0.32114386 -0.00329139 -0.07898659\n",
      "  0.02246162 -0.0329178   0.09753267]\n",
      "Gradient Descent(939/999): loss=0.4028656766449001, w = [-1.55601175  0.22467554 -0.49676418 -1.38276746 -0.06719659  1.0825645\n",
      "  0.83122825 -0.44777851  0.06172471  0.32114159 -0.0032884  -0.07865489\n",
      "  0.02246024 -0.03285319  0.0975287 ]\n",
      "Gradient Descent(940/999): loss=0.4028527041709382, w = [-1.55610846  0.22453248 -0.49672508 -1.38358641 -0.06711996  1.08316187\n",
      "  0.83143199 -0.4480522   0.06167555  0.32113923 -0.00328541 -0.07832332\n",
      "  0.02245885 -0.0327887   0.09752474]\n",
      "Gradient Descent(941/999): loss=0.40283974938640155, w = [-1.55620511  0.22438945 -0.496686   -1.38440476 -0.06704338  1.08375877\n",
      "  0.83163558 -0.44832582  0.06162644  0.32113679 -0.00328243 -0.07799191\n",
      "  0.02245746 -0.03272433  0.09752078]\n",
      "Gradient Descent(942/999): loss=0.4028268122598214, w = [-1.55630172  0.22424644 -0.49664694 -1.38522251 -0.06696685  1.08435521\n",
      "  0.83183902 -0.44859939  0.06157736  0.32113425 -0.00327944 -0.07766064\n",
      "  0.02245607 -0.03266009  0.09751682]\n",
      "Gradient Descent(943/999): loss=0.40281389275980733, w = [-1.55639828  0.22410346 -0.4966079  -1.38603966 -0.06689037  1.08495118\n",
      "  0.83204231 -0.4488729   0.06152833  0.32113163 -0.00327645 -0.07732952\n",
      "  0.02245468 -0.03259597  0.09751287]\n",
      "Gradient Descent(944/999): loss=0.40280099085504184, w = [-1.5564948   0.22396051 -0.49656888 -1.38685623 -0.06681394  1.08554669\n",
      "  0.83224545 -0.44914635  0.06147934  0.32112891 -0.00327346 -0.07699855\n",
      "  0.02245328 -0.03253197  0.09750892]\n",
      "Gradient Descent(945/999): loss=0.4027881065142839, w = [-1.55659127  0.22381758 -0.49652987 -1.3876722  -0.06673755  1.08614172\n",
      "  0.83244844 -0.44941974  0.06143039  0.32112611 -0.00327048 -0.07666772\n",
      "  0.02245189 -0.0324681   0.09750497]\n",
      "Gradient Descent(946/999): loss=0.4027752397063679, w = [-1.55668769  0.22367469 -0.49649089 -1.38848757 -0.06666121  1.0867363\n",
      "  0.83265129 -0.44969308  0.06138149  0.32112321 -0.00326749 -0.07633704\n",
      "  0.02245049 -0.03240435  0.09750103]\n",
      "Gradient Descent(947/999): loss=0.4027623904002021, w = [-1.55678406  0.22353182 -0.49645192 -1.38930236 -0.06658492  1.08733041\n",
      "  0.83285398 -0.44996635  0.06133263  0.32112023 -0.0032645  -0.07600651\n",
      "  0.02244909 -0.03234072  0.09749708]\n",
      "Gradient Descent(948/999): loss=0.40274955856477096, w = [-1.55688039  0.22338899 -0.49641297 -1.39011655 -0.06650868  1.08792406\n",
      "  0.83305653 -0.45023957  0.06128381  0.32111716 -0.00326151 -0.07567612\n",
      "  0.02244769 -0.03227721  0.09749314]\n",
      "Gradient Descent(949/999): loss=0.40273674416913124, w = [-1.55697667  0.22324618 -0.49637404 -1.39093015 -0.06643248  1.08851724\n",
      "  0.83325893 -0.45051272  0.06123503  0.321114   -0.00325852 -0.07534588\n",
      "  0.02244629 -0.03221383  0.09748921]\n",
      "Gradient Descent(950/999): loss=0.402723947182415, w = [-1.5570729   0.2231034  -0.49633513 -1.39174317 -0.06635633  1.08910997\n",
      "  0.83346118 -0.45078582  0.0611863   0.32111075 -0.00325553 -0.07501579\n",
      "  0.02244489 -0.03215057  0.09748528]\n",
      "Gradient Descent(951/999): loss=0.4027111675738281, w = [-1.55716909  0.22296065 -0.49629624 -1.39255559 -0.06628024  1.08970223\n",
      "  0.83366329 -0.45105886  0.06113761  0.32110742 -0.00325253 -0.07468584\n",
      "  0.02244349 -0.03208743  0.09748135]\n",
      "Gradient Descent(952/999): loss=0.4026984053126503, w = [-1.55726523  0.22281793 -0.49625736 -1.39336743 -0.06620418  1.09029402\n",
      "  0.83386525 -0.45133184  0.06108896  0.321104   -0.00324954 -0.07435604\n",
      "  0.02244208 -0.03202441  0.09747742]\n",
      "Gradient Descent(953/999): loss=0.402685660368234, w = [-1.55736133  0.22267524 -0.49621851 -1.39417867 -0.06612818  1.09088536\n",
      "  0.83406706 -0.45160476  0.06104035  0.32110049 -0.00324655 -0.07402639\n",
      "  0.02244067 -0.03196151  0.0974735 ]\n",
      "Gradient Descent(954/999): loss=0.40267293271000526, w = [-1.55745738  0.22253258 -0.49617967 -1.39498934 -0.06605223  1.09147624\n",
      "  0.83426872 -0.45187762  0.06099179  0.32109689 -0.00324356 -0.07369688\n",
      "  0.02243926 -0.03189873  0.09746958]\n",
      "Gradient Descent(955/999): loss=0.4026602223074631, w = [-1.55755338  0.22238995 -0.49614086 -1.39579941 -0.06597632  1.09206666\n",
      "  0.83447024 -0.45215042  0.06094326  0.3210932  -0.00324057 -0.07336752\n",
      "  0.02243785 -0.03183607  0.09746566]\n",
      "Gradient Descent(956/999): loss=0.4026475291301787, w = [-1.55764934  0.22224735 -0.49610206 -1.3966089  -0.06590046  1.09265661\n",
      "  0.83467161 -0.45242316  0.06089478  0.32108943 -0.00323757 -0.0730383\n",
      "  0.02243644 -0.03177353  0.09746175]\n",
      "Gradient Descent(957/999): loss=0.4026348531477975, w = [-1.55774526  0.22210478 -0.49606328 -1.3974178  -0.06582465  1.09324611\n",
      "  0.83487283 -0.45269584  0.06084635  0.32108557 -0.00323458 -0.07270923\n",
      "  0.02243503 -0.03171111  0.09745784]\n",
      "Gradient Descent(958/999): loss=0.4026221943300344, w = [-1.55784113  0.22196224 -0.49602453 -1.39822612 -0.06574889  1.09383515\n",
      "  0.83507391 -0.45296846  0.06079795  0.32108163 -0.00323159 -0.0723803\n",
      "  0.02243362 -0.03164881  0.09745394]\n",
      "Gradient Descent(959/999): loss=0.40260955264667836, w = [-1.55793695  0.22181974 -0.49598579 -1.39903386 -0.06567317  1.09442373\n",
      "  0.83527484 -0.45324103  0.0607496   0.3210776  -0.00322859 -0.07205153\n",
      "  0.0224322  -0.03158663  0.09745004]\n",
      "Gradient Descent(960/999): loss=0.4025969280675902, w = [-1.55803273  0.22167726 -0.49594707 -1.39984101 -0.06559751  1.09501186\n",
      "  0.83547563 -0.45351353  0.06070128  0.32107348 -0.0032256  -0.07172289\n",
      "  0.02243078 -0.03152457  0.09744614]\n",
      "Gradient Descent(961/999): loss=0.40258432056270116, w = [-1.55812846  0.22153482 -0.49590837 -1.40064758 -0.06552189  1.09559952\n",
      "  0.83567627 -0.45378597  0.06065301  0.32106928 -0.00322261 -0.0713944\n",
      "  0.02242937 -0.03146263  0.09744224]\n",
      "Gradient Descent(962/999): loss=0.40257173010201464, w = [-1.55822415  0.2213924  -0.49586969 -1.40145357 -0.06544632  1.09618673\n",
      "  0.83587677 -0.45405835  0.06060479  0.32106499 -0.00321961 -0.07106606\n",
      "  0.02242795 -0.03140081  0.09743835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(963/999): loss=0.4025591566556047, w = [-1.5583198   0.22125002 -0.49583103 -1.40225898 -0.0653708   1.09677349\n",
      "  0.83607712 -0.45433067  0.0605566   0.32106061 -0.00321662 -0.07073786\n",
      "  0.02242653 -0.03133911  0.09743447]\n",
      "Gradient Descent(964/999): loss=0.4025466001936163, w = [-1.5584154   0.22110767 -0.49579238 -1.40306381 -0.06529532  1.09735979\n",
      "  0.83627732 -0.45460293  0.06050845  0.32105615 -0.00321362 -0.07040981\n",
      "  0.0224251  -0.03127752  0.09743058]\n",
      "Gradient Descent(965/999): loss=0.402534060686266, w = [-1.55851095  0.22096536 -0.49575376 -1.40386806 -0.0652199   1.09794563\n",
      "  0.83647738 -0.45487513  0.06046035  0.32105161 -0.00321063 -0.0700819\n",
      "  0.02242368 -0.03121606  0.0974267 ]\n",
      "Gradient Descent(966/999): loss=0.40252153810383984, w = [-1.55860646  0.22082307 -0.49571516 -1.40467173 -0.06514452  1.09853102\n",
      "  0.8366773  -0.45514727  0.06041229  0.32104698 -0.00320763 -0.06975414\n",
      "  0.02242225 -0.03115471  0.09742283]\n",
      "Gradient Descent(967/999): loss=0.40250903241669433, w = [-1.55870193  0.22068082 -0.49567658 -1.40547483 -0.0650692   1.09911595\n",
      "  0.83687707 -0.45541935  0.06036427  0.32104227 -0.00320463 -0.06942652\n",
      "  0.02242083 -0.03109347  0.09741895]\n",
      "Gradient Descent(968/999): loss=0.4024965435952559, w = [-1.55879736  0.2205386  -0.49563802 -1.40627734 -0.06499392  1.09970043\n",
      "  0.8370767  -0.45569137  0.06031629  0.32103747 -0.00320164 -0.06909905\n",
      "  0.0224194  -0.03103236  0.09741508]\n",
      "Gradient Descent(969/999): loss=0.4024840716100209, w = [-1.55889274  0.22039641 -0.49559947 -1.40707928 -0.06491868  1.10028446\n",
      "  0.83727618 -0.45596333  0.06026836  0.32103258 -0.00319864 -0.06877172\n",
      "  0.02241797 -0.03097136  0.09741122]\n",
      "Gradient Descent(970/999): loss=0.4024716164315555, w = [-1.55898807  0.22025426 -0.49556095 -1.40788064 -0.0648435   1.10086803\n",
      "  0.83747552 -0.45623523  0.06022046  0.32102762 -0.00319565 -0.06844454\n",
      "  0.02241654 -0.03091048  0.09740736]\n",
      "Gradient Descent(971/999): loss=0.4024591780304955, w = [-1.55908336  0.22011214 -0.49552245 -1.40868143 -0.06476837  1.10145115\n",
      "  0.83767471 -0.45650706  0.06017261  0.32102257 -0.00319265 -0.0681175\n",
      "  0.02241511 -0.03084972  0.0974035 ]\n",
      "Gradient Descent(972/999): loss=0.4024467563775445, w = [-1.55917861  0.21997005 -0.49548396 -1.40948164 -0.06469328  1.10203382\n",
      "  0.83787376 -0.45677884  0.0601248   0.32101743 -0.00318965 -0.06779061\n",
      "  0.02241368 -0.03078907  0.09739965]\n",
      "Gradient Descent(973/999): loss=0.4024343514434764, w = [-1.55927382  0.219828   -0.4954455  -1.41028128 -0.06461825  1.10261604\n",
      "  0.83807267 -0.45705055  0.06007703  0.32101221 -0.00318665 -0.06746386\n",
      "  0.02241224 -0.03072854  0.0973958 ]\n",
      "Gradient Descent(974/999): loss=0.4024219631991334, w = [-1.55936898  0.21968598 -0.49540705 -1.41108035 -0.06454326  1.10319781\n",
      "  0.83827144 -0.45732221  0.0600293   0.32100691 -0.00318366 -0.06713725\n",
      "  0.02241081 -0.03066812  0.09739195]\n",
      "Gradient Descent(975/999): loss=0.40240959161542594, w = [-1.5594641   0.21954399 -0.49536863 -1.41187884 -0.06446832  1.10377913\n",
      "  0.83847006 -0.4575938   0.05998161  0.32100153 -0.00318066 -0.06681079\n",
      "  0.02240937 -0.03060782  0.09738811]\n",
      "Gradient Descent(976/999): loss=0.4023972366633331, w = [-1.55955917  0.21940204 -0.49533023 -1.41267676 -0.06439343  1.10435999\n",
      "  0.83866854 -0.45786533  0.05993397  0.32099606 -0.00317766 -0.06648448\n",
      "  0.02240794 -0.03054764  0.09738428]\n",
      "Gradient Descent(977/999): loss=0.4023848983139012, w = [-1.55965421  0.21926013 -0.49529184 -1.41347411 -0.06431858  1.10494041\n",
      "  0.83886687 -0.4581368   0.05988636  0.32099051 -0.00317466 -0.06615831\n",
      "  0.0224065  -0.03048757  0.09738044]\n",
      "Gradient Descent(978/999): loss=0.4023725765382461, w = [-1.5597492   0.21911824 -0.49525348 -1.41427089 -0.06424379  1.10552038\n",
      "  0.83906507 -0.45840821  0.0598388   0.32098488 -0.00317167 -0.06583228\n",
      "  0.02240506 -0.03042761  0.09737661]\n",
      "Gradient Descent(979/999): loss=0.40236027130754987, w = [-1.55984414  0.2189764  -0.49521514 -1.4150671  -0.06416905  1.1060999\n",
      "  0.83926312 -0.45867955  0.05979128  0.32097916 -0.00316867 -0.06550639\n",
      "  0.02240362 -0.03036777  0.09737279]\n",
      "Gradient Descent(980/999): loss=0.4023479825930624, w = [-1.55993905  0.21883458 -0.49517681 -1.41586274 -0.06409435  1.10667897\n",
      "  0.83946103 -0.45895084  0.0597438   0.32097336 -0.00316567 -0.06518065\n",
      "  0.02240218 -0.03030805  0.09736897]\n",
      "Gradient Descent(981/999): loss=0.40233571036610055, w = [-1.56003391  0.2186928  -0.49513851 -1.41665781 -0.0640197   1.10725759\n",
      "  0.8396588  -0.45922206  0.05969636  0.32096748 -0.00316267 -0.06485506\n",
      "  0.02240073 -0.03024844  0.09736515]\n",
      "Gradient Descent(982/999): loss=0.40232345459804963, w = [-1.56012873  0.21855106 -0.49510023 -1.41745231 -0.0639451   1.10783577\n",
      "  0.83985642 -0.45949322  0.05964896  0.32096152 -0.00315967 -0.0645296\n",
      "  0.02239929 -0.03018894  0.09736134]\n",
      "Gradient Descent(983/999): loss=0.4023112152603595, w = [-1.56022351  0.21840936 -0.49506196 -1.41824625 -0.06387055  1.1084135\n",
      "  0.8400539  -0.45976432  0.0596016   0.32095548 -0.00315668 -0.0642043\n",
      "  0.02239784 -0.03012956  0.09735753]\n",
      "Gradient Descent(984/999): loss=0.40229899232454885, w = [-1.56031824  0.21826768 -0.49502372 -1.41903962 -0.06379605  1.10899079\n",
      "  0.84025125 -0.46003536  0.05955429  0.32094936 -0.00315368 -0.06387913\n",
      "  0.0223964  -0.03007029  0.09735373]\n",
      "Gradient Descent(985/999): loss=0.4022867857622011, w = [-1.56041294  0.21812605 -0.4949855  -1.41983242 -0.0637216   1.10956763\n",
      "  0.84044845 -0.46030633  0.05950701  0.32094315 -0.00315068 -0.06355411\n",
      "  0.02239495 -0.03001113  0.09734993]\n",
      "Gradient Descent(986/999): loss=0.4022745955449666, w = [-1.56050759  0.21798445 -0.4949473  -1.42062466 -0.0636472   1.11014402\n",
      "  0.84064551 -0.46057724  0.05945978  0.32093687 -0.00314768 -0.06322923\n",
      "  0.0223935  -0.02995209  0.09734613]\n",
      "Gradient Descent(987/999): loss=0.4022624216445619, w = [-1.5606022   0.21784288 -0.49490912 -1.42141634 -0.06357284  1.11071997\n",
      "  0.84084243 -0.46084809  0.05941258  0.3209305  -0.00314468 -0.06290449\n",
      "  0.02239205 -0.02989316  0.09734234]\n",
      "Gradient Descent(988/999): loss=0.40225026403276953, w = [-1.56069676  0.21770136 -0.49487096 -1.42220745 -0.06349853  1.11129547\n",
      "  0.84103921 -0.46111888  0.05936543  0.32092405 -0.00314168 -0.0625799\n",
      "  0.0223906  -0.02983434  0.09733855]\n",
      "Gradient Descent(989/999): loss=0.40223812268143594, w = [-1.56079129  0.21755987 -0.49483282 -1.42299799 -0.06342428  1.11187053\n",
      "  0.84123584 -0.46138961  0.05931832  0.32091753 -0.00313868 -0.06225545\n",
      "  0.02238915 -0.02977564  0.09733477]\n",
      "Gradient Descent(990/999): loss=0.40222599756247507, w = [-1.56088577  0.21741841 -0.4947947  -1.42378798 -0.06335007  1.11244515\n",
      "  0.84143234 -0.46166027  0.05927125  0.32091092 -0.00313568 -0.06193115\n",
      "  0.02238769 -0.02971704  0.09733099]\n",
      "Gradient Descent(991/999): loss=0.4022138886478657, w = [-1.56098022  0.21727699 -0.4947566  -1.4245774  -0.06327591  1.11301932\n",
      "  0.8416287  -0.46193087  0.05922421  0.32090423 -0.00313269 -0.06160699\n",
      "  0.02238624 -0.02965856  0.09732722]\n",
      "Gradient Descent(992/999): loss=0.4022017959096503, w = [-1.56107462  0.21713561 -0.49471852 -1.42536626 -0.0632018   1.11359305\n",
      "  0.84182491 -0.46220141  0.05917722  0.32089746 -0.00312969 -0.06128297\n",
      "  0.02238478 -0.02960019  0.09732345]\n",
      "Gradient Descent(993/999): loss=0.4021897193199371, w = [-1.56116898  0.21699427 -0.49468046 -1.42615456 -0.06312774  1.11416634\n",
      "  0.84202099 -0.46247189  0.05913027  0.32089061 -0.00312669 -0.06095909\n",
      "  0.02238333 -0.02954193  0.09731969]\n",
      "Gradient Descent(994/999): loss=0.4021776588508989, w = [-1.5612633   0.21685296 -0.49464243 -1.4269423  -0.06305372  1.11473919\n",
      "  0.84221693 -0.4627423   0.05908337  0.32088369 -0.00312369 -0.06063536\n",
      "  0.02238187 -0.02948379  0.09731593]\n",
      "Gradient Descent(995/999): loss=0.40216561447477256, w = [-1.56135757  0.21671169 -0.49460441 -1.42772948 -0.06297976  1.1153116\n",
      "  0.84241272 -0.46301265  0.0590365   0.32087668 -0.00312069 -0.06031176\n",
      "  0.02238041 -0.02942575  0.09731217]\n",
      "Gradient Descent(996/999): loss=0.4021535861638599, w = [-1.56145181  0.21657046 -0.49456641 -1.4285161  -0.06290585  1.11588357\n",
      "  0.84260838 -0.46328294  0.05898967  0.3208696  -0.00311769 -0.05998832\n",
      "  0.02237895 -0.02936783  0.09730842]\n",
      "Gradient Descent(997/999): loss=0.40214157389052574, w = [-1.56154601  0.21642927 -0.49452844 -1.42930216 -0.06283198  1.11645509\n",
      "  0.8428039  -0.46355316  0.05894288  0.32086243 -0.00311469 -0.05966501\n",
      "  0.02237749 -0.02931001  0.09730467]\n",
      "Gradient Descent(998/999): loss=0.40212957762719964, w = [-1.56164016  0.21628811 -0.49449048 -1.43008767 -0.06275816  1.11702618\n",
      "  0.84299928 -0.46382332  0.05889614  0.32085519 -0.00311169 -0.05934185\n",
      "  0.02237603 -0.02925231  0.09730093]\n",
      "Gradient Descent(999/999): loss=0.40211759734637387, w = [-1.56173427  0.21614699 -0.49445255 -1.43087261 -0.06268439  1.11759683\n",
      "  0.84319452 -0.46409342  0.05884943  0.32084787 -0.00310869 -0.05901883\n",
      "  0.02237457 -0.02919471  0.09729719]\n"
     ]
    }
   ],
   "source": [
    "#logistic_reg(y,tx,initial_w,max_iters,gamma):\n",
    "w = logistic_reg(y_train,x,w,1000,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 9, 1, 8, 4, 2, 6, 3, 7])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_validation(x,y,k-cross=4):\n",
    "    np.random.shuffle(x)\n",
    "    data_size = len(y)//k-cross\n",
    "    for i in range()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
